<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/7/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/7/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/7/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Hexo</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Hexo</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">231</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">64</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/11/Spark-SQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/11/Spark-SQL/" class="post-title-link" itemprop="url">Spark_SQL</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-11 20:50:18" itemprop="dateCreated datePublished" datetime="2021-12-11T20:50:18+08:00">2021-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-06-07 20:58:21" itemprop="dateModified" datetime="2022-06-07T20:58:21+08:00">2022-06-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="第-1-章-Spark-SQL-概述"><a href="#第-1-章-Spark-SQL-概述" class="headerlink" title="第 1 章    Spark SQL 概述"></a>第 1 章    Spark SQL 概述</h1><h2 id="1-1-什么是-Spark-SQL"><a href="#1-1-什么是-Spark-SQL" class="headerlink" title="1.1    什么是 Spark SQL"></a>1.1    什么是 Spark SQL</h2><p><code>Spark SQL</code> 是 <code>Spark</code> 用于处理结构化数据的模块。</p>
<p>与基础的 <code>Spark RDD API</code> 不同， <code>Spark SQL</code> 的抽象数据类型为 <code>Spark</code> 提供了关于数据结构和正在执行的计算的更多信息。</p>
<p>在内部， <code>Spark SQL</code> 使用这些额外的信息去做一些额外的优化。</p>
<p><code>Spark</code> 提供了多种方式与 <code>Spark SQL</code> 进行交互，比如： <code>SQL</code> 和 <code>Dataset API</code>。当计算结果的时候，使用的是相同的执行引擎，不依赖你正在使用哪种 <code>API</code> 或者语言。这种统一也就意味着开发者可以很容易在不同的 <code>API</code> 之间进行切换，这些 <code>API</code> 提供了最自然的方式来表达给定的转换。</p>
<p>我们已经学习了 <code>Hive</code>，它是将 <code>Hive SQL</code> 转换成 <code>MapReduce</code> 然后提交到集群上执行，大大简化了编写 <code>MapReduce</code> 程序的复杂性，由于 <code>MapReduce</code> 这种计算模型执行效率比较慢，所以 <code>Spark SQL</code> 的应运而生，它是将 <code>Spark SQL</code> 转换成 <code>RDD</code>，然后提交到集群执行，执行效率非常快。</p>
<p><code>Spark SQL</code> 提供了 2 个编程抽象，类似 <code>Spark Core</code> 中的 <code>RDD</code>。它们分别是：</p>
<ul>
<li>  <strong>DataFrame</strong></li>
<li>  <strong>DataSet</strong></li>
</ul>
<hr>
<h2 id="1-2-Spark-SQL-的特点"><a href="#1-2-Spark-SQL-的特点" class="headerlink" title="1.2    Spark SQL 的特点"></a>1.2    Spark SQL 的特点</h2><h3 id="1-Integrated-易整合"><a href="#1-Integrated-易整合" class="headerlink" title="1. Integrated(易整合)"></a>1. Integrated(易整合)</h3><p>无缝的整合了 <code>SQL</code> 查询和 <code>Spark</code> 编程。</p>
<h3 id="2-Uniform-Data-Access-统一的数据访问方式"><a href="#2-Uniform-Data-Access-统一的数据访问方式" class="headerlink" title="2. Uniform Data Access(统一的数据访问方式)"></a>2. Uniform Data Access(统一的数据访问方式)</h3><p>使用相同的方式连接不同的数据源。</p>
<h3 id="3-Hive-Integration（集成-Hive）"><a href="#3-Hive-Integration（集成-Hive）" class="headerlink" title="3. Hive Integration（集成 Hive）"></a>3. Hive Integration（集成 Hive）</h3><p>在已有的仓库上直接运行 <code>SQL</code> 或者 <code>HQL</code></p>
<p><img src="/2021/12/11/Spark-SQL/image-20211221125130508.png" alt="image-20211221125130508"></p>
<h3 id="4-Standard-Connectivity（标准的连接方式）"><a href="#4-Standard-Connectivity（标准的连接方式）" class="headerlink" title="4. Standard Connectivity（标准的连接方式）"></a>4. Standard Connectivity（标准的连接方式）</h3><p>通过 <code>JDBC</code> 或者 <code>ODBC</code> 来连接</p>
<hr>
<h2 id="1-3-什么是-DataFrame"><a href="#1-3-什么是-DataFrame" class="headerlink" title="1.3    什么是 DataFrame"></a>1.3    什么是 DataFrame</h2><p>与 <code>RDD</code> 类似，<strong>DataFrame 也是一个分布式的数据容器</strong>。</p>
<p>然而 <code>DataFrame</code> 更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息（元数据信息），即 <code>schema</code>。</p>
<p>同时，与 <code>Hive</code> 类似，<code>DataFrame</code> 也支持嵌套数据类型（<code>struct</code>、<code>array</code> 和 <code>map</code>）。</p>
<p>从 <code>API</code> 易用性的角度上看， <code>DataFrame API</code> 提供了一套更高层的关系操作，比函数式的 <code>RDD API</code> 要更加友好，门槛更低。</p>
<p><img src="/2021/12/11/Spark-SQL/image-20211216221858916.png" alt="image-20211216221858916"></p>
<p>上图直观地体现了 <code>DataFrame</code> 和 <code>RDD</code> 的区别：</p>
<ul>
<li>  左侧的 <code>RDD[Person]</code> 虽然以 <code>Person</code> 为类型参数，但 <code>Spark</code> 框架本身不了解 <code>Person</code> 类的内部结构。</li>
<li>  而右侧的 <code>DataFrame</code> 却提供了详细的结构信息，通过 <code>Spark SQL</code> 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。</li>
</ul>
<p><code>DataFrame</code> 为数据提供了 <code>Schema（元数据）</code> 的视图，可以把它当做数据库中的一张表来对待。</p>
<p><code>DataFrame</code> 也是懒执行的。</p>
<p><code>DataFrame</code> 性能上比 <code>RDD</code> 要高，原因是 <code>Spark</code> 底层会通过 <code>Spark catalyst optimiser</code> 对这种类 <code>SQL</code> 的语句进行优化。比如下面一个例子：</p>
<p> <img src="/2021/12/11/Spark-SQL/image-20211216222125022.png" alt="image-20211216222125022"></p>
<p>为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个 <code>DataFrame</code>，将它们 <code>join</code> 之后又做了一次 <code>filter</code> 操作。</p>
<p>如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为 <code>join</code> 是一个代价较大的操作，也可能会产生一个较大的数据集。</p>
<p>如果我们能将 <code>filter</code> 下推到 <code>join</code> 下方，先对 <code>DataFrame</code> 进行过滤，再 <code>join</code> 过滤后的较小的结果集，便可以有效缩短执行时间。</p>
<p>而 <code>Spark SQL</code> 的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。</p>
<p><img src="/2021/12/11/Spark-SQL/image-20211216222328574.png" alt="image-20211216222328574"></p>
<hr>
<h2 id="1-4-什么是-DataSet"><a href="#1-4-什么是-DataSet" class="headerlink" title="1.4    什么是 DataSet"></a>1.4    什么是 DataSet</h2><ol>
<li>   是 <code>DataFrame API</code> 的一个扩展，是 <code>SparkSQL</code> 最新的数据抽象（1.6 新增）。</li>
<li>   用户友好的 <code>API</code> 风格，既具有类型安全检查也具有 <code>DataFrame</code> 的查询优化特性。</li>
<li>   <code>Dataset</code> 支持编/解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。</li>
<li>   样例类被用来在 <code>DataSet</code> 中定义数据的结构信息，样例类中每个属性的名称直接映射到 <code>DataSet</code> 中的字段名称。</li>
<li>   <code>DataFrame</code> 是 <code>DataSet</code> 的特例，<code>DataFrame=DataSet[Row]</code>，所以可以通过 <code>as</code> 方法将 <code>DataFrame</code> 转换为 <code>DataSet</code>。 <code>Row</code> 是一个类型，跟 <code>Car</code>、 <code>Person</code> 这些类型一样，所有的表结构信息都可以用 <code>Row</code> 来表示。</li>
<li>   <code>DataSet</code> 是强类型的。比如可以有 <code>DataSet[Car]</code>，<code>DataSet[Person]</code> 等。</li>
<li>   <code>DataFrame</code> 只是知道字段，但是不知道字段的类型，所以是没办法在编译的时候检查是否类型失败的，比如你可以对一个 <code>String</code> 进行减法操作，在执行的时候才报错，而 <code>DataSet</code> 不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟 <code>JSON</code> 对象和类对象之间的类比。</li>
</ol>
<hr>
<h1 id="第-2-章-Spark-SQL-编程"><a href="#第-2-章-Spark-SQL-编程" class="headerlink" title="第 2 章 Spark SQL 编程"></a>第 2 章 Spark SQL 编程</h1><p>本章重点学习如何使用 <code>DataFrame</code> 和 <code>DataSet</code> 进行编程，已以及它们之间的关系和转换。</p>
<hr>
<h2 id="2-1-SparkSession"><a href="#2-1-SparkSession" class="headerlink" title="2.1    SparkSession"></a>2.1    SparkSession</h2><p>在老的版本中， <code>SparkSQL</code> 提供两种 <code>SQL</code> 查询起始点：一个叫 <code>SQLContext</code>，用于 <code>Spark</code> 自己提供的 <code>SQL</code> 查询；一个叫 <code>HiveContext</code>，用于连接 <code>Hive</code> 的查询。</p>
<p>从 2.0 开始，<code>SparkSession</code> 是 <code>Spark</code> 最新的 <code>SQL</code> 查询起始点，实质上是 <code>SQLContext</code> 和 <code>HiveContext</code> 的组合，所以在 <code>SQLContext</code> 和<code>HiveContext</code> 上可用的 <code>API</code> 在 <code>SparkSession</code> 上同样是可以使用的。</p>
<p><code>SparkSession</code> 内部封装了 <code>SparkContext</code>，所以计算实际上是由 <code>SparkContext</code> 完成的。</p>
<p>当我们使用 <code>spark-shell</code> 客户端的时候， <code>Spark</code> 会自动的创建一个叫做 <code>spark</code> 的 <code>SparkSession</code> 对象，就像我们以前可以自动获取到一个 <code>sc</code> 来表示 <code>SparkContext</code> 对象。</p>
<p><img src="/2021/12/11/Spark-SQL/image-20211217222307261.png" alt="image-20211217222307261"></p>
<hr>
<h2 id="2-2-使用-DataFrame-进行编程"><a href="#2-2-使用-DataFrame-进行编程" class="headerlink" title="2.2    使用 DataFrame 进行编程"></a>2.2    使用 DataFrame 进行编程</h2><p>首先学习 <code>DataFrame</code> 相关的知识。</p>
<p><code>Spark SQL</code> 的 <code>DataFrame API</code> 允许我们使用 <code>DataFrame</code> 而不用必须去注册临时表或者生成 <code>SQL</code> 表达式。<br><code>DataFrame API</code> 既有 <code>transformation</code> 操作也有 <code>action</code> 操作。<code>DataFrame</code> 的转换从本质上来说更具有关系，而 <code>DataSet API</code> 提供了更加函数式的 <code>API</code>。</p>
<hr>
<h3 id="2-2-1-创建-DataFrame"><a href="#2-2-1-创建-DataFrame" class="headerlink" title="2.2.1    创建 DataFrame"></a>2.2.1    创建 DataFrame</h3><p>在得到了 <code>SparkSession</code> 对象后，通过 <code>SparkSession</code> 有 2 种方式来创建 <code>DataFrame</code>：</p>
<ol>
<li> 加载数据源创建 <code>DataFrame</code>。常见的数据源有：<code>JDBC、json、scala集合、Hive 等等</code></li>
<li> 通过已知的 <code>RDD</code> 得到 <code>DataFrame</code></li>
</ol>
<hr>
<h4 id="2-2-1-1-通过数据源创建-DataFrame"><a href="#2-2-1-1-通过数据源创建-DataFrame" class="headerlink" title="2.2.1.1    通过数据源创建 DataFrame"></a>2.2.1.1    通过数据源创建 DataFrame</h4><p><code>Spark</code> 支持的数据源：</p>
<p><img src="/2021/12/11/Spark-SQL/image-20211217095851402.png" alt="image-20211217095851402"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取 json 文件</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/employees.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, salary: bigint]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 展示结果</span></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+-------+------+</span><br><span class="line">|   name|salary|</span><br><span class="line">+-------+------+</span><br><span class="line">|<span class="type">Michael</span>|  <span class="number">3000</span>|</span><br><span class="line">|   <span class="type">Andy</span>|  <span class="number">4500</span>|</span><br><span class="line">| <span class="type">Justin</span>|  <span class="number">3500</span>|</span><br><span class="line">|  <span class="type">Berta</span>|  <span class="number">4000</span>|</span><br><span class="line">+-------+------+</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="2-2-1-2-通过-RDD-进行转换"><a href="#2-2-1-2-通过-RDD-进行转换" class="headerlink" title="2.2.1.2    通过 RDD 进行转换"></a>2.2.1.2    通过 RDD 进行转换</h4><p>后面章节专门讨论</p>
<hr>
<h4 id="2-2-1-3-通过查询-Hive-表创建"><a href="#2-2-1-3-通过查询-Hive-表创建" class="headerlink" title="2.2.1.3    通过查询 Hive 表创建"></a>2.2.1.3    通过查询 Hive 表创建</h4><p>后面章节专门讨论</p>
<hr>
<h3 id="2-2-2-DataFrame-语法风格"><a href="#2-2-2-DataFrame-语法风格" class="headerlink" title="2.2.2    DataFrame 语法风格"></a>2.2.2    DataFrame 语法风格</h3><h4 id="2-2-2-1-SQL-语法风格（重要）"><a href="#2-2-2-1-SQL-语法风格（重要）" class="headerlink" title="2.2.2.1    SQL 语法风格（重要）"></a>2.2.2.1    SQL 语法风格（重要）</h4><p><code>SQL</code> 语法风格是指我们查询数据的时候使用 <code>SQL</code> 语句来查询。</p>
<ul>
<li>  <strong>这种风格的查询必须要有临时视图或者全局视图来辅助</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载指定数据源，创建 DataFrom 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个临时视图（临时表）</span></span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)	</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行查询</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from people&quot;</span>).show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<ul>
<li>  临时视图只能在当前 <code>Session</code> 有效，在新的 <code>Session</code> 中无效。</li>
<li>  可以创建全局视图。访问全局视图需要全路径，如：<code>global_temp.xxx</code></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载指定数据源，创建 DataFrom 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建全局视图</span></span><br><span class="line">scala&gt; df.createGlobalTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行查询</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from global_temp.people&quot;</span>)</span><br><span class="line">res31: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; res31.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line"><span class="comment">// 全局视图在新的 session 中同样生效</span></span><br><span class="line">scala&gt; spark.newSession.sql(<span class="string">&quot;select * from global_temp.people&quot;</span>)</span><br><span class="line">res33: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; res33.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure>





<hr>
<h4 id="2-2-2-2-DSL-语法风格（了解）"><a href="#2-2-2-2-DSL-语法风格（了解）" class="headerlink" title="2.2.2.2    DSL 语法风格（了解）"></a>2.2.2.2    DSL 语法风格（了解）</h4><ul>
<li>  <code>DataFrame</code> 提供一个特定领域语言（<code>domain-specific language，DSL</code>）去管理结构化的数据。可以在 <code>Scala</code>、 <code>Java</code>、 <code>Python</code> 和 <code>R</code> 中使用 <code>DSL</code>。</li>
<li>  使用 <code>DSL</code> 语法风格就不必创建临时视图了。</li>
</ul>
<h5 id="1-查看-Schema-信息"><a href="#1-查看-Schema-信息" class="headerlink" title="1    查看 Schema 信息"></a>1    查看 Schema 信息</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载指定数据源，创建 DataFrom 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查看df数据集的元数据信息（字段信息）</span></span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line">|-- age: long (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- name: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>



<hr>
<h5 id="2-使用-DSL-查询"><a href="#2-使用-DSL-查询" class="headerlink" title="2    使用 DSL 查询"></a>2    使用 DSL 查询</h5><ol>
<li><p>只查询 <code>name</code> 列数据</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select($<span class="string">&quot;name&quot;</span>).show</span><br><span class="line">+-------+</span><br><span class="line">|   name|</span><br><span class="line">+-------+</span><br><span class="line">|<span class="type">Michael</span>|</span><br><span class="line">|   <span class="type">Andy</span>|</span><br><span class="line">| <span class="type">Justin</span>|</span><br><span class="line">+-------+</span><br><span class="line"></span><br><span class="line">scala&gt; df.select(<span class="string">&quot;name&quot;</span>).show</span><br><span class="line">+-------+</span><br><span class="line">|   name|</span><br><span class="line">+-------+</span><br><span class="line">|<span class="type">Michael</span>|</span><br><span class="line">|   <span class="type">Andy</span>|</span><br><span class="line">| <span class="type">Justin</span>|</span><br><span class="line">+-------+</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询 <code>name</code> 和 <code>age</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).show</span><br><span class="line">+-------+----+</span><br><span class="line">|   name| age|</span><br><span class="line">+-------+----+</span><br><span class="line">|<span class="type">Michael</span>|<span class="literal">null</span>|</span><br><span class="line">|   <span class="type">Andy</span>|  <span class="number">30</span>|</span><br><span class="line">| <span class="type">Justin</span>|  <span class="number">19</span>|</span><br><span class="line">+-------+----+</span><br></pre></td></tr></table></figure>

</li>
<li><p>查询 <code>name </code>和 <code>age + 1</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select($<span class="string">&quot;name&quot;</span>, $<span class="string">&quot;age&quot;</span> + <span class="number">1</span>).show</span><br><span class="line">+-------+---------+</span><br><span class="line">|   name|(age + <span class="number">1</span>)|</span><br><span class="line">+-------+---------+</span><br><span class="line">|<span class="type">Michael</span>|     <span class="literal">null</span>|</span><br><span class="line">|   <span class="type">Andy</span>|       <span class="number">31</span>|</span><br><span class="line">| <span class="type">Justin</span>|       <span class="number">20</span>|</span><br><span class="line">+-------+---------+</span><br></pre></td></tr></table></figure></li>
<li><p>   <strong>注意：涉及到运算的时候，每列都必须使用 <code>$</code>。</strong></p>
</li>
<li><p>查询 <code>age &gt; 20</code> 的数据</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.filter($<span class="string">&quot;age&quot;</span> &gt; <span class="number">21</span>).show</span><br><span class="line">+---+----+</span><br><span class="line">|age|name|</span><br><span class="line">+---+----+</span><br><span class="line">| <span class="number">30</span>|<span class="type">Andy</span>|</span><br><span class="line">+---+----+</span><br></pre></td></tr></table></figure></li>
<li><p>按照 <code>age</code> 分组，查看数据条数</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.groupBy(<span class="string">&quot;age&quot;</span>).count.show</span><br><span class="line">+----+-----+</span><br><span class="line">| age|count|</span><br><span class="line">+----+-----+</span><br><span class="line">|  <span class="number">19</span>|    <span class="number">1</span>|</span><br><span class="line">|<span class="literal">null</span>|    <span class="number">1</span>|</span><br><span class="line">|  <span class="number">30</span>|    <span class="number">1</span>|</span><br><span class="line">+----+-----+</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="2-2-3-RDD-和-DataFrame-的交互"><a href="#2-2-3-RDD-和-DataFrame-的交互" class="headerlink" title="2.2.3    RDD 和 DataFrame 的交互"></a>2.2.3    RDD 和 DataFrame 的交互</h3><h4 id="1、RDD-gt-DataFrame"><a href="#1、RDD-gt-DataFrame" class="headerlink" title="1、RDD =&gt; DataFrame"></a>1、<code>RDD =&gt; DataFrame</code></h4><p>涉及到 <code>RDD，DataFrame，DataSet</code> 之间的操作时，需要先导入 <code>import spark.implicits._</code>，这里的 <code>spark</code> 不是包名，而是表示 <code>SparkSession</code> 对象。所以必须先创建 <code>SparkSession</code> 对象再导入，<code>implicits</code> 是一个内部 <code>object</code>。</p>
<ol>
<li><p>首先创建一个 <code>RDD</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.txt&quot;</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = /opt/module/spark-local/examples/src/main/resources/people.txt <span class="type">MapPartitionsRDD</span>[<span class="number">10</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h5 id="手动转换"><a href="#手动转换" class="headerlink" title="手动转换"></a>手动转换</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = rdd1.map(line =&gt; &#123; <span class="keyword">val</span> paras = line.split(<span class="string">&quot;, &quot;</span>); (paras(<span class="number">0</span>), paras(<span class="number">1</span>).toInt)&#125;)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">11</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 RDD 转换为 DataFrame。转换的时候需要手动指定每个数据对应的字段名</span></span><br><span class="line">scala&gt; rdd2.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).show</span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|<span class="type">Michael</span>| <span class="number">29</span>|</span><br><span class="line">|   <span class="type">Andy</span>| <span class="number">30</span>|</span><br><span class="line">| <span class="type">Justin</span>| <span class="number">19</span>|</span><br><span class="line">+-------+---+</span><br></pre></td></tr></table></figure>





<h5 id="通过样例类的反射机制转换（常用）"><a href="#通过样例类的反射机制转换（常用）" class="headerlink" title="通过样例类的反射机制转换（常用）"></a>通过样例类的反射机制转换（常用）</h5><ol>
<li><p>创建样例类</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name :<span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">People</span></span></span><br></pre></td></tr></table></figure>

</li>
<li><p>使用样例把 <code>RDD</code> 转换成 <code>DataFrame</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = rdd1.map(line =&gt; &#123; <span class="keyword">val</span> paras = line.split(<span class="string">&quot;, &quot;</span>); <span class="type">People</span>(paras(<span class="number">0</span>), paras(<span class="number">1</span>).toInt) &#125;)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">People</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at map at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.toDF.show</span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|<span class="type">Michael</span>| <span class="number">29</span>|</span><br><span class="line">|   <span class="type">Andy</span>| <span class="number">30</span>|</span><br><span class="line">| <span class="type">Justin</span>| <span class="number">19</span>|</span><br><span class="line">+-------+---+</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h5 id="通过-API-的方式转换（了解）"><a href="#通过-API-的方式转换（了解）" class="headerlink" title="通过 API 的方式转换（了解）"></a>通过 API 的方式转换（了解）</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">IntegerType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataFrameDemo2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">        .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        .appName(<span class="string">&quot;RDD2DF&quot;</span>)</span><br><span class="line">        .getOrCreate()</span><br><span class="line">        <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;lisi&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;zs&quot;</span>, <span class="number">20</span>), (<span class="string">&quot;zhiling&quot;</span>, <span class="number">40</span>)))</span><br><span class="line">        <span class="comment">// 映射出来一个 RDD[Row], 因为 DataFrame其实就是 DataSet[Row]</span></span><br><span class="line">        <span class="keyword">val</span> rowRdd: <span class="type">RDD</span>[<span class="type">Row</span>] = rdd.map(x =&gt; <span class="type">Row</span>(x._1, x._2))</span><br><span class="line">        <span class="comment">// 创建 StructType 类型：用于指定 ROW 中每列的列名和列类型</span></span><br><span class="line">        <span class="keyword">val</span> types = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;name&quot;</span>, <span class="type">StringType</span>), <span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>, <span class="type">IntegerType</span>)))</span><br><span class="line">        <span class="comment">// 创建 DF 对象</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.createDataFrame(rowRdd, types)</span><br><span class="line">        df.show</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="2、从-DataFrame-到-RDD"><a href="#2、从-DataFrame-到-RDD" class="headerlink" title="2、从 DataFrame 到 RDD"></a>2、从 DataFrame 到 RDD</h4><p>从 <code>DataFrame</code> 到 <code>RDD</code>，直接调用 <code>DataFrame</code> 的 <code>rdd</code> 方法就可以完成转换。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1、加载数据源，创建 DataFrame 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2、DataFrame =&gt; RDD</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = df.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at rdd at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res0: <span class="type">Array</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">Array</span>([<span class="literal">null</span>,<span class="type">Michael</span>], [<span class="number">30</span>,<span class="type">Andy</span>], [<span class="number">19</span>,<span class="type">Justin</span>])</span><br></pre></td></tr></table></figure>

<ul>
<li>  <strong>说明：</strong>转换后得到的 <code>RDD</code> 对象中存储的数据类型是 <code>Row</code>。</li>
</ul>
<hr>
<h2 id="2-3-使用-DataSet-进行编程"><a href="#2-3-使用-DataSet-进行编程" class="headerlink" title="2.3    使用 DataSet 进行编程"></a>2.3    使用 DataSet 进行编程</h2><p><code>DataSet</code> 和 <code>RDD</code> 类似，但是 <code>DataSet</code> 没有使用 <code>Java</code> 序列化或者 <code>Kryo</code> 序列化，而是使用了一种专门的编码器去序列化对象，然后在网络上传输。</p>
<p>虽然编码器和标准序列化都负责将对象转换成字节，但编码器是动态生成的代码，使用的格式允许 <code>Spark</code> 执行许多操作，如过滤、排序和哈希，而无需将字节反序列化回对象。</p>
<p><strong><code>DataSet</code> 是一种强类型的数据集合，需要提供对应的类型信息</strong>。而 <code>DataFrame</code> 是一种弱类型的数据集合，<code>DF</code> 中只能存储 <code>ROW</code> 类型的数据，至于 <code>ROW</code> 的具体细节 <code>DF</code> 是不知道的。</p>
<hr>
<h3 id="2-3-1-创建-DataSet"><a href="#2-3-1-创建-DataSet" class="headerlink" title="2.3.1    创建 DataSet"></a>2.3.1    创建 DataSet</h3><blockquote>
<ol>
<li> 通过 Scala 序列（集合）得到</li>
<li> 通过 RDD 转换得到</li>
<li> 通过 DF 转换得到</li>
<li> 通过 DS 转换得到新的 DS</li>
</ol>
</blockquote>
<h4 id="1、通过-Scala-序列（集合）得到"><a href="#1、通过-Scala-序列（集合）得到" class="headerlink" title="1、通过 Scala 序列（集合）得到"></a>1、通过 Scala 序列（集合）得到</h4><p><strong>Demo01</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 13:26</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CreateDS</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 Scala 序列</span></span><br><span class="line">    <span class="keyword">val</span> mylist: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 转换成 DS</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">Int</span>] = mylist.toDS()</span><br><span class="line">    <span class="comment">// 转换成 DF</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = mylist.toDF()</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 显然，将一个集合转换成 DS 后，我们可以通过解析样例类得到其数据的具体类型；</span></span><br><span class="line"><span class="comment">    * 而将集合转换为 DF 后，虽然我们同样可以得到这份数据，但是无法识别数据每个字段的真实类型，</span></span><br><span class="line"><span class="comment">    * 所有的数据都是 ROW 类型的。</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ds.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">+-----+</span></span><br><span class="line"><span class="comment">|value|</span></span><br><span class="line"><span class="comment">+-----+</span></span><br><span class="line"><span class="comment">|   10|</span></span><br><span class="line"><span class="comment">|   20|</span></span><br><span class="line"><span class="comment">|   30|</span></span><br><span class="line"><span class="comment">+-----+</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">我们可以发现，我们没有指定列名，Spark 自动用 value 作为列名了。</span></span><br><span class="line"><span class="comment">如果我们想要使用自己的列名，则可以使用样例类</span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br></pre></td></tr></table></figure>



<p><strong>Demo02</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 13:26</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CreateDS2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 Scala 序列</span></span><br><span class="line">    <span class="keyword">val</span> persons: <span class="type">List</span>[<span class="type">Person</span>] = <span class="type">List</span>(<span class="type">Person</span>(<span class="string">&quot;张三&quot;</span>, <span class="number">20</span>), <span class="type">Person</span>(<span class="string">&quot;李四&quot;</span>, <span class="number">30</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 转换成 DS</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">Person</span>] = persons.toDS()</span><br><span class="line">    <span class="comment">// 转换成 DF</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = persons.toDF()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ds.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">+----+---+</span></span><br><span class="line"><span class="comment">|name|age|</span></span><br><span class="line"><span class="comment">+----+---+</span></span><br><span class="line"><span class="comment">|  张三| 20|</span></span><br><span class="line"><span class="comment">|  李四| 30|</span></span><br><span class="line"><span class="comment">+----+---+</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">临时表的列名使用的是样例类中的属性</span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br></pre></td></tr></table></figure>



<ol>
<li><p>使用样例类的序列（集合）得到 <code>DataSet</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个样例类</span></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 为样例类创建一个编码器</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;lisi&quot;</span>, <span class="number">20</span>), <span class="type">Person</span>(<span class="string">&quot;zs&quot;</span>, <span class="number">21</span>)).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: int]</span><br><span class="line"></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">scala&gt; ds.show</span><br><span class="line">+----+---+</span><br><span class="line">|name|age|</span><br><span class="line">+----+---+</span><br><span class="line">|lisi| <span class="number">20</span>|</span><br><span class="line">| zs| <span class="number">21</span>|</span><br><span class="line">+----+---+</span><br></pre></td></tr></table></figure>

</li>
<li><p>使用基本类型的序列（集合）得到 <code>DataSet</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 基本类型的编码被自动创建. importing spark.implicits._</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Int</span>] = [value: int]</span><br><span class="line"></span><br><span class="line"><span class="comment">// </span></span><br><span class="line">scala&gt; ds.show</span><br><span class="line">+-----+</span><br><span class="line">|value|</span><br><span class="line">+-----+</span><br><span class="line">| <span class="number">1</span>|</span><br><span class="line">| <span class="number">2</span>|</span><br><span class="line">| <span class="number">3</span>|</span><br><span class="line">| <span class="number">4</span>|</span><br><span class="line">| <span class="number">5</span>|</span><br><span class="line">| <span class="number">6</span>|</span><br><span class="line">+-----+</span><br></pre></td></tr></table></figure>

<p>   说明：在实际使用的时候，很少用到把序列转换成 <code>DataSet</code>，更多的是通过 <code>RDD</code> 转换成 <code>DataSet</code>。</p>
</li>
</ol>
<hr>
<h3 id="2-3-2-RDD-和-DataSet-的交互"><a href="#2-3-2-RDD-和-DataSet-的交互" class="headerlink" title="2.3.2    RDD 和 DataSet 的交互"></a>2.3.2    RDD 和 DataSet 的交互</h3><h4 id="1、RDD-gt-DataSet"><a href="#1、RDD-gt-DataSet" class="headerlink" title="1、RDD =&gt; DataSet"></a>1、<code>RDD =&gt; DataSet</code></h4><p>使用反射来推断包含特定类型对象的 RDD 的 <code>schema</code>。</p>
<p>这种基于反射的方法可以生成更简洁的代码，并且当您在编写Spark应用程序时已经知道模式时，这种方法可以很好地工作。</p>
<p>为 <code>Spark SQL</code> 设计的 <code>Scala API</code> 可以自动的把包含样例类的 RDD 转换成 <code>DataSet</code>。</p>
<p>样例类定义了 <code>schema(表结构)</code>：样例类参数名通过反射被读到，然后成为列名。</p>
<p>样例类可以被嵌套，也可以包含复杂类型：像 <code>Seq</code> 或者 <code>Array</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> peopleRDD = sc.textFile(<span class="string">&quot;examples/src/main/resources/people.txt&quot;</span>)</span><br><span class="line">peopleRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = examples/src/main/resources/people.txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; peopleRDD.map(line =&gt; &#123;<span class="keyword">val</span> para = line.split(<span class="string">&quot;,&quot;</span>);<span class="type">Person</span>(para(<span class="number">0</span>),para(<span class="number">1</span>).trim.toInt)&#125;).toDS</span><br><span class="line">res0: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint]</span><br></pre></td></tr></table></figure>



<h4 id="2、DataSet-gt-RDD"><a href="#2、DataSet-gt-RDD" class="headerlink" title="2、DataSet =&gt; RDD"></a>2、<code>DataSet =&gt; RDD</code></h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">调用rdd方法即可</span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;lisi&quot;</span>, <span class="number">40</span>), <span class="type">Person</span>(<span class="string">&quot;zs&quot;</span>, <span class="number">20</span>)).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 把 ds 转换成 rdd</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = ds.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Person</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">8</span>] at rdd at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res5: <span class="type">Array</span>[<span class="type">Person</span>] = <span class="type">Array</span>(<span class="type">Person</span>(lisi,<span class="number">40</span>), <span class="type">Person</span>(zs,<span class="number">20</span>))</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="2-4-DataFrame-和-DataSet-之间的交互"><a href="#2-4-DataFrame-和-DataSet-之间的交互" class="headerlink" title="2.4    DataFrame 和 DataSet 之间的交互"></a>2.4    DataFrame 和 DataSet 之间的交互</h2><h3 id="2-4-1-DataFrame-gt-DataSet"><a href="#2-4-1-DataFrame-gt-DataSet" class="headerlink" title="2.4.1    DataFrame =&gt; DataSet"></a>2.4.1    <code>DataFrame =&gt; DataSet</code></h3><p><strong>spark-shell</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">People</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrame 转换成 DataSet</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = df.as[<span class="type">People</span>]</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">People</span>] = [age: bigint, name: string]</span><br></pre></td></tr></table></figure>

<p><strong>IDEA</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 13:49</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DF2DS</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、通过 SparkSession 创建 DataFrame 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 我们知道，相比于 DF 而言，DS 可以查看其内部保存数据的具体类型，</span></span><br><span class="line"><span class="comment">    * 而 DF 则是将所有的类型都用 ROW 类型来处理。</span></span><br><span class="line"><span class="comment">    * 所以如果想要实现 DF =&gt; DS 的转换，一般需要提供一个样例类用于将 ROW 类型的数据映射到样例类上</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="comment">// 5、DF =&gt; DS</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User</span>] = df.as[<span class="type">User</span>]</span><br><span class="line"></span><br><span class="line">    ds.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6、关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4、样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">  +---+----+</span></span><br><span class="line"><span class="comment">  |age|name|</span></span><br><span class="line"><span class="comment">  +---+----+</span></span><br><span class="line"><span class="comment">  | 15|  ls|</span></span><br><span class="line"><span class="comment">  | 21|  ww|</span></span><br><span class="line"><span class="comment">  | 22|  zs|</span></span><br><span class="line"><span class="comment">  | 23|  zl|</span></span><br><span class="line"><span class="comment">  +---+----+</span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br></pre></td></tr></table></figure>





<h3 id="2-4-2-DataSet-gt-DataFrame"><a href="#2-4-2-DataSet-gt-DataFrame" class="headerlink" title="2.4.2    DataSet =&gt; DataFrame"></a>2.4.2    <code>DataSet =&gt; DataFrame</code></h3><p><strong>DS =&gt; DF</strong></p>
<ol>
<li> 不需要隐式类型转换</li>
<li> 方法：<code>ds.toDF</code></li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;Andy&quot;</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = ds.toDF</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+----+---+</span><br><span class="line">|name|age|</span><br><span class="line">+----+---+</span><br><span class="line">|<span class="type">Andy</span>| <span class="number">32</span>|</span><br><span class="line">+----+---+</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="2-5-RDD-DataFrame-和-DataSet-之间的关系"><a href="#2-5-RDD-DataFrame-和-DataSet-之间的关系" class="headerlink" title="2.5    RDD, DataFrame 和 DataSet 之间的关系"></a>2.5    RDD, DataFrame 和 DataSet 之间的关系</h2><p>在 <code>SparkSQL</code> 为我们提供了两个新的抽象，分别是 <code>DataFrame</code> 和 <code>DataSet</code>。它们和 <code>RDD</code> 有什么区别呢？首先从版本的产生上来看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)</span><br></pre></td></tr></table></figure>

<p>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。</p>
<p>在后期的 <code>Spark</code> 版本中， <code>DataSet</code> 会逐步取代 <code>RDD</code> 和 <code>DataFrame</code> 成为唯一的 <code>API</code> 接口。</p>
<h3 id="2-5-1-三者的共性"><a href="#2-5-1-三者的共性" class="headerlink" title="2.5.1    三者的共性"></a>2.5.1    三者的共性</h3><ol>
<li>   <code>RDD</code>、 <code>DataFrame</code>、 <code>Dataset</code> 全都是 <code>Spark</code> 平台下的分布式弹性数据集，为处理超大型数据提供便利</li>
<li>   三者都有惰性机制，在进行创建、转换，如 <code>map</code> 方法时，不会立即执行，只有在遇到 <code>action</code> 时，三者才会开始遍历运算。</li>
<li>   三者都会根据 <code>Spark</code> 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</li>
<li>   三者都有 <code>partition</code> 的概念</li>
<li>   三者有许多共同的函数，如 <code>map</code>， <code>filter</code>，排序等</li>
<li>   在对 <code>DataFrame</code> 和 <code>Dataset</code> 进行操作许多操作都需要这个包进行支持 <code>import [spark].implicits._</code></li>
<li>   <code>DataFrame</code> 和 <code>Dataset</code> 均可使用模式匹配获取各个字段的值和类型</li>
</ol>
<h3 id="2-5-2-三者的区别"><a href="#2-5-2-三者的区别" class="headerlink" title="2.5.2    三者的区别"></a>2.5.2    三者的区别</h3><h4 id="2-5-2-1-RDD"><a href="#2-5-2-1-RDD" class="headerlink" title="2.5.2.1    RDD"></a>2.5.2.1    RDD</h4><ol>
<li>   <code>RDD</code> 一般和 <code>spark mllib</code> 同时使用</li>
<li>   <code>RDD</code> 不支持 <code>spark sql</code> 操作</li>
</ol>
<h4 id="2-5-2-2-DataFrame"><a href="#2-5-2-2-DataFrame" class="headerlink" title="2.5.2.2    DataFrame"></a>2.5.2.2    DataFrame</h4><ol>
<li>   与 <code>RDD</code> 和 <code>Dataset</code> 不同，<code>DataFrame</code>每一行的类型固定为 <code>Row</code>，每一列的值没法直接访问，只有通过解析才能获取各个字段的值</li>
<li>   <code>DataFrame</code> 与 <code>DataSet</code> 一般不与 <code>spark mlib</code> 同时使用</li>
<li>   <code>DataFrame</code> 与 <code>DataSet</code> 均支持 <code>SparkSQL</code> 的操作，比如 <code>select</code>， <code>groupby</code> 之类，还能注册临时表/视窗，进行 <code>sql</code> 语句操作</li>
<li>   <code>DataFrame</code> 与 <code>DataSet</code> 支持一些特别方便的保存方式，比如保存成 <code>csv</code>，可以带上表头，这样每一列的字段名一目了然（后面专门讲解）</li>
</ol>
<h4 id="2-5-2-3-DataSet"><a href="#2-5-2-3-DataSet" class="headerlink" title="2.5.2.3    DataSet"></a>2.5.2.3    DataSet</h4><ol>
<li>   <code>Dataset</code> 和 <code>DataFrame</code> 拥有完全相同的成员函数，区别只是每一行的数据类型不同。</li>
<li>   <code>DataFrame</code> 其实就是 <code>DataSet</code> 的一个特例</li>
<li>   <code>DataFrame</code> 也可以叫 <code>Dataset[Row]</code>，每一行的类型是 <code>Row</code>，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 <code>getAS</code> 方法或者共性中的第七条提到的模式匹配拿出特定字段。而 <code>Dataset</code> 中，每一行是什么类型是不一定的，在自定义了 <code>case class</code> 之后可以很自由的获得每一行的信息</li>
</ol>
<h3 id="2-5-3-三者的互相转换"><a href="#2-5-3-三者的互相转换" class="headerlink" title="2.5.3    三者的互相转换"></a>2.5.3    三者的互相转换</h3><ul>
<li><code>基础类型 =&gt; 高级类型</code>：<ol>
<li> 需要提供样例类</li>
<li> 需要导入隐式转换</li>
</ol>
</li>
<li><code>高级类型 =&gt; 基础类型</code>：<ol>
<li> 无需提供额外的信息，可以直接完成类型之间的转换</li>
</ol>
</li>
</ul>
<p><img src="/2021/12/11/Spark-SQL/image-20211221135048075.png" alt="image-20211221135048075"></p>
<hr>
<h2 id="2-6-使用-IDEA-创建-SparkSQL-程序"><a href="#2-6-使用-IDEA-创建-SparkSQL-程序" class="headerlink" title="2.6    使用 IDEA 创建 SparkSQL 程序"></a>2.6    使用 IDEA 创建 SparkSQL 程序</h2><h3 id="步骤1：添加-SparkSQL-依赖"><a href="#步骤1：添加-SparkSQL-依赖" class="headerlink" title="步骤1：添加 SparkSQL 依赖"></a>步骤1：添加 SparkSQL 依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="步骤2：具体代码"><a href="#步骤2：具体代码" class="headerlink" title="步骤2：具体代码"></a>步骤2：具体代码</h3><p><code>user.json</code></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;ls&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span>  <span class="number">15</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;ww&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span>  <span class="number">21</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;zs&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span>  <span class="number">22</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;zl&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span>  <span class="number">23</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>



<h4 id="1、创建-DF-对象"><a href="#1、创建-DF-对象" class="headerlink" title="1、创建 DF 对象"></a>1、创建 DF 对象</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/22 9:57</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 创建 DF 对象</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CreateDF</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 1、先创建一个 SparkSession 对象【构造器模式】</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;Word Count&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、通过 SparkSession 创建 DataFrame 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、对 DF 做各种操作</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建临时表</span></span><br><span class="line">    df.createTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">    <span class="comment">// 查询临时表</span></span><br><span class="line">    spark.sql(<span class="string">&quot;select * from user where age &gt; 19&quot;</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、关闭连接</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>Tip：</strong>在导入隐式转换时，使用到的 <code>spark</code> 关键字是由 <code>SparkSession</code> 的实例对象名来决定的，也就是说，如果我们创建的 <code>SparkSession</code> 实例名字叫 <code>hello</code>，那么在导入隐式转换时就应该导入 <code>import hello.implicits._</code></p>
<p><img src="/2021/12/11/Spark-SQL/image-20211224130331108.png" alt="image-20211224130331108"></p>
<h4 id="2、DF-gt-RDD"><a href="#2、DF-gt-RDD" class="headerlink" title="2、DF =&gt; RDD"></a>2、<code>DF =&gt; RDD</code></h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/22 10:23</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DF2RDD</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;Word Count&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个 sc 对象</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 得到一个 DF 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = (<span class="number">1</span> to <span class="number">10</span>).toDF(<span class="string">&quot;num&quot;</span>)</span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// df =&gt; rdd</span></span><br><span class="line">    <span class="comment">// 由 df 转换的来的 rdd 的泛型一定是 [Row]</span></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Row</span>] = df.rdd</span><br><span class="line">    rdd.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 取出 rdd 中每个 row 中的数据</span></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.map(row =&gt; row.getInt(<span class="number">0</span>))</span><br><span class="line">    rdd2.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h4 id="3、RDD-gt-DF"><a href="#3、RDD-gt-DF" class="headerlink" title="3、RDD =&gt; DF"></a>3、<code>RDD =&gt; DF</code></h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/22 10:08</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDD2DF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、先创建一个 SparkSession 对象【构造器模式】</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;Word Count&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、创建一个 sc 对象</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、导入用到的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、如果 RDD 中的数据保存在元组中，则在执行 toDF 方法时需要显式指定每列的名称</span></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.parallelize((<span class="string">&quot;ls&quot;</span>, <span class="number">10</span>) :: (<span class="string">&quot;zs&quot;</span>, <span class="number">20</span>) :: <span class="type">Nil</span>)</span><br><span class="line">    rdd.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5、如果 RDD 中保存的数据是一个样例类对象，则在执行 toDF 方法时无需手动指定每列的名称就可完成自动解析，</span></span><br><span class="line">    <span class="comment">// 因为样例中已经有了属性做列名</span></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">User</span>] = sc.parallelize(<span class="type">Array</span>(<span class="type">User</span>(<span class="string">&quot;路飞&quot;</span>, <span class="number">10</span>), <span class="type">User</span>(<span class="string">&quot;黄猿&quot;</span>, <span class="number">53</span>)))</span><br><span class="line">    rdd2.toDF().show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6、关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="2-7-自定义-SparkSQL-函数"><a href="#2-7-自定义-SparkSQL-函数" class="headerlink" title="2.7    自定义 SparkSQL 函数"></a>2.7    自定义 SparkSQL 函数</h2><p>在 <code>Shell</code> 窗口中可以通过 <code>spark.udf</code> 功能自定义函数。</p>
<hr>
<h3 id="2-7-1-自定义-UDF-函数"><a href="#2-7-1-自定义-UDF-函数" class="headerlink" title="2.7.1    自定义 UDF 函数"></a>2.7.1    自定义 UDF 函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个 udf 函数: toUpper是函数名, 第二个参数是函数的具体实现</span></span><br><span class="line">scala&gt; spark.udf.register(<span class="string">&quot;toUpper&quot;</span>, (s: <span class="type">String</span>) =&gt; s.toUpperCase)</span><br><span class="line">res1: org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span> = <span class="type">UserDefinedFunction</span>(&lt;function1&gt;,<span class="type">StringType</span>,<span class="type">Some</span>(<span class="type">List</span>(<span class="type">StringType</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 测试自定义的 UDF 函数</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select toUpper(name), age from people&quot;</span>).show</span><br><span class="line">+-----------------+----+</span><br><span class="line">|<span class="type">UDF</span>:toUpper(name)| age|</span><br><span class="line">+-----------------+----+</span><br><span class="line">|          <span class="type">MICHAEL</span>|<span class="literal">null</span>|</span><br><span class="line">|             <span class="type">ANDY</span>|  <span class="number">30</span>|</span><br><span class="line">|           <span class="type">JUSTIN</span>|  <span class="number">19</span>|</span><br><span class="line">+-----------------+----+</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="2-7-2-用户自定义聚合函数-UDAF"><a href="#2-7-2-用户自定义聚合函数-UDAF" class="headerlink" title="2.7.2    用户自定义聚合函数(UDAF)"></a>2.7.2    用户自定义聚合函数(UDAF)</h3><p>强类型的 <code>Dataset</code> 和弱类型的 <code>DataFrame</code> 都提供了相关的聚合函数，如 <code>count()</code>， <code>countDistinct()</code>， <code>avg()</code>， <code>max()</code>， <code>min()</code>。除此之外，用户还可以设定自己的自定义聚合函数。</p>
<p>自定义聚合函数需要继承 <code>UserDefinedAggregateFunction</code></p>
<p><strong>自定义聚合函数实现 sum</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataType</span>, <span class="type">DoubleType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 14:18</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *          自定义聚合函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UDAFDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 DF 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建临时视图</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册自定义函数</span></span><br><span class="line">    spark.udf.register(<span class="string">&quot;mySum&quot;</span>, <span class="keyword">new</span> <span class="type">CustomSum</span>())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用自定义的求和函数 mySum 执行聚合查询</span></span><br><span class="line">    spark.sql(<span class="string">&quot;select mySum(age) from user&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">  +------------------------------+</span></span><br><span class="line"><span class="comment">  |customsum(CAST(age AS DOUBLE))|</span></span><br><span class="line"><span class="comment">  +------------------------------+</span></span><br><span class="line"><span class="comment">  |                          81.0|</span></span><br><span class="line"><span class="comment">  +------------------------------+</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义 UDAF 函数</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 1、自定义 UDAF 函数需要继承 UserDefinedAggregateFunction 类</span></span><br><span class="line"><span class="comment"> * 2、CustomSum 函数的功能等价于 Sum</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomSum</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定输入的类型。</span></span><br><span class="line"><span class="comment">  * 比如，在 select mySum(age) from User 中，age 的类型就是输入类型</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="comment">// 在自定义函数 mySum(age) 中，只有一个参数，且指定类型为 Double</span></span><br><span class="line">    <span class="comment">// 如果有多个参数，则需要指定多个 StructField</span></span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定缓冲区数据的类型。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 在计算聚合函数时（以 sum 为例），需要先计算出前两数之和，将临时结果放入缓冲区，</span></span><br><span class="line"><span class="comment">  * 再使用缓冲区中的结果与下一个数进行求和。就类似于 flap</span></span><br><span class="line"><span class="comment">  * 缓冲区的类型也需要手动指定</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;tmp&quot;</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定聚合函数执行结束后，最终返回结果的类型</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = &#123;</span><br><span class="line">    <span class="type">DoubleType</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定查询结果是否具有幂等性</span></span><br><span class="line"><span class="comment">  * （即多次查询是否能得到统一结果）</span></span><br><span class="line"><span class="comment">  * 一般都设置为 true</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 初始化缓冲区</span></span><br><span class="line"><span class="comment">  *   对于求和函数而言，初始化值应该为 0.00</span></span><br><span class="line"><span class="comment">  *   并且由于缓冲区可能不只缓冲一个字段，所以缓冲区的数据结构为可变集合</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// buffer 就是缓冲区中数据的集合</span></span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>D</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 分区内聚合：</span></span><br><span class="line"><span class="comment">   * 由于 RDD 是分布式数据集，所以聚合又可分为分区内聚合和分区间聚合</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param buffer 缓冲区内的值</span></span><br><span class="line"><span class="comment">   * @param input 传入缓冲区中的值</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 分区内求和</span></span><br><span class="line">    input <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="comment">// 如果收到的参数是 Double 类型，则取出 input 对象中的值，通过模式匹配匹配到 age 中，执行累加</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(age: <span class="type">Double</span>) =&gt; buffer(<span class="number">0</span>) = buffer.getDouble(<span class="number">0</span>) + age</span><br><span class="line">      <span class="comment">// 其它情况</span></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 分区间聚合</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param buffer1 一个分区的聚合结果</span></span><br><span class="line"><span class="comment">   * @param buffer2 另一个分区的聚合结果</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 分区间求和</span></span><br><span class="line">    <span class="comment">// 把 buffer1 和 buffer2 临时结果聚合到一起，然后再把值写回到 buffer1</span></span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getDouble(<span class="number">0</span>) + buffer2.getDouble(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 返回最终的输出值</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer.getDouble(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>自定义聚合函数实现 avg</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> day01.udf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataType</span>, <span class="type">DoubleType</span>, <span class="type">LongType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 14:18</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *          自定义聚合函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UDAFDemo2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 DF 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建临时视图</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册自定义函数</span></span><br><span class="line">    spark.udf.register(<span class="string">&quot;myAvg&quot;</span>, <span class="keyword">new</span> <span class="type">CustomAvg</span>())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用自定义的求和函数 mySum 执行聚合查询</span></span><br><span class="line">    spark.sql(<span class="string">&quot;select myAvg(age) from user&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomAvg</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定输入的类型。</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="comment">// 在自定义函数 myAvg(age) 中，只有一个参数，且指定类型为 Double</span></span><br><span class="line">    <span class="comment">// 如果有多个参数，则需要指定多个 StructField</span></span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定缓冲区数据的类型。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 在计算聚合函数时（以 sum 为例），需要先计算出前两数之和，将临时结果放入缓冲区，</span></span><br><span class="line"><span class="comment">  * 再使用缓冲区中的结果与下一个数进行求和。就类似于 flap</span></span><br><span class="line"><span class="comment">  * 缓冲区的类型也需要手动指定</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="comment">// count 用于计算 User 的个数</span></span><br><span class="line">    <span class="comment">// sum 用于计算 User 中 age 的累加和</span></span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>) :: <span class="type">StructField</span>(<span class="string">&quot;sum&quot;</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定聚合函数执行结束后，最终返回结果的类型</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = &#123;</span><br><span class="line">    <span class="type">DoubleType</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定查询结果是否具有幂等性</span></span><br><span class="line"><span class="comment">  * （即多次查询是否能得到统一结果）</span></span><br><span class="line"><span class="comment">  * 一般都设置为 true</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 初始化缓冲区</span></span><br><span class="line"><span class="comment">  *   对于求和函数而言，初始化值应该为 0.00</span></span><br><span class="line"><span class="comment">  *   并且由于缓冲区可能不只缓冲一个字段，所以缓冲区的数据结构为可变集合</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// buffer 就是缓冲区中数据的集合</span></span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L  <span class="comment">// 缓冲区中 count 的初始值</span></span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>D  <span class="comment">// 缓冲区中 sum 的初始值</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 分区内聚合：</span></span><br><span class="line"><span class="comment">   * 由于 RDD 是分布式数据集，所以聚合又可分为分区内聚合和分区间聚合</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param buffer 缓冲区内的值</span></span><br><span class="line"><span class="comment">   * @param input 传入缓冲区中的值</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 分区内求和</span></span><br><span class="line">    input <span class="keyword">match</span> &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(age: <span class="type">Double</span>) =&gt; &#123;</span><br><span class="line">        buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + <span class="number">1</span>L</span><br><span class="line">        buffer(<span class="number">1</span>) = buffer.getDouble(<span class="number">1</span>) + age</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 其它情况</span></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 分区间聚合</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param buffer1 一个分区的聚合结果</span></span><br><span class="line"><span class="comment">   * @param buffer2 另一个分区的聚合结果</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 分区间求和</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 把 buffer1 和 buffer2 临时结果聚合到一起，然后再把值写回到 buffer1</span></span><br><span class="line">    buffer2 <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(count : <span class="type">Long</span>, age: <span class="type">Double</span>) =&gt; &#123;</span><br><span class="line">        buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + count</span><br><span class="line">        buffer1(<span class="number">1</span>) = buffer1.getDouble(<span class="number">1</span>) + age</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 其它情况</span></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 返回最终的输出值</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer.getDouble(<span class="number">1</span>) / buffer.getLong(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="第-3-章-SparkSQL-数据源"><a href="#第-3-章-SparkSQL-数据源" class="headerlink" title="第 3 章    SparkSQL 数据源"></a>第 3 章    SparkSQL 数据源</h1><p>本章介绍 <code>SparkSQL</code> 支持的各种数据源<code>(Data Sources)</code>。</p>
<p><code>Spark SQL</code> 的 <code>DataFrame</code> 接口支持操作多种数据源。一个 <code>DataFrame</code> 类型的对象可以像 <code>RDD</code> 那样操作（比如各种转换），也可以用来创建临时表。把 <code>DataFrame</code> 注册为一个临时表之后，就可以在它的数据上面执行 <code>SQL</code> 查询。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">读</span><br><span class="line">	通用读：spark.read.format(&quot;文件格式&quot;).load(&quot;文件路径&quot;)</span><br><span class="line">	专用读：spark.read.文件格式(&quot;文件路径&quot;)</span><br><span class="line"></span><br><span class="line">写</span><br><span class="line">	通用写：df.write.format(&quot;文件格式&quot;).save(&quot;文件路径&quot;)</span><br><span class="line">	专用写：df.write.文件格式(&quot;文件路径&quot;)</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="3-1-通用加载和保存函数"><a href="#3-1-通用加载和保存函数" class="headerlink" title="3.1    通用加载和保存函数"></a>3.1    通用加载和保存函数</h2><p>默认数据源格式是 <code>parquet</code>，我们也可以通过使用 <code>spark.sql.sources.default</code> 这个属性来设置默认的数据源格式。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载数据源中的数据到 DF 中</span></span><br><span class="line"><span class="keyword">val</span> usersDF = spark.read.load(<span class="string">&quot;examples/src/main/resources/users.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DF 中的数据写出到目的地</span></span><br><span class="line">usersDF.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;favorite_color&quot;</span>).write.save(<span class="string">&quot;namesAndFavColors.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>说明：</strong></p>
<ol>
<li>   <code>spark.read.load</code> 是加载数据的通用方法。</li>
<li>   <code>df.write.save</code> 是保存数据的通用方法。</li>
</ol>
<h3 id="3-1-1-手动指定选项"><a href="#3-1-1-手动指定选项" class="headerlink" title="3.1.1    手动指定选项"></a>3.1.1    手动指定选项</h3><p>也可以手动给数据源指定一些额外的选项。数据源应该用全名称来指定，但是对一些内置的数据源也可以使用短名称：<code>json</code>，<code>parquet</code>，<code>jdbc</code>，<code>orc,</code>libsvm<code>,</code>csv<code>，text</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// </span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// </span></span><br><span class="line">peopleDF.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).write.format(<span class="string">&quot;parquet&quot;</span>).save(<span class="string">&quot;namesAndAges.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>





<h3 id="3-1-2-在文件上直接运行-SQL"><a href="#3-1-2-在文件上直接运行-SQL" class="headerlink" title="3.1.2    在文件上直接运行 SQL"></a>3.1.2    在文件上直接运行 SQL</h3><p>我们前面都是使用 <code>read API</code> 先把文件加载到 <code>DataFrame</code>，然后再查询。其实，我们也可以直接在文件上进行查询。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from json.examples/src/main/resources/people.json&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>说明：<code>json</code> 表示文件的格式。后面的文件具体路径需要用反引号括起来。</p>
<h3 id="3-1-3-文件保存选项-SaveMode"><a href="#3-1-3-文件保存选项-SaveMode" class="headerlink" title="3.1.3    文件保存选项(SaveMode)"></a>3.1.3    文件保存选项(SaveMode)</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 方式一：</span></span><br><span class="line">df.write.format(<span class="string">&quot;文件格式&quot;</span>).mode(<span class="string">&quot;模式名称&quot;</span>).save(<span class="string">&quot;文件路径&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方式二：</span></span><br><span class="line">df.write.mode(<span class="string">&quot;模式名称&quot;</span>).文件格式(<span class="string">&quot;文件路径&quot;</span>)</span><br></pre></td></tr></table></figure>



<p>保存操作可以使用 <code>SaveMode</code>，用来指明如何处理数据。使用 <code>mode()</code> 方法来设置。</p>
<p>有一点很重要：这些 <code>SaveMode</code> 都是没有加锁的，也不是原子操作。还有，如果你执行的是 <code>Overwrite</code> 操作，在写入新的数据之前会先删除旧的数据。</p>
<table>
<thead>
<tr>
<th align="left">Scala/Java</th>
<th>Any Language</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>SaveMode.ErrorIfExists(default)</code></td>
<td><code>&quot;error&quot;(default)</code></td>
<td>如果文件已经存在则抛出异常</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Append</code></td>
<td><code>&quot;append&quot;</code></td>
<td>如果文件已经存在则追加</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Overwrite</code></td>
<td><code>&quot;overwrite&quot;</code></td>
<td>如果文件已经存在则覆盖</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Ignore</code></td>
<td><code>&quot;ignore&quot;</code></td>
<td>如果文件已经存在则忽略</td>
</tr>
</tbody></table>
<hr>
<h2 id="3-2-加载-JSON-文件"><a href="#3-2-加载-JSON-文件" class="headerlink" title="3.2    加载 JSON 文件"></a>3.2    加载 JSON 文件</h2><p><code>Spark SQL</code> 能够自动推测 <code>JSON</code> 数据集的结构，并将它加载为一个 <code>Dataset[Row]</code>。</p>
<p>可以通过 <code>SparkSession.read.json()</code> 去加载一个 <code>JSON</code> 文件。也可以通过 <code>SparkSession.read.format(&quot;json&quot;).load()</code> 来加载。</p>
<p>注意：这个 <code>JSON</code> 文件不是一个传统的 <code>JSON</code> 文件，每一行都必须是一个完整的 <code>JSON</code> 串。</p>
<p><strong>json 文件</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lisi&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span> <span class="punctuation">:</span> <span class="number">20</span><span class="punctuation">,</span> <span class="attr">&quot;friends&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;lisi&quot;</span><span class="punctuation">,</span> <span class="string">&quot;zs&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;zs&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span> <span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span> <span class="attr">&quot;friends&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;lisi&quot;</span><span class="punctuation">,</span> <span class="string">&quot;zs&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wangwu&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span> <span class="punctuation">:</span> <span class="number">15</span><span class="punctuation">,</span> <span class="attr">&quot;friends&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;lisi&quot;</span><span class="punctuation">,</span> <span class="string">&quot;zs&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p><strong>代码</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataSourceDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;target/classes/user.json&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User</span>] = df.as[<span class="type">User</span>]</span><br><span class="line">        ds.foreach(user =&gt; println(user.friends(<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age: <span class="type">Long</span>, friends: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span></span><br></pre></td></tr></table></figure>



<hr>
<h2 id="3-3-读取-Parquet-文件"><a href="#3-3-读取-Parquet-文件" class="headerlink" title="3.3    读取 Parquet 文件"></a>3.3    读取 Parquet 文件</h2><p><code>Parquet</code> 是一种流行的列式存储格式，可以高效地存储具有嵌套字段的记录。<code>Parquet</code> 格式经常在 <code>Hadoop</code> 生态圈中被使用，它也支持 <code>Spark SQL</code> 的全部数据类型。 <code>Spark SQL</code> 提供了直接读取和存储 <code>Parquet</code> 格式文件的方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataSourceDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> jsonDF: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;target/classes/user.json&quot;</span>)</span><br><span class="line">        jsonDF.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).parquet(<span class="string">&quot;target/classes/user.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> parDF: <span class="type">DataFrame</span> = spark.read.parquet(<span class="string">&quot;target/classes/user.parquet&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> userDS: <span class="type">Dataset</span>[<span class="type">User</span>] = parDF.as[<span class="type">User</span>]</span><br><span class="line">        userDS.map(user =&gt; &#123;user.name = <span class="string">&quot;zl&quot;</span>; user.friends(<span class="number">0</span>) = <span class="string">&quot;志玲&quot;</span>;user&#125;).show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">var name:<span class="type">String</span>, age: <span class="type">Long</span>, friends: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong> <code>Parquet</code> 格式的文件是 <code>Spark</code> 默认格式的数据源。所以，当使用通用的方式时可以直接保存和读取，而不需要使用 <code>format spark.sql.sources.default</code> 这个配置来修改默认数据源。</p>
<hr>
<h2 id="3-4-JDBC"><a href="#3-4-JDBC" class="headerlink" title="3.4    JDBC"></a>3.4    JDBC</h2><p><code>Spark SQL</code> 也支持使用 <code>JDBC</code> 从其它的数据库中读取数据。<code>JDBC</code> 数据源比使用 <code>JdbcRDD</code> 更爽一些，这是因为返回的结果直接就是一个 <code>DataFrame</code>，<code>DataFrame</code> 更加容易被处理或者与其它的数据源进行 <code>join</code>。</p>
<p><code>Spark SQL</code> 可以通过 <code>JDBC</code> 从关系型数据库中读取数据的方式创建 <code>DataFrame</code>，通过对 <code>DataFrame</code> 一系列的计算后，还可以将数据再写回关系型数据库中。</p>
<p>注意：如果想在 <code>spark-shell</code> 中操作 <code>jdbc</code>，需要把相关的 <code>jdbc</code> 驱动拷贝到 <code>jars</code> 目录下。</p>
<p><strong>导入依赖：</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.40<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="3-4-1-从-JDBC-读数据"><a href="#3-4-1-从-JDBC-读数据" class="headerlink" title="3.4.1    从 JDBC 读数据"></a>3.4.1    从 JDBC 读数据</h3><p>可以使用通用的 <code>load</code> 方法，也可以使用 <code>jdbc</code> 方法。</p>
<ol>
<li><p>使用通用的 <code>load</code> 方法加载</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 11:20</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">    <span class="keyword">val</span> user = <span class="string">&quot;root&quot;</span></span><br><span class="line">    <span class="keyword">val</span> password = <span class="string">&quot;123456&quot;</span></span><br><span class="line">    <span class="keyword">val</span> dbtable = <span class="string">&quot;users&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 通过 JDBC 读取数据到 DF 中</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">      .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;url&quot;</span>, url)</span><br><span class="line">      .option(<span class="string">&quot;user&quot;</span>, user)</span><br><span class="line">      .option(<span class="string">&quot;password&quot;</span>, password)</span><br><span class="line">      .option(<span class="string">&quot;dbtable&quot;</span>, dbtable)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
<li><p>专用方法加载数据</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 11:20</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCDemo2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">    <span class="keyword">val</span> user = <span class="string">&quot;root&quot;</span></span><br><span class="line">    <span class="keyword">val</span> password = <span class="string">&quot;123456&quot;</span></span><br><span class="line">    <span class="keyword">val</span> dbtable = <span class="string">&quot;users&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> properties: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.put(<span class="string">&quot;user&quot;</span>, user)</span><br><span class="line">    properties.put(<span class="string">&quot;password&quot;</span>, password)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过 JDBC 读取数据到 DF 中</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">      .jdbc(url, dbtable, properties)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="3-4-2-向-JDBC-写入数据"><a href="#3-4-2-向-JDBC-写入数据" class="headerlink" title="3.4.2    向 JDBC 写入数据"></a>3.4.2    向 JDBC 写入数据</h3><p>也分两种方法：通用 <code>write.save</code> 和 <code>write.jdbc</code></p>
<p><strong>通用写法</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 11:38</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCWriteDemo01</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">URL</span> = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">USER</span> = <span class="string">&quot;root&quot;</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">PASSWORD</span> = <span class="string">&quot;123456&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 1、先读取 json 文件中的数据</span></span><br><span class="line"><span class="comment">    * 2、再写入数据库中</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="comment">// 读</span></span><br><span class="line">    <span class="keyword">val</span> source: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 写。如果表不存在，会自动创建</span></span><br><span class="line">    source.write</span><br><span class="line">      .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;url&quot;</span>, <span class="type">URL</span>)</span><br><span class="line">      .option(<span class="string">&quot;user&quot;</span>, <span class="type">USER</span>)</span><br><span class="line">      .option(<span class="string">&quot;password&quot;</span>, <span class="type">PASSWORD</span>)</span><br><span class="line">      .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;new_users&quot;</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)	<span class="comment">// SaveMode 是一个枚举类</span></span><br><span class="line">      .save()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>  代码执行完毕后可在数据库中查询是否写数据成功</li>
</ul>
<p><strong>专用写法</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 11:38</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCWriteDemo02</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">URL</span> = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">USER</span> = <span class="string">&quot;root&quot;</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">PASSWORD</span> = <span class="string">&quot;123456&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 1、先读取 json 文件中的数据</span></span><br><span class="line"><span class="comment">    * 2、再写入数据库中</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="comment">// 读</span></span><br><span class="line">    <span class="keyword">val</span> source: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 写。如果表不存在，会自动创建</span></span><br><span class="line">    <span class="keyword">val</span> properties: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.put(<span class="string">&quot;user&quot;</span>, <span class="type">USER</span>)</span><br><span class="line">    properties.put(<span class="string">&quot;password&quot;</span>, <span class="type">PASSWORD</span>)</span><br><span class="line"></span><br><span class="line">    source.write</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">      .jdbc(<span class="type">URL</span>, table = <span class="string">&quot;new_users&quot;</span>, properties)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="3-5-Hive-数据库（重要）"><a href="#3-5-Hive-数据库（重要）" class="headerlink" title="3.5    Hive 数据库（重要）"></a>3.5    Hive 数据库（重要）</h2><p><code>Apache Hive</code> 是 <code>Hadoop</code> 上的 <code>SQL</code> 引擎，<code>Spark SQL</code> 编译时可以包含 <code>Hive</code> 支持，也可以不包含。</p>
<p>包含 <code>Hive</code> 支持的 <code>Spark SQL</code> 可以支持 <code>Hive</code> 表访问、 <code>UDF(用户自定义函数)</code> 以及 <code>Hive 查询语言(HiveQL/HQL)</code> 等。</p>
<p>需要强调的一点是，如果要在 <code>Spark SQL</code> 中包含 <code>Hive</code> 的库，并不需要事先安装 <code>Hive</code>。一般来说，最好还是在编译 <code>Spark SQL</code> 时引入 <code>Hive</code> 支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 <code>Spark</code>，它应该已经在编译时添加了 <code>Hive</code> 支持。</p>
<p>若要把 <code>Spark SQL</code> 连接到一个部署好的 <code>Hive</code> 上，你必须把 <code>hive-site.xml</code> 复制到 <code>Spark</code> 的配置文件目录中 <code>($SPARK_HOME/conf)</code>。即使没有部署好 <code>Hive</code>，<code>Spark SQL</code> 也可以运行。 </p>
<p>需要注意的是，如果你没有部署好 <code>Hive</code>，<code>Spark SQL</code> 会在当前的工作目录中创建出自己的 <code>Hive</code> 元数据仓库，叫作 <code>metastore_db</code>。此外，如果你尝试使用 <code>HiveQL</code> 中的 <code>CREATE TABLE</code>（并非 <code>CREATE EXTERNAL TABLE</code>）语句来创建表，这些表会被放在你默认的文件系统中的 <code>/user/hive/warehouse</code> 目录中（如果你的 <code>classpath</code> 中有配好的 <code>hdfs-site.xml</code>，默认的文件系统就是 <code>HDFS</code>，否则就是本地文件系统）。</p>
<hr>
<h3 id="3-5-1-使用内嵌的-Hive"><a href="#3-5-1-使用内嵌的-Hive" class="headerlink" title="3.5.1    使用内嵌的 Hive"></a>3.5.1    使用内嵌的 Hive</h3><p>如果使用 <code>Spark</code> 内嵌的 <code>Hive</code>，则什么都不用做，直接使用即可。</p>
<p><code>Hive</code> 的元数据存储在 <code>derby</code> 中，仓库地址：<code>$SPARK_HOME/spark-warehouse</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;create table aa(id int)&quot;</span>)</span><br><span class="line"><span class="number">19</span>/<span class="number">02</span>/<span class="number">09</span> <span class="number">18</span>:<span class="number">36</span>:<span class="number">10</span> <span class="type">WARN</span> <span class="type">HiveMetaStore</span>: <span class="type">Location</span>: file:/opt/module/spark-local/spark-warehouse/aa specified <span class="keyword">for</span> non-external table:aa</span><br><span class="line">res2: org.apache.spark.sql.<span class="type">DataFrame</span> = []</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| <span class="keyword">default</span>|       aa|      <span class="literal">false</span>|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line"><span class="comment">// 向表中加载本地数据数据</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;load data local inpath &#x27;./ids.txt&#x27; into table aa&quot;</span>)</span><br><span class="line">res8: org.apache.spark.sql.<span class="type">DataFrame</span> = []</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from aa&quot;</span>).show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|<span class="number">100</span>|</span><br><span class="line">|<span class="number">101</span>|</span><br><span class="line">|<span class="number">102</span>|</span><br><span class="line">|<span class="number">103</span>|</span><br><span class="line">|<span class="number">104</span>|</span><br><span class="line">|<span class="number">105</span>|</span><br><span class="line">|<span class="number">106</span>|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>

<p>然而在实际使用中，几乎没有任何人会使用内置的 <code>Hive</code>。</p>
<hr>
<h3 id="3-5-2-Spark集成外置Hive"><a href="#3-5-2-Spark集成外置Hive" class="headerlink" title="3.5.2 Spark集成外置Hive"></a>3.5.2 Spark集成外置Hive</h3><p><code>Spark</code> 和 <code>Hive</code> 集成的两种方式：</p>
<ol>
<li><code>Spark on Hive</code><ul>
<li>  只需要让 SparkSQL 找到 Hive 的元数据就 OK 了。</li>
</ul>
</li>
<li><code>Hive on Spark</code><ul>
<li>  了解</li>
</ul>
</li>
</ol>
<h4 id="3-5-2-1-集成步骤"><a href="#3-5-2-1-集成步骤" class="headerlink" title="3.5.2.1    集成步骤"></a>3.5.2.1    集成步骤</h4><ol>
<li>   <code>Spark</code> 要接管 <code>Hive</code> 需要把 <code>hive-site.xml</code> 拷贝到 Spark 的配置目录 <code>conf/</code> 下</li>
<li>   由于 <code>Hive</code> 的元数据保存在 <code>MySQL</code> 中，如果 <code>Spark</code> 想要访问元数据的话必须作为 <code>MySQL</code> 的客户端，所以还需要把 <code>Mysql</code> 的驱动拷贝到 Spark  的 <code>jars/</code> 目录下</li>
<li>   如果访问不到 <code>HDFS</code>，则还需要把 <code>core-site.xml</code> 和 <code>hdfs-site.xml</code> 拷贝到当前项目的 resource 目录下【可选】</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把 hive-site.xml 拷贝到 Spark 的配置目录 conf/ 下</span></span><br><span class="line">[lvnengdong@hadoop102 hive]$ <span class="built_in">cp</span> /opt/module/hive/conf/hive-site.xml /opt/module/spark-local/conf/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 Mysql 的驱动拷贝到 Spark  的 jars/ 目录下</span></span><br><span class="line">[lvnengdong@hadoop102 /]$ <span class="built_in">cp</span> /opt/software/mysql/mysql-connector-java-5.1.27/mysql-connector-java-5.1.27-bin.jar /opt/module/spark-local/jars</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/ 目录下（可选）</span></span><br></pre></td></tr></table></figure>



<h4 id="3-5-2-2-启动-spark-shell"><a href="#3-5-2-2-启动-spark-shell" class="headerlink" title="3.5.2.2    启动 spark-shell"></a>3.5.2.2    启动 spark-shell</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| <span class="keyword">default</span>|      emp|      <span class="literal">false</span>|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from emp&quot;</span>).show</span><br><span class="line"><span class="number">19</span>/<span class="number">02</span>/<span class="number">09</span> <span class="number">19</span>:<span class="number">40</span>:<span class="number">28</span> <span class="type">WARN</span> <span class="type">LazyStruct</span>: <span class="type">Extra</span> bytes detected at the end of the row! <span class="type">Ignoring</span> similar problems.</span><br><span class="line">+-----+-------+---------+----+----------+------+------+------+</span><br><span class="line">|empno|  ename|      job| mgr|  hiredate|   sal|  comm|deptno|</span><br><span class="line">+-----+-------+---------+----+----------+------+------+------+</span><br><span class="line">| <span class="number">7369</span>|  <span class="type">SMITH</span>|    <span class="type">CLERK</span>|<span class="number">7902</span>|<span class="number">1980</span><span class="number">-12</span><span class="number">-17</span>| <span class="number">800.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7499</span>|  <span class="type">ALLEN</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-2</span><span class="number">-20</span>|<span class="number">1600.0</span>| <span class="number">300.0</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7521</span>|   <span class="type">WARD</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-2</span><span class="number">-22</span>|<span class="number">1250.0</span>| <span class="number">500.0</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7566</span>|  <span class="type">JONES</span>|  <span class="type">MANAGER</span>|<span class="number">7839</span>|  <span class="number">1981</span><span class="number">-4</span><span class="number">-2</span>|<span class="number">2975.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7654</span>| <span class="type">MARTIN</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-9</span><span class="number">-28</span>|<span class="number">1250.0</span>|<span class="number">1400.0</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7698</span>|  <span class="type">BLAKE</span>|  <span class="type">MANAGER</span>|<span class="number">7839</span>|  <span class="number">1981</span><span class="number">-5</span><span class="number">-1</span>|<span class="number">2850.0</span>|  <span class="literal">null</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7782</span>|  <span class="type">CLARK</span>|  <span class="type">MANAGER</span>|<span class="number">7839</span>|  <span class="number">1981</span><span class="number">-6</span><span class="number">-9</span>|<span class="number">2450.0</span>|  <span class="literal">null</span>|    <span class="number">10</span>|</span><br><span class="line">| <span class="number">7788</span>|  <span class="type">SCOTT</span>|  <span class="type">ANALYST</span>|<span class="number">7566</span>| <span class="number">1987</span><span class="number">-4</span><span class="number">-19</span>|<span class="number">3000.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7839</span>|   <span class="type">KING</span>|<span class="type">PRESIDENT</span>|<span class="literal">null</span>|<span class="number">1981</span><span class="number">-11</span><span class="number">-17</span>|<span class="number">5000.0</span>|  <span class="literal">null</span>|    <span class="number">10</span>|</span><br><span class="line">| <span class="number">7844</span>| <span class="type">TURNER</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>|  <span class="number">1981</span><span class="number">-9</span><span class="number">-8</span>|<span class="number">1500.0</span>|   <span class="number">0.0</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7876</span>|  <span class="type">ADAMS</span>|    <span class="type">CLERK</span>|<span class="number">7788</span>| <span class="number">1987</span><span class="number">-5</span><span class="number">-23</span>|<span class="number">1100.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7900</span>|  <span class="type">JAMES</span>|    <span class="type">CLERK</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>| <span class="number">950.0</span>|  <span class="literal">null</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7902</span>|   <span class="type">FORD</span>|  <span class="type">ANALYST</span>|<span class="number">7566</span>| <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>|<span class="number">3000.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7934</span>| <span class="type">MILLER</span>|    <span class="type">CLERK</span>|<span class="number">7782</span>| <span class="number">1982</span><span class="number">-1</span><span class="number">-23</span>|<span class="number">1300.0</span>|  <span class="literal">null</span>|    <span class="number">10</span>|</span><br><span class="line">| <span class="number">7944</span>|zhiling|    <span class="type">CLERK</span>|<span class="number">7782</span>| <span class="number">1982</span><span class="number">-1</span><span class="number">-23</span>|<span class="number">1300.0</span>|  <span class="literal">null</span>|    <span class="number">50</span>|</span><br><span class="line">+-----+-------+---------+----+----------+------+------+------+</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="3-5-2-3-使用-spark-sql-客户端"><a href="#3-5-2-3-使用-spark-sql-客户端" class="headerlink" title="3.5.2.3    使用 spark-sql 客户端"></a>3.5.2.3    使用 spark-sql 客户端</h4><p>在 <code>spark-shell</code> 中执行 <code>Hive</code> 方面的查询比较麻烦。格式为：<code>spark.sql(&quot;SQL语句&quot;).show</code></p>
<p><code>Spark</code> 专门给我们提供了书写 <code>HiveQL</code> 的工具： <code>spark-sql cli</code></p>
<ol>
<li><p>启动 <code>spark-sql</code> 客户端：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-local]$ bin/spark-sql</span><br></pre></td></tr></table></figure></li>
<li><p>在 <code>spark-sql</code> 中执行 SQL 语句</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark<span class="operator">-</span><span class="keyword">sql</span> (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> movie_info;</span><br><span class="line">movie	category</span><br><span class="line">《疑犯追踪》	[&quot;悬疑&quot;,&quot;动作&quot;,&quot;科幻&quot;,&quot;剧情&quot;]</span><br><span class="line">《Lie <span class="keyword">to</span> me》	[&quot;悬疑&quot;,&quot;警匪&quot;,&quot;动作&quot;,&quot;心理&quot;,&quot;剧情&quot;]</span><br><span class="line">《战狼<span class="number">2</span>》	[&quot;战争&quot;,&quot;动作&quot;,&quot;灾难&quot;]</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.395</span> seconds, Fetched <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure></li>
<li><p>退出 <code>spark-sql</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-sql (default)&gt; quit;</span><br></pre></td></tr></table></figure>

</li>
<li><p> <code>spark-sql</code> 客户端一般用于测试，生产环境中不常用</p>
</li>
</ol>
<hr>
<h4 id="3-5-2-4-使用-hiveserver2-beeline"><a href="#3-5-2-4-使用-hiveserver2-beeline" class="headerlink" title="3.5.2.4    使用 hiveserver2 + beeline"></a>3.5.2.4    使用 <code>hiveserver2 + beeline</code></h4><p><code>spark-sql</code> 得到的结果不够友好，所以可以使用 <code>hiveserver2 + beeline</code></p>
<ol>
<li><p>启动 <code>thrift</code> 服务器</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以 local 模式启动 thriftserver 服务</span></span><br><span class="line">[lvnengdong@hadoop102 spark-local]$ sbin/start-thriftserver.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 yarn 模式启动 thriftserver 服务 </span></span><br><span class="line">sbin/start-thriftserver.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--hiveconf hive.server2.thrift.bind.host=hadoop102 \</span><br><span class="line">-–hiveconf hive.server2.thrift.port=10000 \</span><br></pre></td></tr></table></figure>

</li>
<li><p>启动 <code>beeline</code> 客户端</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">    [lvnengdong@hadoop102 spark-local]$ bin/beeline</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 然后输入</span></span><br><span class="line">    !connect jdbc:hive2://hadoop102:10000</span><br><span class="line">    <span class="comment"># 然后按照提示输入用户名和密码</span></span><br><span class="line">    admin</span><br><span class="line">123456</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看</p>
 <img src="/2021/12/11/Spark-SQL/image-20211225162805804.png" alt="image-20211225162805804" style="zoom: 67%;"></li>
</ol>
<hr>
<h3 id="3-5-3-在代码中访问-Hive"><a href="#3-5-3-在代码中访问-Hive" class="headerlink" title="3.5.3    在代码中访问 Hive"></a>3.5.3    在代码中访问 Hive</h3><h4 id="步骤1-拷贝-hive-site-xml-到-resources-目录下"><a href="#步骤1-拷贝-hive-site-xml-到-resources-目录下" class="headerlink" title="步骤1: 拷贝 hive-site.xml 到 resources 目录下"></a>步骤1: 拷贝 <code>hive-site.xml</code> 到 resources 目录下</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Hive连接哪个库来查找元数据--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 default 数据库的默认位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="步骤2-添加依赖"><a href="#步骤2-添加依赖" class="headerlink" title="步骤2: 添加依赖"></a>步骤2: 添加依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 添加 Spark 支持 Hive 的 jar 包--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="步骤3-代码"><a href="#步骤3-代码" class="headerlink" title="步骤3: 代码"></a>步骤3: 代码</h4><h5 id="从-Hive-中读数据"><a href="#从-Hive-中读数据" class="headerlink" title="从 Hive 中读数据"></a>从 Hive 中读数据</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 17:00</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HiveRead</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 如果 HDFS 提示权限不足，则可以修改文件属主</span></span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;lvnengdong&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .enableHiveSupport()  <span class="comment">// 添加支持外置 Hive，如果不设置默认使用 Spark 内置的 Hive</span></span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">import</span> spark.sql</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行 SQL</span></span><br><span class="line">    sql(<span class="string">&quot;show databases&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h5 id="向-Hive-中写数据"><a href="#向-Hive-中写数据" class="headerlink" title="向 Hive 中写数据"></a>向 Hive 中写数据</h5><ol>
<li> 使用 Hive 的 <code>insert</code> 语句</li>
<li> <code>df.saveAsTable(&quot;表名&quot;)</code>：将 DF 中的数据直接写出到 HIve 表中。使用列名分配 value</li>
<li> <code>df.write.insertInto(&quot;表名&quot;)</code>：基本等价于方式二的 <code>append</code> 模式，但是插入数据时要求对应的 Hive 表必须提前存在。使用位置分配 value</li>
</ol>
<h6 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h6><p><strong>注意：</strong></p>
<p>在 Spark 中创建数据库，默认情况下，元数据信息保存在 MySQL 中，而数据则会保存在本地。</p>
<p>如果想要创建的数据库数据保存在 HDFS 上，主要有两种解决方案：</p>
<ol>
<li><p>在创建数据库时，通过参数修改数据库仓库的地址</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://hadoop201:9000/user/hive/warehouse&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p> 不要使用 Spark 创建数据库，而是使用 Hive 去创建数据库</p>
</li>
</ol>
<p><strong>Demo</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 19:10</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HiveWrite</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      <span class="comment">// 显式配置数据仓库地址</span></span><br><span class="line">      .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://hadoop102:9000/spark_hive/warehouse&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、创建数据库</span></span><br><span class="line">    spark.sql(<span class="string">&quot;create database spark_hive01&quot;</span>).show()</span><br><span class="line">    <span class="comment">// 2、创建表</span></span><br><span class="line">    spark.sql(<span class="string">&quot;use spark_hive01&quot;</span>).show()</span><br><span class="line">    spark.sql(<span class="string">&quot;create table user_info(id int, name string)&quot;</span>).show()</span><br><span class="line">    <span class="comment">// 3、插入数据</span></span><br><span class="line">    spark.sql(<span class="string">&quot;insert into user_info values(10, &#x27;zs&#x27;)&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>  插入的数据保存在 HDFS 上</li>
</ul>
<p><img src="/2021/12/11/Spark-SQL/image-20211225195825371.png" alt="image-20211225195825371"></p>
<p><strong>Demo2</strong></p>
<p>把读取到的数据写入到 Hive 中，表可以存在也可以不存在</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 19:10</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HiveWrite2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      <span class="comment">// 显式配置数据仓库地址，默认保存在本地文件系统上</span></span><br><span class="line">      .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://hadoop102:9000/spark_hive/warehouse&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、读取数据</span></span><br><span class="line">    <span class="keyword">val</span> source: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、写出数据到Hive表中（若表不存在则自动创建）</span></span><br><span class="line">    spark.sql(<span class="string">&quot;use spark_hive01&quot;</span>)</span><br><span class="line">    source.write.mode(<span class="string">&quot;append&quot;</span>).saveAsTable(<span class="string">&quot;user_info2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 方式二：将 df 数据写出到 Hive 表中（若表不存在则抛出异常）</span></span><br><span class="line"><span class="comment">//    source.write.insertInto(&quot;user_info2&quot;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<ul>
<li><p>聚合后，默认分区数为 200。如果想要调整分区数，可以使用 <code>coalesce</code> 函数</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 调整分区数量</span></span><br><span class="line">df.coalesce(<span class="number">1</span>).write.saveAsTable(<span class="string">&quot;表名&quot;</span>)	</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/11/Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/11/Spark/" class="post-title-link" itemprop="url">Spark 基础</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-11 20:42:32" itemprop="dateCreated datePublished" datetime="2021-12-11T20:42:32+08:00">2021-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-01-16 13:05:22" itemprop="dateModified" datetime="2022-01-16T13:05:22+08:00">2022-01-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>  start</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">大数据</span><br><span class="line">1、数据传输</span><br><span class="line">	Flume	实时</span><br><span class="line">	Sqoop	离线</span><br><span class="line"></span><br><span class="line">2、数据计算</span><br><span class="line">	MapReduce</span><br><span class="line">	Spark：比MR快100倍</span><br><span class="line">	</span><br><span class="line">3、数据存储</span><br><span class="line">	HDFS</span><br></pre></td></tr></table></figure>



<hr>
<h1 id="第-1-章-Spark-内置模块介绍"><a href="#第-1-章-Spark-内置模块介绍" class="headerlink" title="第 1 章 Spark 内置模块介绍"></a>第 1 章 Spark 内置模块介绍</h1><p><img src="/2021/12/11/Spark/image-20211211214657898.png" alt="image-20211211214657898"></p>
<h2 id="1-Cluster-Manager"><a href="#1-Cluster-Manager" class="headerlink" title="1    Cluster Manager"></a>1    Cluster Manager</h2><blockquote>
<p>  <strong>Cluster Manager；集群管理器；资源调度器</strong></p>
</blockquote>
<p>主要负责调度管理程序运算时所需的软、硬件资源，如 CPU、内存等。有多种实现方式，常见的落地实现有：</p>
<ol>
<li> <code>Standalone</code>：Spark 内置的资源调度器，需要在集群中的每台节点上安装 Spark</li>
<li> <code>Hadoop YARN</code>：使用 Hadoop 的 Yarn 管理计算资源，在国内使用最广泛</li>
<li> <code>Apache Mesos</code>：国内使用较少, 国外使用较多</li>
</ol>
<p>Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。也就是说 Spark 可以在一个节点上计算，也可以利用上千个节点进行运算，并且这种转换是非常容易实现的。为了实现这样的要求，同时获得最大灵活性，Spark 支持在各种集群管理器(Cluster Manager)上运行。</p>
<h2 id="2-SparkCore"><a href="#2-SparkCore" class="headerlink" title="2    SparkCore"></a>2    SparkCore</h2><p>实现了 Spark 的基本功能，包括任务调度、内存管理、错误恢复、与存储系统交互等模块。SparkCore 中还包含了对弹性分布式数据集 <code>RDD(Resilient Distributed DataSet)</code> 的 API 定义。</p>
<h2 id="3-Spark-SQL"><a href="#3-Spark-SQL" class="headerlink" title="3    Spark SQL"></a>3    Spark SQL</h2><p>是 Spark 用来操作结构化数据的程序包。通过 SparkSQL，我们可以使用 SQL 或者Apache Hive 版本的 SQL 方言(HQL)来查询数据。Spark SQL 支持多种数据源，比如 Hive 表、Parquet 以及 JSON 等。</p>
<p>就是将 Hive 底层的执行引擎由 MapReduce 换成了 Spark。</p>
<h2 id="4-Spark-Streaming"><a href="#4-Spark-Streaming" class="headerlink" title="4  Spark Streaming"></a>4  Spark Streaming</h2><p>是 Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API，并且与 Spark Core 中的 RDD API 高度对应。</p>
<h2 id="5-Spark-MLlib"><a href="#5-Spark-MLlib" class="headerlink" title="5  Spark MLlib"></a>5  Spark MLlib</h2><p>提供常见的机器学习 (ML) 功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据导入等额外的支持功能。</p>
<hr>
<h1 id="第-2-章-Spark-运行模式"><a href="#第-2-章-Spark-运行模式" class="headerlink" title="第 2 章 Spark 运行模式"></a>第 2 章 Spark 运行模式</h1><p>本章介绍在各种运行模式如何运行 Spark 应用.</p>
<p>首先需要下载 Spark</p>
<p>1．官网地址 <a target="_blank" rel="noopener" href="http://spark.apache.org/">http://spark.apache.org/</a></p>
<p>2．文档查看地址 <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.1.1/">https://spark.apache.org/docs/2.1.1/</a></p>
<p>3．下载地址 <a target="_blank" rel="noopener" href="https://archive.apache.org/dist/spark/">https://archive.apache.org/dist/spark/</a></p>
<h2 id="2-1-Local-模式"><a href="#2-1-Local-模式" class="headerlink" title="2.1 Local 模式"></a>2.1 Local 模式</h2><p>Local 模式就是在一台计算机上利用多线程模拟多台服务器来运行 Spark。</p>
<h3 id="2-1-1-解压-Spark-安装包"><a href="#2-1-1-解压-Spark-安装包" class="headerlink" title="2.1.1    解压 Spark 安装包"></a>2.1.1    解压 Spark 安装包</h3><ol>
<li><p>把安装包上传到 <code>/opt/software/</code> 下，并解压到 <code>/opt/module/</code> 目录下</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 software]$ tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module</span><br></pre></td></tr></table></figure></li>
<li><p>复制一份刚刚解压得到的目录，并重命名为 <code>spark-local</code>。【这一步操作的意义在于测试多种运行模式时可以独立测试】</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 module]$ <span class="built_in">cp</span> -r spark-2.1.1-bin-hadoop2.7 spark-local</span><br></pre></td></tr></table></figure></li>
<li><p>查看 spark 的安装目录</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-2.1.1-bin-hadoop2.7]$ ll</span><br><span class="line">总用量 84</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong  4096 4月  26 2017 bin	<span class="comment"># 保存可执行的二进制脚本文件的目录</span></span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong   230 4月  26 2017 conf	<span class="comment"># 配置文件目录</span></span><br><span class="line">drwxr-xr-x. 5 lvnengdong lvnengdong    50 4月  26 2017 data	<span class="comment"># 提供一些用于测试的数据集</span></span><br><span class="line">drwxr-xr-x. 4 lvnengdong lvnengdong    29 4月  26 2017 examples	<span class="comment"># 提供一些写好的测试案例</span></span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong 12288 4月  26 2017 jars	<span class="comment"># 项目依赖的jar包</span></span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong 17811 4月  26 2017 LICENSE</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong  4096 4月  26 2017 licenses</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong 24645 4月  26 2017 NOTICE</span><br><span class="line">drwxr-xr-x. 8 lvnengdong lvnengdong   240 4月  26 2017 python	<span class="comment"># python调用相关</span></span><br><span class="line">drwxr-xr-x. 3 lvnengdong lvnengdong    17 4月  26 2017 R		<span class="comment"># R调用相关</span></span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong  3817 4月  26 2017 README.md</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   128 4月  26 2017 RELEASE</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong  4096 4月  26 2017 sbin	<span class="comment"># 群起集群时使用的一些命令</span></span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong    42 4月  26 2017 yarn	<span class="comment"># 整合Yarn相关</span></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="2-1-2-运行官方求PI的案例"><a href="#2-1-2-运行官方求PI的案例" class="headerlink" title="2.1.2 运行官方求PI的案例"></a>2.1.2 运行官方求PI的案例</h3><p><strong>命令：</strong><code>spark-submit</code>（含义：<code>spark</code> 客户端将任务提交给 <code>Cluster Manager</code> 去执行）  </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-2.1.1-bin-hadoop2.7]$ bin/spark-submit \</span><br><span class="line">&gt; --class org.apache.spark.examples.SparkPi \</span><br><span class="line">&gt; --master <span class="built_in">local</span>[2] \</span><br><span class="line">&gt; ./examples/jars/spark-examples_2.11-2.1.1.jar 100</span><br></pre></td></tr></table></figure>

<p><strong>分析：</strong>运行 <code>spark-examples_2.11-2.1.1.jar</code> 程序，程序中的主类名是 <code>org.apache.spark.examples.SparkPi</code>。</p>
<p><strong>结果展示：</strong></p>
<p><img src="/2021/12/11/Spark/image-20211211222524138.png" alt="image-20211211222524138"></p>
<p><strong>语法</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line"> --class &lt;main-class&gt; \				<span class="comment"># 参数1：应用程序的主类名（全限定类名）</span></span><br><span class="line"> --master &lt;master-url&gt; \			<span class="comment"># 参数2：Cluster Manager 的地址</span></span><br><span class="line"> --deploy-mode &lt;deploy-mode&gt; \		<span class="comment">#  </span></span><br><span class="line"> --conf &lt;key&gt;=&lt;value&gt; \				<span class="comment"># 显式指定配置信息</span></span><br><span class="line"> ... <span class="comment"># other options</span></span><br><span class="line"> &lt;application-jar&gt; \</span><br><span class="line"> [application-arguments]</span><br></pre></td></tr></table></figure>

<ul>
<li>  <code>--master</code>：<code>master</code> 是真正执行程序的 <code>Cluster Manager</code> 的地址。默认为 <code>local</code>，表示在本地机器上运行。</li>
<li>  <code>--class</code>：待执行的应用程序会被打成一个 jar 包，该参数用于指定 jar 包的启动类（如 <code>org.apache.spark.examples.SparkPi</code>）。</li>
<li>  <code>--deploy-mode</code>：是否发布你的驱动到 worker 节点(cluster 模式) 或者作为一个本地客户端 (client 模式) (default: client)</li>
<li>  <code>--conf</code>：在程序运行前显式指定配置信息。格式 <code>key=value</code>，如果值包含空格，可以加引号 <code>&quot;key=value&quot;</code>。</li>
<li>  <code>application-jar</code>：待执行任务打包成的 jar，包含依赖。这个 <code>URL</code> 需要在集群中全局可见。比如 <code>hdfs:// 共享存储系统</code>，如果是  <code>file:// path</code>，那么需要保证所有的节点的 <code>path</code> 下都包含相同的 jar。</li>
<li>  <code>application-arguments</code>：传给主类中启动方法 <code>main()</code> 的参数</li>
<li>  <code>--executor-memory:1G</code>：指定每个 <code>executor</code> 可用内存为 1G</li>
<li>  <code>--total-executor-cores:6</code>：指定所有 <code>executor</code> 使用的 cpu 核数为 6 个</li>
<li>   <code>--executor-cores</code>：表示每个 executor 使用的 cpu 的核数</li>
</ul>
<p><strong>关于 Master URL 的说明</strong></p>
<p><code>Master URL</code> 就是 Master 节点的 IP 地址和端口号。</p>
<table>
<thead>
<tr>
<th>Master URL</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td><code>local</code></td>
<td>用一个线程在本地运行 Spark（即完全没有并行性）</td>
</tr>
<tr>
<td><code>local[K]</code></td>
<td>使用 K 个线程在本地运行 Spark</td>
</tr>
<tr>
<td><code>local[*]</code></td>
<td>在本地运行 Spark，使用的线程数量为当前机器可以提供的最大线程数</td>
</tr>
<tr>
<td><code>spark://HOST:PORT</code></td>
<td>Connect to the  given <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.1.1/spark-standalone.html">Spark   standalone cluster</a> master. The port must be whichever one your master is  configured to use, which is 7077 by default.</td>
</tr>
<tr>
<td><code>mesos://HOST:PORT</code></td>
<td>Connect to the  given <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.1.1/running-on-mesos.html">Mesos</a>  cluster. The port must be whichever one your is configured to use, which is  5050 by default. Or, for a Mesos cluster using ZooKeeper, use mesos://zk://…. To submit with –deploy-mode cluster, the HOST:PORT  should be configured to connect to the <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.1.1/running-on-mesos.html#cluster-mode">MesosClusterDispatcher</a>.</td>
</tr>
<tr>
<td><code>yarn</code></td>
<td>Connect to a <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.1.1/running-on-yarn.html">YARN</a>cluster  in client or cluster mode depending on the value of  –deploy-mode. The  cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable.</td>
</tr>
</tbody></table>
<hr>
<h3 id="2-1-3-使用-spark-shell"><a href="#2-1-3-使用-spark-shell" class="headerlink" title="2.1.3    使用 spark-shell"></a>2.1.3    使用 spark-shell</h3><p><code>spark-shell</code> 是 Spark 提供的一个交互式命令窗口形式的客户端。本案例将会使用 <code>spark-shell</code> 统计文件中各个单词的数量.</p>
<ol>
<li><p> 创建 2 个文本文件<code>a.txt</code> 和 <code>b.txt</code>，分别在两个文件内输入一些单词。</p>
</li>
<li><p>打开 <code>spark-shell</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-2.1.1-bin-hadoop2.7]$ bin/spark-shell --master <span class="built_in">local</span>[2]	</span><br><span class="line"><span class="comment"># --master local[2]，该参数可加可不加，若不加默认启动就是local模式</span></span><br></pre></td></tr></table></figure></li>
<li><p>启动成功页面。</p>
<ul>
<li>  <code>spark-shell</code> 启动成功后，我们可以看到 <code>shell</code> 客户端自动帮我们创建了一个 <code>Spark Context</code> 对象（Spark 上下文对象）并将其命名为 <code>sc</code>，我们在 shell 客户端中调用 Spark 就需要使用这个对象来实现。</li>
<li>  并且提供了 UI 界面：<code>http://192.168.1.102:4040</code></li>
</ul>
<p> <img src="/2021/12/11/Spark/image-20211212102939164.png" alt="image-20211212102939164"></p>
<ul>
<li>  UI 界面</li>
</ul>
<p> <img src="/2021/12/11/Spark/image-20211212103539619.png" alt="image-20211212103539619"></p>
</li>
<li><p>退出 <code>spark-shell</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; :q</span><br></pre></td></tr></table></figure></li>
<li><p><strong>运行 <code>wordcount</code> 程序</strong></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取 `input/` 目录下的所有文件到 RDD 对象中</span></span><br><span class="line">scala&gt; sc.textFile(<span class="string">&quot;input/&quot;</span>)</span><br><span class="line">res0: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = input/ <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// RDD是一个集合，其中保存数据的基本单位是行（line），这一步是将每行的单词按空格分开</span></span><br><span class="line">scala&gt; .flatMap(e = e.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">res1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将每个单词映射为一个元组，key为单词本身，value为1，即每个单词出现的个数</span></span><br><span class="line">scala&gt; .map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line">res2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">29</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 按照key相同的进行聚合（key不变，value相加）</span></span><br><span class="line">scala&gt; .reduceByKey(_+_)</span><br><span class="line">res3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">4</span>] at reduceByKey at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 结果收集</span></span><br><span class="line">scala&gt; .collect</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((resource,<span class="number">2</span>), (created,<span class="number">2</span>), (<span class="keyword">this</span>,<span class="number">2</span>), (<span class="class"><span class="keyword">class</span>,,2), (<span class="params">load,2</span>), (<span class="params">is,4</span>), (<span class="params"><span class="type">Building</span>,2</span>), (<span class="params">can,4</span>), (<span class="params">file,,2</span>), (<span class="params">build,2</span>), (<span class="params">configuration,,2</span>), ...</span></span><br></pre></td></tr></table></figure></li>
<li><p><code>wordcount</code> 数据流程分析</p>
<p> <img src="/2021/12/11/Spark/image-20211212110617449.png" alt="image-20211212110617449"></p>
<ol>
<li> <code>textFile(&quot;input&quot;)</code>：读取本地文件input文件夹数据；</li>
<li> <code>flatMap(_.split(&quot; &quot;))</code>：先 <code>map</code> 再 <code>flatten</code>，按照空格分割符将一行数据映射成一个个单词；</li>
<li> <code>map((_,1))</code>：对每一个元素操作，将单词映射为元组；</li>
<li> <code>reduceByKey(_+_)</code>：将 key 相同的值进行聚合，相加；</li>
<li> <code>collect</code>：将数据收集到Driver端展示。</li>
</ol>
</li>
</ol>
<hr>
<h2 id="2-2-Spark-核心概念介绍"><a href="#2-2-Spark-核心概念介绍" class="headerlink" title="2.2    Spark 核心概念介绍"></a>2.2    Spark 核心概念介绍</h2><h3 id="2-2-1-Master"><a href="#2-2-1-Master" class="headerlink" title="2.2.1    Master"></a>2.2.1    Master</h3><p><code>Master</code> 其实就是上文提到过的 <code>Cluster Manager</code>，负责管理调度整个集群的软、硬件资源。如果 Spark 接入了 Hadoop Yarn，那么 Yarn 中的 <code>ResourceManager</code> 就是一个 <code>Master</code>。</p>
<p>主要功能：</p>
<ol>
<li>   接收 <code>Worker</code> 的注册并管理集群中所有的 <code>Worker</code>；</li>
<li>   接收 <code>Client</code> 提交的 <code>Application</code>，调度等待的 <code>Application</code> 并向 <code>Worker</code> 分配任务。</li>
<li>   管理 Worker、Application 等。</li>
</ol>
<blockquote>
<p>  Master 是一种角色，负责集群中的资源分配与调度，可以有多种实现，在 Spark 的 Yarn 模式下，Master 就是 ResourceManager。在 Spark 的 standalone 模式下，Master 是由 Spark 自己实现的 </p>
</blockquote>
<h3 id="2-2-2-Worker"><a href="#2-2-2-Worker" class="headerlink" title="2.2.2    Worker"></a>2.2.2    Worker</h3><p>Spark 资源调度系统的 Slave，有多个。每个 Slave 掌管着当前节点的资源信息，类似于 Yarn 框架中的 NodeManager，主要功能：</p>
<ol>
<li>   通过 RegisterWorker 注册到 Master；</li>
<li>   定时发送心跳给 Master；</li>
<li>   根据 Master 发送的 Application 配置进程环境，并启动 ExecutorBackend（执行 Task 所需的临时进程）</li>
</ol>
<hr>
<h3 id="2-2-3-Driver-Program"><a href="#2-2-3-Driver-Program" class="headerlink" title="2.2.3 Driver Program"></a>2.2.3 Driver Program</h3><blockquote>
<p>  <strong>Driver Program；驱动程序</strong></p>
</blockquote>
<p><strong>每个</strong> Spark 应用程序都包含一个驱动程序，驱动程序负责把并行操作（并行Task）发布到集群上。</p>
<p>驱动程序包含 Spark 应用程序中的主函数，定义了分布式数据集以应用在集群中。</p>
<p>在前面的 wordcount 案例集中，spark-shell 就是我们的驱动程序，所以我们可以在其中键入我们任何想要的操作，然后由他负责发布。</p>
<p>驱动程序通过 SparkContext 对象来访问 Spark，SparkContext 对象相当于一个到 Spark 集群的连接。</p>
<p>在 spark-shell 中, 会自动创建一个 SparkContext 对象，并把这个对象命名为 sc。</p>
<hr>
<h3 id="2-2-4-Executor"><a href="#2-2-4-Executor" class="headerlink" title="2.2.4  Executor"></a>2.2.4  Executor</h3><blockquote>
<p>  <strong>Executor；执行器</strong></p>
</blockquote>
<p><code>executor</code> 是一个运行在 <code>Worker</code> 节点上的用于执行具体任务的线程。</p>
<p>SparkContext 对象一旦成功连接到集群管理器（Master），就可以获取到集群中每个节点上的执行器（Executor）。</p>
<p>执行器是一个进程（进程名：ExecutorBackend，运行在 Worker 节点上），用来执行计算和为应用程序存储数据。（一个 Worker上会启动多个 Executor）</p>
<p>然后，Spark 会发送应用程序代码（比如:jar包）到每个执行器。最后，SparkContext 对象发送任务到执行器开始执行程序。</p>
<p><img src="/2021/12/11/Spark/image-20211212111132277.png" alt="image-20211212111132277"></p>
<hr>
<h3 id="1-2-4-RDDs"><a href="#1-2-4-RDDs" class="headerlink" title="1.2.4  RDDs"></a>1.2.4  RDDs</h3><blockquote>
<p>  <strong>Resilient Distributed Dataset；弹性分布式数据集</strong></p>
</blockquote>
<p>一旦拥有了 SparkContext 对象，就可以使用它来创建 RDD 了。在前面的例子中，我们调用 <code>sc.textFile(...)</code> 来创建了一个 RDD，表示文件中的每一行文本，我们可以对这些文本行运行各种各样的操作。</p>
<p>在第二部分的 SparkCore 中，我们重点就是学习 RDD。</p>
<hr>
<h3 id="1-2-5-Cluster-Managers"><a href="#1-2-5-Cluster-Managers" class="headerlink" title="1.2.5  Cluster Managers"></a>1.2.5  Cluster Managers</h3><blockquote>
<p>  <strong>Cluster Managers；集群管理器</strong></p>
</blockquote>
<p>为了在一个 Spark 集群上运行计算，SparkContext 对象可以连接到几种集群管理器。包括 Spark 自己的 standalone cluster manager、 Mesos 或者 Yarn 等。</p>
<p>集群管理器负责跨应用程序分配资源。</p>
<hr>
<h3 id="1-2-6-专业术语列表"><a href="#1-2-6-专业术语列表" class="headerlink" title="1.2.6  专业术语列表"></a>1.2.6  专业术语列表</h3><table>
<thead>
<tr>
<th>Term</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td>Application</td>
<td>User program  built on Spark. Consists of a <em>driver  program</em> and <em>executors</em> on the  cluster. (构建于 Spark 之上的应用程序. 包含驱动程序和运行在集群上的执行器)</td>
</tr>
<tr>
<td>Application jar</td>
<td>A jar  containing the user’s Spark application. In some cases users will want to create  an “uber jar” containing their application along with its dependencies. The  user’s jar should never include Hadoop or Spark libraries, however, these  will be added at runtime</td>
</tr>
<tr>
<td>Driver program</td>
<td>The thread  running the main() function of the application and creating the SparkContext</td>
</tr>
<tr>
<td>Cluster manager</td>
<td>An external  service for acquiring resources on the cluster (e.g. standalone manager,  Mesos, YARN)</td>
</tr>
<tr>
<td>Deploy mode<br>部署模式</td>
<td>Distinguishes  where the driver process runs. In “cluster” mode, the framework launches the  driver inside of the cluster. In “client” mode, the submitter launches the  driver outside of the cluster.<br>翻译：用于指定 driver 进程运行的位置。在“cluster”模式下，driver 进程运行在集群中；在“client”模式下，driver 进程运行在集群外部，也就是 client 所在的机器上。默认是 client 模式，Driver运行在Client客户机上</td>
</tr>
<tr>
<td>Worker node</td>
<td>Any node that  can run application code in the cluster</td>
</tr>
<tr>
<td>Executor</td>
<td>A process  launched for an application on a worker node, that runs tasks and keeps data  in memory or disk storage across them. Each application has its own  executors.</td>
</tr>
<tr>
<td>Task</td>
<td>A unit of work  that will be sent to one executor</td>
</tr>
<tr>
<td>Job</td>
<td>A parallel  computation consisting of multiple tasks that gets spawned in response to a  Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs.</td>
</tr>
<tr>
<td>Stage</td>
<td>Each job gets  divided into smaller sets of tasks called <em>stages</em>  that depend on each other (similar to the map and reduce stages in  MapReduce); you’ll see this term used in the driver’s logs.</td>
</tr>
</tbody></table>
<hr>
<h2 id="2-3-Standalone-模式"><a href="#2-3-Standalone-模式" class="headerlink" title="2.3 Standalone 模式"></a>2.3 Standalone 模式</h2><p><code>Standalone</code> 模式指使用 Spark 内置的 <code>Cluster Manager</code> 资源管理器搭建的集群。不需要借助其他的框架，是相对于 Yarn 和 Mesos 来说的。</p>
<p>构建一个由 Master + Slave 构成的 Spark 集群，Spark 运行在集群中。</p>
<h3 id="2-3-1-配置-Standalone-模式"><a href="#2-3-1-配置-Standalone-模式" class="headerlink" title="2.3.1 配置 Standalone 模式"></a>2.3.1 配置 Standalone 模式</h3><h4 id="步骤1-复制-spark-并命名为spark-standalone"><a href="#步骤1-复制-spark-并命名为spark-standalone" class="headerlink" title="步骤1: 复制 spark, 并命名为spark-standalone"></a>步骤1: 复制 spark, 并命名为spark-standalone</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> -r spark-2.1.1-bin-hadoop2.7 spark-standalone</span><br></pre></td></tr></table></figure>



<h4 id="步骤2-进入配置文件目录conf-配置spark-evn-sh"><a href="#步骤2-进入配置文件目录conf-配置spark-evn-sh" class="headerlink" title="步骤2: 进入配置文件目录conf, 配置spark-evn.sh"></a>步骤2: 进入配置文件目录conf, 配置spark-evn.sh</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> conf/</span><br><span class="line"><span class="built_in">cp</span> spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure>



<p>在 <code>spark-env.sh</code> 文件中配置如下内容：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SPARK_MASTER_HOST=hadoop102	<span class="comment"># 配置集群中Master的IP地址</span></span><br><span class="line">SPARK_MASTER_PORT=7077 <span class="comment"># 配置集群中Master的端口号。默认端口就是7077, 可以省略不配</span></span><br></pre></td></tr></table></figure>



<h4 id="步骤3-修改-slaves-文件-添加-worker-节点"><a href="#步骤3-修改-slaves-文件-添加-worker-节点" class="headerlink" title="步骤3: 修改 slaves 文件, 添加 worker 节点"></a>步骤3: 修改 slaves 文件, 添加 worker 节点</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> slaves.template slaves</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在slaves文件中配置如下内容（配置Slave节点的IP地址）:</span></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure>





<h4 id="步骤4-分发spark-standalone配置文件到整个集群中"><a href="#步骤4-分发spark-standalone配置文件到整个集群中" class="headerlink" title="步骤4: 分发spark-standalone配置文件到整个集群中"></a>步骤4: 分发spark-standalone配置文件到整个集群中</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="步骤5-启动-Spark-集群"><a href="#步骤5-启动-Spark-集群" class="headerlink" title="步骤5: 启动 Spark 集群"></a>步骤5: 启动 Spark 集群</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure>

<h4 id="步骤6-在网页中查看-Spark-集群情况"><a href="#步骤6-在网页中查看-Spark-集群情况" class="headerlink" title="步骤6: 在网页中查看 Spark 集群情况"></a>步骤6: 在网页中查看 Spark 集群情况</h4><p>由于 Master 节点配置在 hadoop102 上，所以要通过以下地址来访问 UI 界面: <a target="_blank" rel="noopener" href="http://hadoop201:8080/">http://hadoop201:8080</a></p>
<h3 id="2-3-2-使用-Standalone-模式运行计算-PI-的程序"><a href="#2-3-2-使用-Standalone-模式运行计算-PI-的程序" class="headerlink" title="2.3.2 使用 Standalone 模式运行计算 PI 的程序"></a>2.3.2 使用 Standalone 模式运行计算 PI 的程序</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop201:7077 \	<span class="comment"># 需要显式指定master的URL</span></span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 6 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar 100</span><br></pre></td></tr></table></figure>





<h3 id="2-3-3-在-Standalone-模式下启动-Spark-shell"><a href="#2-3-3-在-Standalone-模式下启动-Spark-shell" class="headerlink" title="2.3.3 在 Standalone 模式下启动 Spark-shell"></a>2.3.3 在 Standalone 模式下启动 Spark-shell</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell \</span><br><span class="line">--master spark://hadoop102:7077	<span class="comment"># 需要显式指定master</span></span><br></pre></td></tr></table></figure>



<p>执行 wordcount 程序</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">&quot;input/&quot;</span>).flatMap(.split(<span class="string">&quot; &quot;</span>)).map((,<span class="number">1</span>)).reduceByKey(+).collect</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((are,<span class="number">2</span>), (how,<span class="number">2</span>), (hello,<span class="number">4</span>), (atguigu,<span class="number">2</span>), (world,<span class="number">2</span>), (you,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<p><strong>注意:</strong></p>
<ul>
<li>  每个 worker 节点上要有相同的文件夹 <code>:input/</code>，否则会报文件不存在的异常</li>
</ul>
<hr>
<h2 id="2-4-Yarn-模式"><a href="#2-4-Yarn-模式" class="headerlink" title="2.4    Yarn 模式"></a>2.4    Yarn 模式</h2><h3 id="2-4-1-Yarn-模式概述"><a href="#2-4-1-Yarn-模式概述" class="headerlink" title="2.4.1    Yarn 模式概述"></a>2.4.1    Yarn 模式概述</h3><p><code>Spark Client</code> 可以直接连接 <code>Yarn</code>，利用 <code>Yarn</code> 管理的计算资源来进行 <code>Spark</code> 程序的运算，不需要额外构建 <code>Spark</code> 集群。</p>
<p>有 <code>client</code> 和 <code>cluster</code> 两种模式，主要区别在于：Driver 程序的运行节点不同。</p>
<ul>
<li>  <strong>client</strong>：Driver 程序运行在客户端，适用于交互、调试，希望立即看到 APP 的输出。</li>
<li>  <strong>cluster</strong>：Driver 程序运行在由 ResourceManager 启动的 AplicationMaster 上，适用于生产环境。</li>
</ul>
<p><strong>工作模式介绍：</strong></p>
<p><img src="/2021/12/11/Spark/image-20211213194827471.png" alt="image-20211213194827471"></p>
<hr>
<h3 id="2-4-2-Yarn-模式配置"><a href="#2-4-2-Yarn-模式配置" class="headerlink" title="2.4.2 Yarn 模式配置"></a>2.4.2 Yarn 模式配置</h3><p><strong>一、修改 Hadoop 集群的配置文件 yarn-site.xml，添加如下内容：</strong></p>
<p>Spark 对内存的要求特别高，所以 Spark 总是会检测到内存不够，把一些耗费内存大的进程杀死，而我们的测试环境内存比较小，我们需要把自动杀进程的配置给关掉。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>  修改后分发配置文件。</li>
</ul>
<p><strong>二、复制 spark 安装包，并重命名为 spark-yarn</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 module]$ <span class="built_in">cp</span> -r spark-2.1.1-bin-hadoop2.7 spark-yarn</span><br></pre></td></tr></table></figure>





<p><strong>三、修改 spark-evn.sh 文件</strong></p>
<p>Spark 想要在 Yarn 上运行就需要知道 Yarn 中 <code>ResourceManager</code> 的地址，而我们已经配置好了 Hadoop 的集群，所以只需要让 Spark 和 Hadoop 关联起来，Spark 就能间接地知道 <code>ResourceManager</code> 的地址了。</p>
<ul>
<li><p>复制 conf 包下的 <code>spark-env.sh.template</code> 为 <code>spark-env.sh</code></p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 conf]$ <span class="built_in">cp</span> spark-env.sh.template spark-env.sh</span><br><span class="line">[lvnengdong@hadoop102 conf]$ vim spark-env.sh</span><br></pre></td></tr></table></figure></li>
<li><p>并添加如下配置：告诉 Spark 客户端 Yarn 的相关配置</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><strong>四、配置历史日志服务器</strong></p>
<ol>
<li><p>复制 conf 目录下的 <code>spark-defaults.conf.template</code> 为 <code>spark-defaults.conf</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 conf]$ <span class="built_in">cp</span> spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure></li>
<li><p>配置 <code>spark-default.conf</code> 文件，开启 Log</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Before</span></span><br><span class="line"><span class="comment"># spark.eventLog.enabled           true</span></span><br><span class="line"><span class="comment"># spark.eventLog.dir               hdfs://namenode:8021/directory</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># After</span></span><br><span class="line">spark.eventLog.enabled           <span class="literal">true</span></span><br><span class="line">spark.eventLog.<span class="built_in">dir</span>               hdfs://hadoop102:9000/spark-job-log	<span class="comment"># 保存历史日志的地址</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>hdfs://hadoop102:9000/spark-job-log</code> 目录必须提前存在，目录名字可以随便起。所以我们在启动 HDFS 后，必须在 HDFS 的根目录下创建 spark-job-log 这个目录。</li>
</ul>
</li>
<li><p>在 <code>spark-env.sh</code> 文件中添加如下配置 </p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">&quot;-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://hadoop102:9000/spark-job-log&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 说明：</span></span><br><span class="line"><span class="comment"># -Dspark.history.ui.port=18080 ：历史日志服务器的UI端口号</span></span><br><span class="line"><span class="comment"># -Dspark.history.retainedApplications=30</span></span><br><span class="line"><span class="comment"># -Dspark.history.fs.logDirectory=hdfs://hadoop102:9000/spark-job-log&quot;	历史日志服务器保存文件的地址</span></span><br></pre></td></tr></table></figure>

</li>
<li><p>启动 Hadoop 集群（包括 HDFS、Yarn 等等）</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 群起HDFS</span></span><br><span class="line">[lvnengdong@hadoop103 conf]$ start-dfs.sh</span><br><span class="line"><span class="comment"># 群起Yarn</span></span><br><span class="line">[lvnengdong@hadoop103 apache-zookeeper-3.5.7-bin]$ start-yarn.sh</span><br><span class="line"><span class="comment"># 启动Hadoop的历史日志服务器</span></span><br><span class="line">[lvnengdong@hadoop102 conf]$ mr-jobhistory-daemon.sh start historyserver</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看启动的JPS进程</span></span><br><span class="line">[lvnengdong@hadoop103 apache-zookeeper-3.5.7-bin]$ xcall jps</span><br><span class="line">要执行的命令是jps</span><br><span class="line">---------------------hadoop102-----------------</span><br><span class="line">34515 NodeManager</span><br><span class="line">33958 NameNode</span><br><span class="line">34151 DataNode</span><br><span class="line">34921 Jps</span><br><span class="line">34811 JobHistoryServer	<span class="comment"># Hadoop的历史日志服务器</span></span><br><span class="line">---------------------hadoop103-----------------</span><br><span class="line">82307 Jps</span><br><span class="line">81637 ResourceManager</span><br><span class="line">81467 DataNode</span><br><span class="line">81786 NodeManager</span><br><span class="line">---------------------hadoop104-----------------</span><br><span class="line">81984 NodeManager</span><br><span class="line">81831 SecondaryNameNode</span><br><span class="line">82311 Jps</span><br><span class="line">81673 DataNode</span><br></pre></td></tr></table></figure>

</li>
<li><p>在 HDFS 的根目录下创建  <code>spark-job-log</code> 目录，用于保存 Spark 运行时产生的日志。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-yarn]$ hadoop fs -<span class="built_in">mkdir</span> /spark-job-log</span><br></pre></td></tr></table></figure>

<p> <img src="/2021/12/11/Spark/image-20211213211239915.png" alt="image-20211213211239915"></p>
</li>
<li><p> 启动 Spark 的历史日志服务器（需要先启动 HDFS，因为日志文件保存在 HDFS 上）</p>
</li>
<li><p>启动 Yarn 的历史日志服务器</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动 Spark 的历史日志服务器</span></span><br><span class="line">[lvnengdong@hadoop102 spark-yarn]$ sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动Hadoop的历史日志服务器</span></span><br><span class="line">[lvnengdong@hadoop102 spark-yarn]$ mr-jobhistory-daemon.sh start historyserver</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看JPS进程详情</span></span><br><span class="line">[lvnengdong@hadoop102 spark-yarn]$ jps -l</span><br><span class="line">34515 org.apache.hadoop.yarn.server.nodemanager.NodeManager</span><br><span class="line">33958 org.apache.hadoop.hdfs.server.namenode.NameNode</span><br><span class="line">34151 org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line">35306 org.apache.spark.deploy.history.HistoryServer		<span class="comment"># spark的历史日志服务器</span></span><br><span class="line">34811 org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer	<span class="comment"># Hadoop的历史日志服务器</span></span><br><span class="line">35563 sun.tools.jps.Jps</span><br></pre></td></tr></table></figure>

<ul>
<li>  Spark 历史日志服务器的 UI 地址：<code>http://hadoop102:18080/</code></li>
<li>  Hadoop 历史日志服务器的 UI 地址：</li>
<li>  <strong>注意：</strong>因为 Spark 需要访问 HDFS 来读/写日志信息，所以一定要保证 Spark 有权限读写 HDFS 文件系统。</li>
</ul>
</li>
<li><p>执行一段程序</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar 100</span><br></pre></td></tr></table></figure>

<p> 在 Yarn 的 ResourceManager 提供的 UI 页面上可以查看日志：地址为<code>http://hadoop103:8088/</code></p>
<p> <img src="/2021/12/11/Spark/image-20211213224417923.png" alt="image-20211213224417923"></p>
</li>
<li><p>日志服务</p>
<p> 在第8步的页面上点击 <code>history</code> 无法直接连接到 Spark 的日志。可以在 <code>spark-default.conf</code> 中添加如下配置达到上述目的。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.yarn.historyServer.address=hadoop102:18080</span><br><span class="line">spark.history.ui.port=18080</span><br></pre></td></tr></table></figure></li>
<li><p>启动 Yarn 模式下的 Spark-shell</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-yarn]$ bin/spark-shell --master yarn</span><br></pre></td></tr></table></figure>

</li>
<li><p> 注意：在 Yarn 模式下启动 Spark 后，处理的文件就会直接从 HDFS 上读取，而不是本地磁盘系统。</p>
</li>
</ol>
<hr>
<h2 id="2-6-几种运行模式的对比"><a href="#2-6-几种运行模式的对比" class="headerlink" title="2.6 几种运行模式的对比"></a>2.6 几种运行模式的对比</h2><table>
<thead>
<tr>
<th>模式</th>
<th>Spark安装机器数</th>
<th>需启动的进程</th>
<th>所属者</th>
</tr>
</thead>
<tbody><tr>
<td>Local</td>
<td>1</td>
<td>无</td>
<td>Spark</td>
</tr>
<tr>
<td>Standalone</td>
<td>多台</td>
<td>Master及Worker</td>
<td>Spark</td>
</tr>
<tr>
<td>Yarn</td>
<td>1</td>
<td>Yarn及HDFS</td>
<td>Hadoop</td>
</tr>
</tbody></table>
<h1 id="入门案例"><a href="#入门案例" class="headerlink" title="入门案例"></a>入门案例</h1><p><code>spark shell</code> 仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在 IDE 中编制程序，然后打成 jar 包，然后提交到集群，最常用的是创建一个 Maven 项目，利用 Maven 来管理 jar 包的依赖。</p>
<h2 id="1、创建-maven-项目-导入依赖"><a href="#1、创建-maven-项目-导入依赖" class="headerlink" title="1、创建 maven 项目, 导入依赖"></a>1、创建 maven 项目, 导入依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Spark核心依赖--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 打包插件, 否则 scala 类不会编译并打包进去 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="2、创建WordCount-scala"><a href="#2、创建WordCount-scala" class="headerlink" title="2、创建WordCount.scala"></a>2、创建WordCount.scala</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> lnd</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2021/12/14 10:49</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Version</span> 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 1、创建一个 SparkContext</span></span><br><span class="line"><span class="comment"> * 2、从数据源得到一个RDD</span></span><br><span class="line"><span class="comment"> * 3、对RDD做各种转换</span></span><br><span class="line"><span class="comment"> * 4、执行一个行动算子</span></span><br><span class="line"><span class="comment"> * 5、关闭SparkContext</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">object WordCount &#123;</span><br><span class="line"></span><br><span class="line">  def <span class="title function_">main</span><span class="params">(args: Array[String])</span>: Unit = &#123;</span><br><span class="line">    <span class="comment">// 1、创建一个 SparkContext</span></span><br><span class="line">    val sparkConf: SparkConf = <span class="keyword">new</span> <span class="title class_">SparkConf</span>()</span><br><span class="line">    sparkConf.setMaster(<span class="string">&quot;local[2]&quot;</span>)	<span class="comment">// 本地模式</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        	如果想要使用 Yarn 环境运行 WordCount 程序，需要把该代码打成一个 jar 包</span></span><br><span class="line"><span class="comment">        	上传到 Linux 中去执行，打包的时候，需要把 Master 的设置去掉，在 Linux 下</span></span><br><span class="line"><span class="comment">        	提交的时候再使用 --master 来显式设置 master 为 Yarn</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">    sparkConf.setAppName(<span class="string">&quot;wc&quot;</span>)</span><br><span class="line">    val sc: SparkContext = <span class="keyword">new</span> <span class="title class_">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、从数据源得到一个RDD</span></span><br><span class="line">    <span class="type">val</span> <span class="variable">lineRDD</span> <span class="operator">=</span> sc.textFile(args(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、对RDD做各种转换</span></span><br><span class="line">    <span class="type">val</span> <span class="variable">resultRDD</span> <span class="operator">=</span> lineRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、执行一个行动算子【Collect：把各个节点计算后的数据，拉取到驱动端】</span></span><br><span class="line">    val wordCountArr: Array[(String, Int)] = resultRDD.collect()</span><br><span class="line">    wordCountArr.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5、关闭SparkContext</span></span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="3、测试"><a href="#3、测试" class="headerlink" title="3、测试"></a>3、测试</h2><h3 id="本地模式测试"><a href="#本地模式测试" class="headerlink" title="本地模式测试"></a>本地模式测试</h3><p>使用 local 模式执行，相当于代码是在 window 下执行的。</p>
<p>在执行 WordCount 程序时，我们文件的路径 path 是从 main() 方法的 args(0) 中获取的，下图为向 args() 参数类表中传参的过程。</p>
<p><img src="/2021/12/11/Spark/image-20211214110732541.png" alt="image-20211214110732541"></p>
<h3 id="打包到-Linux-下测试"><a href="#打包到-Linux-下测试" class="headerlink" title="打包到 Linux 下测试"></a>打包到 Linux 下测试</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sparkConf.setMaster(<span class="string">&quot;local[2]&quot;</span>)	<span class="comment">// 本地模式</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    	如果想要使用 Yarn 环境运行 WordCount 程序，需要把该代码打成一个 jar 包</span></span><br><span class="line"><span class="comment">    	上传到 Linux 中去执行，打包的时候，需要把 Master 的设置去掉，在 Linux 下</span></span><br><span class="line"><span class="comment">    	提交的时候再使用 --master 来显式设置 master 为 Yarn</span></span><br><span class="line"><span class="comment">    */</span></span><br></pre></td></tr></table></figure>



<ol>
<li><p> 打包：<code>Maven --&gt; package</code></p>
</li>
<li><p>这里我们只需对 spark-core 项目进行打包即可，因为我们代码相关的东西都在这个包下</p>
<p> <img src="/2021/12/11/Spark/image-20211214112248174.png" alt="image-20211214112248174"></p>
</li>
<li><p>打包成功后，在 <code>target</code> 目录下会多出来一个 jar 包</p>
<p> <img src="/2021/12/11/Spark/image-20211214112620970.png" alt="image-20211214112620970"></p>
</li>
<li><p> 将 jar 包上传到 Linux 中 Spark 的安装目录下，即<code>/opt/module/spark-yarn</code></p>
</li>
<li><p>执行如下命令</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-yarn]$ bin/spark-submit --master yarn --deploy-mode client --class WordCount ./spark-core-1.0-SNAPSHOT.jar hdfs://hadoop102:9000/wcinput</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 命令分解</span></span><br><span class="line">bin/spark-submit	<span class="comment"># 提交任务</span></span><br><span class="line">--master yarn 	<span class="comment"># 指定Master</span></span><br><span class="line">--deploy-mode client <span class="comment"># 指定deploy-mode</span></span><br><span class="line">--class WordCount <span class="comment"># 全限定类名</span></span><br><span class="line">./spark-core-1.0-SNAPSHOT.jar 	<span class="comment"># Application应用jar包的位置</span></span><br><span class="line">hdfs://hadoop102:9000/wcinput	<span class="comment"># main() 方法中传入的参数</span></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/07/Scala/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/07/Scala/" class="post-title-link" itemprop="url">Scala</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-07 11:34:21" itemprop="dateCreated datePublished" datetime="2021-12-07T11:34:21+08:00">2021-12-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-12-23 14:36:07" itemprop="dateModified" datetime="2021-12-23T14:36:07+08:00">2021-12-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Scala/" itemprop="url" rel="index"><span itemprop="name">Scala</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h1><h2 id="6-高阶函数"><a href="#6-高阶函数" class="headerlink" title="6    高阶函数"></a>6    高阶函数</h2><p>说明</p>
<ul>
<li>  定义：<strong>参数为函数的函数称为高阶函数</strong> </li>
</ul>
<p>案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//高阶函数————函数作为参数</span></span><br><span class="line">        <span class="comment">/* </span></span><br><span class="line"><span class="comment">        	参数a，类型是 Int</span></span><br><span class="line"><span class="comment">        	参数b，类型是 Int</span></span><br><span class="line"><span class="comment">        	参数operater，类型是函数</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">calculator</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>, operater: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">            operater(a, b)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//函数————求和</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">plus</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">            x + y</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//方法————求积</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">multiply</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">            x * y</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//函数作为参数</span></span><br><span class="line">        println(calculator(<span class="number">2</span>, <span class="number">3</span>, plus))</span><br><span class="line">        println(calculator(<span class="number">2</span>, <span class="number">3</span>, multiply))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<hr>
<h2 id="8-函数柯里化-amp-闭包"><a href="#8-函数柯里化-amp-闭包" class="headerlink" title="8 函数柯里化&amp;闭包"></a>8 函数柯里化&amp;闭包</h2><p><strong>说明</strong></p>
<p>函数柯里化：将一个接收多个参数的函数转化成一个接受一个参数的函数过程，可以简单的理解为一种<strong>特殊的参数列表声明方式</strong>。</p>
<p>闭包：就是<strong>一个函数</strong>和与<strong>其相关的引用环境（变量）</strong>组合的一个<strong>整体</strong>(实体)</p>
<p>【闭包会阻止外部局部变量的销毁，会把使用到的局部变量的生命周期延长到函数的外部】</p>
<p><strong>案例实操</strong></p>
<ol>
<li><p>闭包</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">    </span><br></pre></td></tr></table></figure>

</li>
<li><p>柯里化</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestFunction</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 匿名函数，接收三个参数</span></span><br><span class="line">    <span class="keyword">val</span> sum = (x: <span class="type">Int</span>, y: <span class="type">Int</span>, z: <span class="type">Int</span>) =&gt; x + y + z</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 分别接收三个参数</span></span><br><span class="line">    <span class="keyword">val</span> sum1 = (x: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">        y: <span class="type">Int</span> =&gt; &#123;</span><br><span class="line">            z: <span class="type">Int</span> =&gt; &#123;</span><br><span class="line">                x + y + z</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 方式二</span></span><br><span class="line">    <span class="keyword">val</span> sum2 = (x: <span class="type">Int</span>) =&gt; (y: <span class="type">Int</span>) =&gt; (z: <span class="type">Int</span>) =&gt; x + y + z</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sum3</span></span>(x: <span class="type">Int</span>)(y: <span class="type">Int</span>)(z: <span class="type">Int</span>) = x + y + z</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 测试</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        sum(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">        sum1(<span class="number">1</span>)(<span class="number">2</span>)(<span class="number">3</span>)</span><br><span class="line">        sum2(<span class="number">1</span>)(<span class="number">2</span>)(<span class="number">3</span>)</span><br><span class="line">        sum3(<span class="number">1</span>)(<span class="number">2</span>)(<span class="number">3</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="11-惰性求值"><a href="#11-惰性求值" class="headerlink" title="11 惰性求值"></a>11 惰性求值</h2><p>1）说明</p>
<p>当函数返回值被声明为lazy时，函数的执行将被推迟，直到我们首次对此取值，该函数才会执行。这种函数我们称之为惰性函数。</p>
<p>2）案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// lazy修饰</span></span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> res = sum(<span class="number">10</span>, <span class="number">30</span>)</span><br><span class="line">    println(<span class="string">&quot;----------------&quot;</span>)</span><br><span class="line">    println(<span class="string">&quot;res=&quot;</span> + res)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(n1: <span class="type">Int</span>, n2: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    println(<span class="string">&quot;sum被执行。。。&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> n1 + n2</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意：lazy不能修饰var类型的变量</p>
<hr>
<h1 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h1><h2 id="类和对象"><a href="#类和对象" class="headerlink" title="类和对象"></a>类和对象</h2><h3 id="6-2-1-定义类"><a href="#6-2-1-定义类" class="headerlink" title="6.2.1 定义类"></a>6.2.1 定义类</h3><ol>
<li><p>基本语法</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[修饰符] <span class="class"><span class="keyword">class</span> <span class="title">类名</span> </span>&#123;</span><br><span class="line">  类体</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<ul>
<li>说明<ol>
<li> Scala语法中，类并<strong>不能声明为public</strong>，所有这些类默认就是public</li>
<li> 一个Scala源文件可以包含多个类</li>
</ol>
</li>
</ul>
</li>
<li><p>案例实操</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">    </span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="6-2-2-属性"><a href="#6-2-2-属性" class="headerlink" title="6.2.2 属性"></a>6.2.2 属性</h3><p>属性是类的一个组成部分</p>
<ol>
<li> 基本语法</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[修饰符] <span class="keyword">var</span> 属性名称 [：类型] = 属性值</span><br></pre></td></tr></table></figure>

<ul>
<li>  注：Bean属性（@BeanPropetry），可以自动生成规范的setXxx/getXxx方法</li>
</ul>
<ol start="2">
<li> 案例实操</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.scala.test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.beans.<span class="type">BeanProperty</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> name: <span class="type">String</span> = <span class="string">&quot;bobo&quot;</span> <span class="comment">//定义属性</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> age: <span class="type">Int</span> = _ <span class="comment">// _表示给属性一个默认值</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//Bean属性（@BeanProperty）</span></span><br><span class="line">  <span class="meta">@BeanProperty</span> <span class="keyword">var</span> sex: <span class="type">String</span> = <span class="string">&quot;男&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> person = <span class="keyword">new</span> <span class="type">Person</span>()</span><br><span class="line">    println(person.name)</span><br><span class="line"></span><br><span class="line">    person.setSex(<span class="string">&quot;女&quot;</span>)</span><br><span class="line">    println(person.getSex)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="6-2-4-创建对象"><a href="#6-2-4-创建对象" class="headerlink" title="6.2.4 创建对象"></a>6.2.4 创建对象</h3><p>1）基本语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val | var 对象名 [：类型] = new 类型()</span><br></pre></td></tr></table></figure>



<p>2）案例实操</p>
<p>（1）val修饰对象，不能改变对象的引用（即：内存地址），可以改变对象属性的值。</p>
<p>（2）var修饰对象，可以修改对象的引用和修改对象的属性值</p>
<hr>
<h3 id="6-2-5-构造器"><a href="#6-2-5-构造器" class="headerlink" title="6.2.5 构造器"></a>6.2.5 构造器</h3><p>和Java一样，Scala构造对象也需要调用构造方法，并且可以有任意多个构造方法。</p>
<p>Scala类的构造器包括：<strong>主构造器和辅助构造器</strong></p>
<p>1）基本语法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">类名</span>(<span class="params">形参列表</span>) </span>&#123;  <span class="comment">// 主构造器</span></span><br><span class="line">    <span class="comment">// 类体</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">this</span></span>(形参列表) &#123;  <span class="comment">// 辅助构造器</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">this</span></span>(形参列表) &#123;  <span class="comment">//辅助构造器可以有多个...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>



<p>说明：</p>
<p>（1）辅助构造器，函数的名称this，可以有多个，编译器通过参数的个数来区分。</p>
<p>（2）<strong>辅助构造方法不能直接构建对象，必须直接或者间接调用主构造方法</strong>。</p>
<p>2）案例实操</p>
<p>（1）<strong>如果主构造器无参数，小括号可省略</strong>，构建对象时调用的构造方法的小括号也可以省略。</p>
<h3 id="6-2-6-构造器参数"><a href="#6-2-6-构造器参数" class="headerlink" title="6.2.6 构造器参数"></a>6.2.6 构造器参数</h3><p>1）说明</p>
<p>Scala类的主构造器函数的形参包括三种类型：未用任何修饰、var修饰、val修饰</p>
<p>（1）未用任何修饰符修饰，这个参数就是一个局部变量</p>
<p>（2）var修饰参数，作为类的成员属性使用，可以修改</p>
<p>（3）val修饰参数，作为类只读属性使用，不能修改</p>
<p>2）案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, var age: <span class="type">Int</span>, val sex: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">var</span> person = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">&quot;bobo&quot;</span>, <span class="number">18</span>, <span class="string">&quot;男&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// （1）未用任何修饰符修饰，这个参数就是一个局部变量，在类之外的地方不能被访问到</span></span><br><span class="line">        <span class="comment">// printf(person.name)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// （2）var修饰参数，作为类的成员属性使用，可以修改</span></span><br><span class="line">        person.age = <span class="number">19</span></span><br><span class="line">        println(person.age)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// （3）val修饰参数，作为类的只读属性使用，不能修改</span></span><br><span class="line">        <span class="comment">// person.sex = &quot;女&quot;</span></span><br><span class="line">        println(person.sex)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="6-5-抽象属性和抽象方法"><a href="#6-5-抽象属性和抽象方法" class="headerlink" title="6.5 抽象属性和抽象方法"></a>6.5 抽象属性和抽象方法</h2><h3 id="6-5-1-抽象属性和抽象方法"><a href="#6-5-1-抽象属性和抽象方法" class="headerlink" title="6.5.1 抽象属性和抽象方法"></a>6.5.1 抽象属性和抽象方法</h3><p>一、基本语法</p>
<p>  1.定义抽象类：abstract class Person{} //通过abstract关键字标记抽象类</p>
<ol start="2">
<li> 定义抽象属性：val|var name:String //一个属性没有初始化，就是抽象属性</li>
</ol>
<p>​    （3）定义抽象方法：def hello():String //只声明而没有实现的方法，就是抽象方法</p>
<p>二、继承&amp;重写</p>
<p>java 中只支持方法重写，而 scala 中还支持属性重写</p>
<p>（1）如果父类为抽象类，那么子类需要将抽象的属性和方法实现，否则子类也需声明为抽象类</p>
<p>（2）重写非抽象方法需要用override修饰，重写抽象方法则可以不加override。</p>
<p>（3）子类中调用父类的方法使用super关键字</p>
<p>（4）<strong>属性重写只支持val类型，而不支持var</strong>。 </p>
<hr>
<h2 id="6-6-单例对象（伴生对象）"><a href="#6-6-单例对象（伴生对象）" class="headerlink" title="6.6 单例对象（伴生对象）"></a>6.6 单例对象（伴生对象）</h2><p>Scala语言是完全面向对象的语言，所以并没有静态的操作（即在Scala中没有静态的概念）。但是为了能够和 JVM 交互（因为 JVM 字节码中有静态概念），就产生了一种特殊的对象来模拟类对象，该对象为<strong>单例对象</strong>。若单例对象名与类名一致，则称该单例对象这个类的<strong>伴生对象</strong>，这个类的所有“静态”内容都可以放置在它的伴生对象中声明。</p>
<p><strong>伴生对象和伴生类</strong>：编译成字节码之后，伴生对象中的成员就是一个 .class 文件中的静态成员，伴生类中的成员就是该文件中的普通成员。并且伴生类和伴生对象可以互相访问对方的私有成员。</p>
<p>1）基本语法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">    <span class="keyword">val</span> country:<span class="type">String</span>=<span class="string">&quot;China&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>2）说明</p>
<p>（1）单例对象采用object关键字声明</p>
<p>（2）单例对象对应的类称之为伴生类，伴生对象的名称应该和伴生类名一致。</p>
<p>（3）<strong>单例对象中的属性和方法都可以通过伴生对象名（类名）直接调用访问。</strong></p>
<h3 id="6-6-2-apply方法"><a href="#6-6-2-apply方法" class="headerlink" title="6.6.2 apply方法"></a>6.6.2 apply方法</h3><p>1）说明</p>
<p>（1）通过伴生对象的apply方法，实现不使用new方法创建对象。</p>
<p>（2）如果想让主构造器变成私有的，可以在()之前加上private。</p>
<p>（3）apply方法可以重载。</p>
<p>（4）Scala中 <code>obj(arg)</code> 语句实际是在调用该对象的 <code>apply()</code> 方法，即 <code>obj.apply(arg)</code>。用以同一面向对象编程和函数式编程的风格。</p>
<p>2）案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//（1）通过伴生对象的apply方法，实现不使用new关键字创建对象。</span></span><br><span class="line">        <span class="keyword">val</span> p1 = <span class="type">Person</span>()</span><br><span class="line">        println(<span class="string">&quot;p1.name=&quot;</span> + p1.name)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> p2 = <span class="type">Person</span>(<span class="string">&quot;bobo&quot;</span>)</span><br><span class="line">        println(<span class="string">&quot;p2.name=&quot;</span> + p2.name)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//（2）如果想让主构造器变成私有的，可以在()之前加上private</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="title">private</span>(<span class="params">cName: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> name: <span class="type">String</span> = cName</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(): <span class="type">Person</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;apply空参被调用&quot;</span>)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">&quot;xx&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(name: <span class="type">String</span>): <span class="type">Person</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;apply有参被调用&quot;</span>)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Person</span>(name)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="trait"><a href="#trait" class="headerlink" title="trait"></a>trait</h2><p><strong>trait</strong>：特质。类似于 Java 中的接口，编译后就会成为 JVM 中的接口</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">抽象类中有的东西，trait 中都会有。trait 与抽象类不同之处在于多继承。</span><br></pre></td></tr></table></figure>

<p>Scala 语言中，采用特质trait（特征）来代替接口的概念，也就是说，多个类具有相同的特征（特征）时，就可以将这个特质（特征）独立出来，采用关键字 <code>trait</code> 声明。</p>
<p>Scala 中的 <code>trait</code> 中既可以有抽象属性和方法，也可以有具体的属性和方法<strong>，</strong>一个类可以混入（mixin）多个特质。</p>
<p>Scala引入trait特征，第一可以替代Java的接口，第二个也是对单继承机制的一种补充。</p>
<h3 id="6-7-1-特质声明"><a href="#6-7-1-特质声明" class="headerlink" title="6.7.1 特质声明"></a>6.7.1 特质声明</h3><p>1）基本语法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">特质名</span> </span>&#123;</span><br><span class="line">	<span class="class"><span class="keyword">trait</span><span class="title">体</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PersonTrait</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 声明属性</span></span><br><span class="line">    <span class="keyword">var</span> name:<span class="type">String</span> = _</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 声明方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eat</span></span>():<span class="type">Unit</span>=&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 抽象属性</span></span><br><span class="line">    <span class="keyword">var</span> age:<span class="type">Int</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 抽象方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">say</span></span>():<span class="type">Unit</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="6-7-2-特质基本语法"><a href="#6-7-2-特质基本语法" class="headerlink" title="6.7.2 特质基本语法"></a>6.7.2 特质基本语法</h3><p>一个类具有某种特质（特征），就意味着这个类满足了这个特质（特征）的所有要素，所以在使用时，也采用了extends关键字，如果有多个特质或存在父类，那么需要采用with关键字连接。</p>
<p>1）基本语法：</p>
<p><strong>没有父类</strong>：class 类名 <strong>extends</strong> 特质1  <strong>with</strong>  特质2  <strong>with</strong>  特质3 …</p>
<p><strong>有父类</strong>：class 类名  <strong>extends</strong> 父类  <strong>with</strong> 特质1  <strong>with</strong>  特质2 <strong>with</strong> 特质3…</p>
<p>2）说明</p>
<p>  1.类和特质的关系：使用继承的关系。</p>
<ol start="2">
<li> 当一个类去继承特质时，第一个连接词是extends，后面是with。</li>
</ol>
<p>​    （3）如果一个类在继承特质和父类时，应当把父类写在extends后。</p>
<p>3）案例实操</p>
<p>（1）<strong>特质可以同时拥有抽象方法和具体方法</strong></p>
<p>（2）一个类可以混入（mixin）多个特质</p>
<p>（3）所有的Java接口都可以当做Scala特质使用</p>
<p>（4）<strong>动态混入</strong>：可灵活的扩展类的功能</p>
<ul>
<li>  （4.1）动态混入：在创建对象的时候再混入trait，而无需直接在类上混入该trait</li>
<li>  （4.2）如果混入的trait中有未实现的方法，则需要实现</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PersonTrait</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//（1）特质可以同时拥有抽象方法和具体方法</span></span><br><span class="line">  <span class="comment">// 声明属性</span></span><br><span class="line">  <span class="keyword">var</span> name: <span class="type">String</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 抽象属性</span></span><br><span class="line">  <span class="keyword">var</span> age: <span class="type">Int</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 声明方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eat</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">&quot;eat&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 抽象方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">say</span></span>(): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">SexTrait</span> </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> sex: <span class="type">String</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//（2）一个类可以实现/继承多个特质</span></span><br><span class="line"><span class="comment">//（3）所有的Java接口都可以当做Scala特质使用</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Teacher</span> <span class="keyword">extends</span> <span class="title">PersonTrait</span> <span class="keyword">with</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">say</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">&quot;say&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">var</span> age: <span class="type">Int</span> = _</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestTrait</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> teacher = <span class="keyword">new</span> <span class="type">Teacher</span></span><br><span class="line"></span><br><span class="line">    teacher.say()</span><br><span class="line">    teacher.eat()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//（4）动态混入：可灵活的扩展类的功能</span></span><br><span class="line">    <span class="keyword">val</span> t2 = <span class="keyword">new</span> <span class="type">Teacher</span> <span class="keyword">with</span> <span class="type">SexTrait</span> &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="keyword">var</span> sex: <span class="type">String</span> = <span class="string">&quot;男&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//调用混入trait的属性</span></span><br><span class="line">    println(t2.sex)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<hr>
<h3 id="6-7-3-特质叠加"><a href="#6-7-3-特质叠加" class="headerlink" title="6.7.3 特质叠加"></a>6.7.3 特质叠加</h3><p>由于一个类可以混入（mixin）多个 trait，且 trait 中可以有具体的属性和方法，若混入的特质中具有相同的方法（方法名，参数列表，返回值均相同），必然会出现继承冲突问题。冲突分为以下两种：</p>
<p>第一种，一个类（Sub）混入的两个trait（TraitA，TraitB）中具有相同的具体方法，且两个trait之间没有任何关系，解决这类冲突问题，直接在类（Sub）中重写冲突方法。</p>
<p><img src="/2021/12/07/Scala/image-20211210134503220.png" alt="image-20211210134503220"></p>
<p>​                               </p>
<p>第二种，一个类（Sub）混入的两个trait（TraitA，TraitB）中具有相同的具体方法，且两个trait继承自相同的trait（TraitC），及所谓的“钻石问题”，解决这类冲突问题，Scala采用了<strong>特质叠加</strong>的策略。</p>
<p> <img src="/2021/12/07/Scala/image-20211210134518909.png" alt="image-20211210134518909"></p>
<p>所谓的<strong>特质叠加</strong>，就是将混入的多个trait中的冲突方法叠加起来，案例如下，</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Ball</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">describe</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="string">&quot;ball&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Color</span> <span class="keyword">extends</span> <span class="title">Ball</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">describe</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="string">&quot;blue-&quot;</span> + <span class="keyword">super</span>.describe()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Category</span> <span class="keyword">extends</span> <span class="title">Ball</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">describe</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="string">&quot;foot-&quot;</span> + <span class="keyword">super</span>.describe()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyBall</span> <span class="keyword">extends</span> <span class="title">Category</span> <span class="keyword">with</span> <span class="title">Color</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">describe</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="string">&quot;my ball is a &quot;</span> + <span class="keyword">super</span>.describe()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestTrait</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(<span class="keyword">new</span> <span class="type">MyBall</span>().describe())</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 结果为：my ball is a blue-foot-ball</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="6-7-4-特质叠加执行顺序"><a href="#6-7-4-特质叠加执行顺序" class="headerlink" title="6.7.4 特质叠加执行顺序"></a>6.7.4 特质叠加执行顺序</h3><p><strong>思考：</strong>上述案例中的super.describe()调用的是父trait中的方法吗？</p>
<p>当一个类混入多个特质的时候，scala会对所有的特质及其父特质按照一定的顺序进行排序，而此案例中的super.describe()调用的实际上是排好序后的下一个特质中的describe()方法。，排序规则如下：</p>
<p><img src="/2021/12/07/Scala/image-20211210134904813.png" alt="image-20211210134904813"></p>
<p><strong>结论：</strong></p>
<p>（1）案例中的super，不是表示其父特质对象，而是表示上述叠加顺序中的下一个特质，即，MyClass中的super指代Color，Color中的super指代Category，Category中的super指代Ball</p>
<p>（2）如果想要调用某个指定的混入特质中的方法，可以增加约束：super[]，例如super[Category].describe()。</p>
<hr>
<h3 id="6-7-5-特质自身类型"><a href="#6-7-5-特质自身类型" class="headerlink" title="6.7.5 特质自身类型"></a>6.7.5 特质自身类型</h3><p>1）说明</p>
<p>​    自身类型可实现依赖注入的功能。</p>
<p>2）案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">val name: <span class="type">String</span>, val age: <span class="type">Int</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Dao</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(user: <span class="type">User</span>) = &#123;</span><br><span class="line">        println(<span class="string">&quot;insert into database :&quot;</span> + user.name)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">APP</span> </span>&#123;</span><br><span class="line">    _: <span class="type">Dao</span> =&gt;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span></span>(user: <span class="type">User</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;login :&quot;</span> + user.name)</span><br><span class="line">        insert(user)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyApp</span> <span class="keyword">extends</span> <span class="title">APP</span> <span class="keyword">with</span> <span class="title">Dao</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        login(<span class="keyword">new</span> <span class="type">User</span>(<span class="string">&quot;bobo&quot;</span>, <span class="number">11</span>))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="6-8-3-Type定义新类型"><a href="#6-8-3-Type定义新类型" class="headerlink" title="6.8.3 Type定义新类型"></a>6.8.3 Type定义新类型</h3><p>1）说明</p>
<p>使用type关键字可以定义新的数据数据类型名称，本质上就是类型的一个别名</p>
<p>2）案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="class"><span class="keyword">type</span> <span class="title">S=String</span></span></span><br><span class="line">        <span class="keyword">var</span> v:<span class="type">S</span>=<span class="string">&quot;abc&quot;</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">test</span></span>():<span class="type">S</span>=<span class="string">&quot;xyz&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>









<hr>
<h1 id="隐式转换"><a href="#隐式转换" class="headerlink" title="隐式转换"></a>隐式转换</h1><h2 id="隐式函数"><a href="#隐式函数" class="headerlink" title="隐式函数"></a>隐式函数</h2><p><strong>说明：</strong>隐式转换可以在无需修改源代码的情况下，扩展某个类的功能。</p>
<p><strong>Demo：</strong></p>
<p>需求：通过隐式转换为 <code>Int</code> 类增加方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> test</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/23 11:18</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestImplicitFunction</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 隐式转换，接收一个 Int 类型的参数，转换成 MyRichInt 类型的参数</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">convert</span></span>(arg: <span class="type">Int</span>) : <span class="type">MyRichInt</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MyRichInt</span>(arg)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="number">2.</span>myMax(<span class="number">6</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRichInt</span>(<span class="params">val num: <span class="type">Int</span></span>)</span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">myMax</span></span>(x : <span class="type">Int</span>) = &#123;</span><br><span class="line">    <span class="keyword">if</span> (x &gt; num)  </span><br><span class="line">      x</span><br><span class="line">    <span class="keyword">else</span> </span><br><span class="line">      num</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="隐式参数"><a href="#隐式参数" class="headerlink" title="隐式参数"></a>隐式参数</h2><p>普通方法或者函数可以通过 <strong>implicit</strong> 关键字声明隐式参数，调用该方法时，就可以传入该参数，编译器会在相应的作用域寻找符合条件的隐式值。</p>
<p><strong>说明：</strong></p>
<ol>
<li> 同一个作用域中，相同类型的隐式值只能有一个</li>
<li> <strong>编译器按照隐式参数的类型去寻找对应类型的隐式值，与隐式值的名称无关</strong>。</li>
<li> 隐式参数优先于默认参数</li>
</ol>
<p><strong>Demo：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> test</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/23 11:27</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestImplicitParameter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> str: <span class="type">String</span> = <span class="string">&quot;hello world!&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这里的 implicit 用于声明使用隐式参数。即如果有隐式参数就使用，若无则不使用</span></span><br><span class="line">  <span class="comment">// &quot;good bey world!&quot; 是为参数 arg 提供的默认值</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hello</span></span>(<span class="keyword">implicit</span> arg: <span class="type">String</span> = <span class="string">&quot;good bey world!&quot;</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(arg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 在调用 hello 方法时，若不传递参数，由于 hello 方法声明使用了隐式值，</span></span><br><span class="line">    <span class="comment">// 则会从上下文中先查找是否有隐式值，若有则使用，若无则再考虑默认值</span></span><br><span class="line">    hello</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">good bey world!</span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br></pre></td></tr></table></figure>



<hr>
<h2 id="隐式类"><a href="#隐式类" class="headerlink" title="隐式类"></a>隐式类</h2><p>从 <code>Scala2.10</code> 开始提供了隐式类，使用 <code>implicit</code> 修饰类，隐式类非常强大，同样可以扩展类的功能，在集合中隐式类会发挥重要的作用。</p>
<p><strong>隐式类说明</strong></p>
<ol>
<li> 其所带的构造参数有且只能有一个</li>
<li> 隐式类必须被定义在“类”或“伴生对象”或“包对象”里，即隐式类不能是<strong>顶级的</strong>。</li>
</ol>
<p><strong>Demo</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/23 11:35</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *          隐式类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestImplicitClass</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 声明隐式类：</span></span><br><span class="line"><span class="comment">  *   该隐式类接收一个 Int 类型的参数，即在代码的上下文中，</span></span><br><span class="line"><span class="comment">  * 如果某个对象调用的方法的参数列表只有一个 Int 类型，在执行时</span></span><br><span class="line"><span class="comment">  * 就会检查是否是调用该隐式类中的方法。</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRichInt</span>(<span class="params">arg: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">myMax</span></span>(i: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (arg &lt; i) i <span class="keyword">else</span> arg</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">myMin</span></span>(i: <span class="type">Int</span>) = &#123;</span><br><span class="line">      <span class="keyword">if</span> (arg &lt; i) arg <span class="keyword">else</span> i</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="number">1.</span>myMax(<span class="number">3</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<hr>
<h2 id="隐式解析机制"><a href="#隐式解析机制" class="headerlink" title="隐式解析机制"></a>隐式解析机制</h2><p><strong>说明</strong></p>
<ol>
<li> 首先会在当前代码作用域下查找隐式实体（隐式方法、隐式类、隐式对象）。（一般是这种情况）</li>
<li> 如果第一条规则查找隐式实体失败，会继续在隐式参数的类型的作用域里查找。类型的作用域是指与<strong>该类型相关联的全部伴生对象</strong>以及<strong>该类型所在包的包对象</strong>。</li>
</ol>
<p><strong>Demo：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//（2）如果第一条规则查找隐式实体失败，会继续在隐式参数的类型的作用域里查找。类型的作用域是指与该类型相关联的全部伴生模块，</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestTransform</span> <span class="keyword">extends</span> <span class="title">PersonTrait</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//（1）首先会在当前代码作用域下查找隐式实体</span></span><br><span class="line">        <span class="keyword">val</span> teacher = <span class="keyword">new</span> <span class="type">Teacher</span>()</span><br><span class="line">        teacher.eat()</span><br><span class="line">        teacher.say()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Teacher</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">eat</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println(<span class="string">&quot;eat...&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PersonTrait</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PersonTrait</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 隐式类 : 类型1 =&gt; 类型2</span></span><br><span class="line">    <span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">Person5</span>(<span class="params">user:<span class="type">Teacher</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">say</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println(<span class="string">&quot;say...&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/06/Mahout/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/06/Mahout/" class="post-title-link" itemprop="url">Mahout</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-06 10:28:21" itemprop="dateCreated datePublished" datetime="2021-12-06T10:28:21+08:00">2021-12-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-12-07 11:32:54" itemprop="dateModified" datetime="2021-12-07T11:32:54+08:00">2021-12-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>  参考视频时间：</p>
<ul>
<li>  mahout01d==09：00</li>
<li></li>
</ul>
</blockquote>
<h2 id="传统工具的困境"><a href="#传统工具的困境" class="headerlink" title="传统工具的困境"></a>传统工具的困境</h2><ul>
<li>  处理数据量受限于内存，因此无法处理海量数据</li>
<li>  能处理海量数据的软件，却又缺乏有效快速专业的分析功能</li>
<li>  可以采用抽样等方法，但有局限性，比如对于聚类，推荐系统则无法使用抽样</li>
<li>  解决方向：<strong>Hadoop集群和Map-Reduce并行计算</strong></li>
</ul>
<p><strong>抽样的局限性</strong>，需要使用全量数据分析</p>
<p>使用全量数据进行建模。将模型求解出来后，将模型部署到真实的生产环境中去。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/04/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/04/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/" class="post-title-link" itemprop="url">装饰器模式</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-04 21:57:27" itemprop="dateCreated datePublished" datetime="2021-12-04T21:57:27+08:00">2021-12-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-12-05 10:23:18" itemprop="dateModified" datetime="2021-12-05T10:23:18+08:00">2021-12-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/" itemprop="url" rel="index"><span itemprop="name">装饰器模式</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<ul>
<li>  装饰者</li>
<li>  被装饰者</li>
<li>  装饰</li>
<li>  委托</li>
<li>  组合</li>
<li></li>
</ul>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/29/ZooKeeper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/11/29/ZooKeeper/" class="post-title-link" itemprop="url">ZooKeeper</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-11-29 21:37:18" itemprop="dateCreated datePublished" datetime="2021-11-29T21:37:18+08:00">2021-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-12-13 15:55:18" itemprop="dateModified" datetime="2021-12-13T15:55:18+08:00">2021-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Zookeeper/" itemprop="url" rel="index"><span itemprop="name">Zookeeper</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>  官网地址：<a target="_blank" rel="noopener" href="https://zookeeper.apache.org/">https://zookeeper.apache.org/</a></p>
</blockquote>
<h1 id="ZooKeeper-理论基础"><a href="#ZooKeeper-理论基础" class="headerlink" title="ZooKeeper 理论基础"></a>ZooKeeper 理论基础</h1><h2 id="ZooKeeper-工作机制"><a href="#ZooKeeper-工作机制" class="headerlink" title="ZooKeeper 工作机制"></a>ZooKeeper 工作机制</h2><p>从设计模式角度来理解，Zookeeper 是一个基于观察者模式的分布式<strong>服务管理</strong>框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper 将会负责将变化通知那些已注册并订阅服务的观察者。</p>
<h2 id="ZooKeeper-特点"><a href="#ZooKeeper-特点" class="headerlink" title="ZooKeeper 特点"></a>ZooKeeper 特点</h2><ol>
<li> Zookeeper 是由一个 Leader（领导者）和多个 Follower（跟随者）组成的集群。</li>
<li> 在 ZK 集群中，只要有半数以上节点存活，集群就能正常提供服务。所以 ZK 集群一般会安装奇数台服务器。比如 8 个节点的集群只有 4 个节点存活就不能正常运行，而 7 个节点的集群只需要 4 个节点存活就能正常运行。</li>
<li> 全局数据一致：每个服务器节点保存一份相同的数据副本，Client 无论连接到哪个节点数据都是一致的。</li>
<li> 更新请求顺序执行：来自同一个 Client 的更新请求按其发送顺序依次执行。</li>
<li> 数据更新原子性：一次数据更新要么成功，要么失败。</li>
<li> 实时性：在一定时间范围内，Client 能读到最新数据。</li>
</ol>
<h2 id="ZooKeeper-数据结构"><a href="#ZooKeeper-数据结构" class="headerlink" title="ZooKeeper 数据结构"></a>ZooKeeper 数据结构</h2><p>ZooKeeper 的数据结构有点类似于 Linux，整体是一个文件系统，可以看作是一棵树，从根路径<code>/</code> 出发可以到达任意位置。</p>
<p>每个节点称为一个 <strong>ZNode</strong>。每个 ZNode 默认能够存储 1MB 的数据，每个 ZNode 都可以通过其路径唯一标识。</p>
<p><img src="/2021/11/29/ZooKeeper/image-20211129222922413.png" alt="image-20211129222922413"></p>
<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>ZooKeeper 提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。</p>
<h3 id="软负载均衡-vs-硬负载均衡"><a href="#软负载均衡-vs-硬负载均衡" class="headerlink" title="软负载均衡 vs. 硬负载均衡"></a>软负载均衡 vs. 硬负载均衡</h3><p><strong>软负载均衡</strong>是指，假设集群中现有 3 台机器，A 机器承担了 60% 的流量，B 机器承担了 30% 的流量，C 机器只承担了 10% 的流量，即集群中功能相同的 3 台机器的负载不均衡，为了让负载能均衡一点，接下来的大部分请求就都会被分配到 C 机器上进行处理。所谓的软负载均衡，就是在软件层面，通过一些算法，让集群中多台机器承担的流量尽可能地相近。</p>
<p>而<strong>硬负载均衡</strong>则是在硬件上的突破，即如果某台机器承担的流量非常大，那么就新增一台机器去分担它的流量，达到负载均衡的目的。</p>
<hr>
<h1 id="ZooKeeper-安装"><a href="#ZooKeeper-安装" class="headerlink" title="ZooKeeper 安装"></a>ZooKeeper 安装</h1><h2 id="安装前准备"><a href="#安装前准备" class="headerlink" title="安装前准备"></a>安装前准备</h2><h3 id="JDK-环境"><a href="#JDK-环境" class="headerlink" title="JDK 环境"></a>JDK 环境</h3><p>由于 ZooKeeper 是基于 Java 开发的，所以在安装 ZooKeeper 前必须保证当前计算机上已经安装了 JDK，并配置了 JAVA_HOME 环境变量。</p>
<hr>
<h3 id="安装包选择"><a href="#安装包选择" class="headerlink" title="安装包选择"></a>安装包选择</h3><p><strong>问题描述：</strong><br>在安装 <code>zookeeper-3.5.7</code> 时，已经在 <code>conf</code> 文件夹下拷贝并重命名了一份 <code>zoo.cfg</code> 文件，结果在执行 <code>bin</code> 目录下的 <code>zkServer.sh</code> 脚本时依然启动失败，查看日志中的错误信息，提示如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">找不到或无法加载主类 org.apache.zookeeper.server.quorum.QuorumPeerMain</span><br></pre></td></tr></table></figure>

<p><strong>原因分析：</strong></p>
<ul>
<li>  使用的是未编译的 jar 包。</li>
<li>  注：ZooKeeper 从 3.5 版本开始，命名就发生了改变，如果是 <code>apache-zookeeper-3.5.5.tar.gz</code> 这种命名的，都是未编译的，而 <code>apache-zookeeper-3.5.5-bin.tar.gz</code> 这种命名的，才是已编译的包。</li>
</ul>
<p><strong>解决方案：</strong></p>
<ul>
<li>  重新下载 <code>apache-zookeeper-3.5.5-bin.tar.gz</code> 包，然后解压使用。</li>
</ul>
<hr>
<h2 id="本地模式安装部署（Linux系统）"><a href="#本地模式安装部署（Linux系统）" class="headerlink" title="本地模式安装部署（Linux系统）"></a>本地模式安装部署（Linux系统）</h2><ol>
<li><p>拷贝 ZooKeeper 安装包<code>（apache-zookeeper-3.5.7-bin-bin.tar.gz）</code>到 Linux 系统下的指定目录<code>(/opt/software)</code>中。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 software]$ ll</span><br><span class="line">总用量 375040</span><br><span class="line">-rw-rw-r--. 1 lvnengdong lvnengdong   3130819 11月 30 11:09 apache-zookeeper-3.5.7-bin-bin.tar.gz	<span class="comment"># ZooKeeper安装包</span></span><br></pre></td></tr></table></figure></li>
<li><p>将安装包解压到 <code>/opt/module</code> 目录下</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 software]$ tar -zxvf apache-zookeeper-3.5.7-bin-bin.tar.gz -C ../module/</span><br></pre></td></tr></table></figure></li>
<li><p>ZooKeeper 目录简析</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin-bin]$ ll</span><br><span class="line">总用量 32</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong   232 2月  10 2020 bin	<span class="comment"># 保存运行ZooKeeper的脚本文件</span></span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong    77 2月   7 2020 conf	<span class="comment"># 配置文件目录</span></span><br><span class="line">drwxr-xr-x. 5 lvnengdong lvnengdong  4096 2月  10 2020 docs</span><br><span class="line">drwxrwxr-x. 2 lvnengdong lvnengdong  4096 11月 30 12:02 lib</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong 11358 9月  13 2018 LICENSE.txt</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   432 2月  10 2020 NOTICE.txt</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong  1560 2月   7 2020 README.md</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong  1347 2月   7 2020 README_packaging.txt</span><br></pre></td></tr></table></figure>

</li>
<li><p><code>bin</code> 目录简析</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin-bin]$ ll bin</span><br><span class="line">总用量 56</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong  232 5月   4 2018 README.txt</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 2067 2月   7 2020 zkCleanup.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1158 2月  10 2020 zkCli.cmd</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1621 2月   7 2020 zkCli.sh	<span class="comment"># 启动ZK客户端</span></span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1766 2月   7 2020 zkEnv.cmd</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 3690 1月  31 2020 zkEnv.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1286 1月  31 2020 zkServer.cmd</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 4573 2月   7 2020 zkServer-initialize.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 9386 2月   7 2020 zkServer.sh	<span class="comment"># 启动ZK服务器端</span></span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong  996 10月  3 2019 zkTxnLogToolkit.cmd</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1385 2月   7 2020 zkTxnLogToolkit.sh</span><br></pre></td></tr></table></figure>

</li>
<li><p>尝试启动 ZooKeeper 服务器端</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin-bin]$ bin/zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/apache-zookeeper-3.5.7-bin/bin/../conf/zoo.cfg</span><br><span class="line">grep: /opt/module/apache-zookeeper-3.5.7-bin/bin/../conf/zoo.cfg: 没有那个文件或目录</span><br><span class="line">grep: /opt/module/apache-zookeeper-3.5.7-bin/bin/../conf/zoo.cfg: 没有那个文件或目录</span><br><span class="line"><span class="built_in">mkdir</span>: 无法创建目录<span class="string">&quot;&quot;</span>: 没有那个文件或目录</span><br><span class="line">Usage: bin/zkServer.sh [--config &lt;conf-dir&gt;] &#123;start|start-foreground|stop|restart|status|print-cmd&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>  异常分析：ZK 启动时要加载 <code>/conf/zoo.cfg</code> 配置文件，而我们当前的服务器上并没有创建这个配置文件，所以我们需要去创建这个配置文件。</li>
</ul>
</li>
<li><p>修改配置文件</p>
<ul>
<li><p>在 <code>conf</code> 目录下有一个文件叫做 <code>zoo_sample.cfg</code>，这是一个配置文件的模板，我们可以复制其副本为 <code>zoo.cfg</code>，作为 ZK 启动时加载的配置文件，在这个基础上进行自定义修改。</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ <span class="built_in">cd</span> conf/</span><br><span class="line">[lvnengdong@hadoop102 conf]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong  535 5月   4 2018 configuration.xsl</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong 2712 2月   7 2020 log4j.properties</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong  922 2月   7 2020 zoo_sample.cfg</span><br></pre></td></tr></table></figure></li>
<li><p>拷贝配置文件</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 conf]$ <span class="built_in">cp</span> zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure></li>
<li><p>修改配置文件 <code>zoo.cfg</code>【配置文件分析见下文】</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 conf]$ vim zoo.cfg</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 dataDir 属性为：</span></span><br><span class="line">dataDir=/opt/module/apache-zookeeper-3.5.7-bin-bin/data</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>创建 <code>/opt/module/apache-zookeeper-3.5.7-bin/data</code>目录</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ <span class="built_in">mkdir</span> data</span><br></pre></td></tr></table></figure></li>
<li><p>再次尝试启动 ZK</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ bin/zkServer.sh --<span class="built_in">help</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/apache-zookeeper-3.5.7-bin/bin/../conf/zoo.cfg</span><br><span class="line">Usage: bin/zkServer.sh [--config &lt;conf-dir&gt;] &#123;start|start-foreground|stop|restart|status|print-cmd&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动ZK</span></span><br><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin-bin]$ bin/zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/apache-zookeeper-3.5.7-bin-bin/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure>

</li>
<li><p>查看是否启动成功</p>
<ul>
<li><p>JPS</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin-bin]$ jps</span><br><span class="line">80707 QuorumPeerMain</span><br></pre></td></tr></table></figure></li>
<li><p>执行 <code>zkServer.sh status</code> 脚本查看 ZK 的状态</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin-bin]$ bin/zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/apache-zookeeper-3.5.7-bin-bin/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: standalone	<span class="comment"># 单机模式</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<hr>
<h2 id="ZK-配置文件分析"><a href="#ZK-配置文件分析" class="headerlink" title="ZK 配置文件分析"></a>ZK 配置文件分析</h2><p><strong>zoo.cfg</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The number of milliseconds of each tick</span></span><br><span class="line"><span class="comment"># Zookeeper服务器与客户端的心跳间隔时间，单位毫秒。</span></span><br><span class="line"><span class="comment"># 即默认每 2000ms 客户端与服务器端进行一次心跳</span></span><br><span class="line"><span class="comment"># “心跳”是Zookeeper中使用的基本时间单位</span></span><br><span class="line">tickTime=2000</span><br><span class="line"></span><br><span class="line"><span class="comment"># The number of ticks that the initial </span></span><br><span class="line"><span class="comment"># synchronization phase can take</span></span><br><span class="line"><span class="comment"># 集群中的Follower与Leader之间初始连接时能容忍的</span></span><br><span class="line"><span class="comment"># 最多心跳次数（tickTime的数量），用它来限定集群中的</span></span><br><span class="line"><span class="comment"># Zookeeper服务器连接到Leader的时限。</span></span><br><span class="line"><span class="comment"># 人话：也就是说ZK集群中的Follower在刚启动时需要从Leader中</span></span><br><span class="line"><span class="comment"># 同步数据，同步数据耗时最长不能超过10次心跳（默认），</span></span><br><span class="line"><span class="comment"># 如果超过了就抛出异常</span></span><br><span class="line">initLimit=10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># The number of ticks that can pass between </span></span><br><span class="line"><span class="comment"># sending a request and getting an acknowledgement</span></span><br><span class="line"><span class="comment"># 集群中Leader与Follower之间的最大响应时间单位，假如响应</span></span><br><span class="line"><span class="comment"># 时间超过5次心跳，Leader就认为Follwer已经死掉了，就会从</span></span><br><span class="line"><span class="comment"># 服务器列表中删除Follwer。</span></span><br><span class="line">syncLimit=5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just </span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line"><span class="comment"># ZK集群保存数据的目录。</span></span><br><span class="line"><span class="comment"># 建议不要使用 /tmp 目录，因为在Linux中临时文件目录会被定期清理</span></span><br><span class="line">dataDir=/tmp/zookeeper</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line"><span class="comment"># 客户端要连接的端口，也就是ZK服务器端进程的端口号</span></span><br><span class="line">clientPort=2181</span><br><span class="line"></span><br><span class="line"><span class="comment"># the maximum number of client connections.</span></span><br><span class="line"><span class="comment"># increase this if you need to handle more clients</span></span><br><span class="line"><span class="comment"># 最大客户端连接数（需要使用时可以将注释放开）</span></span><br><span class="line"><span class="comment">#maxClientCnxns=60</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># The number of snapshots to retain in dataDir</span></span><br><span class="line"><span class="comment"># 在 dataDir 目录中保留的不同版本的快照数量（需要使用时可以将注释放开）</span></span><br><span class="line"><span class="comment">#autopurge.snapRetainCount=3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Purge task interval in hours</span></span><br><span class="line"><span class="comment"># Set to &quot;0&quot; to disable auto purge feature</span></span><br><span class="line"><span class="comment"># 清除任务间隔时间(小时)</span></span><br><span class="line"><span class="comment"># 设置为 “0” 表示禁用自动清除功能</span></span><br><span class="line"><span class="comment">#autopurge.purgeInterval=1</span></span><br></pre></td></tr></table></figure>





<hr>
<h2 id="分布式安装部署"><a href="#分布式安装部署" class="headerlink" title="分布式安装部署"></a>分布式安装部署</h2><h3 id="ZK-集群的特点"><a href="#ZK-集群的特点" class="headerlink" title="ZK 集群的特点"></a>ZK 集群的特点</h3><ol>
<li> ZK 集群中包含一个 Leader 和多个 Follower。Leader 负责发起投票和决议，更新系统状态；Follower 用于接收客户端请求并向客户端返回结果，在选举 Leader 的过程中参与投票。</li>
<li> ZK 集群实现了主从复制，但没有实现读写分离。</li>
<li> Follower 在掉线后，重新上线后首先需要同步数据。</li>
</ol>
<ol>
<li><p><strong>集群规划</strong></p>
<p> 在 hadoop102、hadoop103 和 hadoop104 三个节点上部署 Zookeeper。</p>
</li>
<li><p><strong>解压安装</strong></p>
<p> （1）解压 Zookeeper 安装包到 <code>/opt/module/</code> 目录下</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 software]$ tar -zxvf apache-zookeeper-3.5.7-bin -C /opt/module/</span><br></pre></td></tr></table></figure>

<p> （2）同步 <code>/opt/module/apache-zookeeper-3.5.7-bin</code> 目录内容到hadoop103、hadoop104</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ xsync /opt/module/apache-zookeeper-3.5.7-bin/</span><br><span class="line"><span class="comment"># xsync是将某个文件分发到集群中所有机器上的脚本</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>配置服务器编号</strong></p>
<p> （1）在 <code>/opt/module/apache-zookeeper-3.5.7-bin/</code> 目录下创建 <code>data</code> 目录，并在该目录下创建一个 <code>myid</code> 文件</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ <span class="built_in">mkdir</span> -p data</span><br><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ <span class="built_in">touch</span> ./data/myid</span><br></pre></td></tr></table></figure>

<p> （2）编辑 <code>myid</code> 文件，在文件中添加当前服务器节点的编号（这个编号只是为了用于标识当前的服务器，可以是任意值，只要不重复即可）</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># myid文件的内容</span></span><br><span class="line">2</span><br></pre></td></tr></table></figure>

<p> （3）拷贝配置好的 zookeeper 到其他机器上，并分别在hadoop103、hadoop104上修改 myid 文件中内容为3、4</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ xsync data/</span><br></pre></td></tr></table></figure></li>
<li><p><strong>配置zoo.cfg文件</strong></p>
<p> （1）重命名 <code>/opt/module/apache-zookeeper-3.5.7-bin/conf</code> 这个目录下的 <code>zoo_sample.cfg</code> 为 <code>zoo.cfg</code></p>
<p> （2）修改 <code>zoo.cfg</code> 文件</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、修改数据存储路径配置</span></span><br><span class="line">dataDir=/opt/module/apache-zookeeper-3.5.7-bin/data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、增加如下配置</span></span><br><span class="line"><span class="comment">#######################cluster##########################</span></span><br><span class="line">server.2=hadoop102:2888:3888</span><br><span class="line">server.3=hadoop103:2888:3888</span><br><span class="line">server.4=hadoop104:2888:3888</span><br></pre></td></tr></table></figure>

<p> （3）同步zoo.cfg配置文件</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ xsync ./conf/zoo.cfg</span><br></pre></td></tr></table></figure>

<p> （4）配置参数解读</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server.A=B:C:D</span><br></pre></td></tr></table></figure>

<ul>
<li>  A 是一个数字，表示服务器编号，即当前服务器是第几号服务器。集群模式下配置一个文件 <code>myid</code>，这个文件在 <code>dataDir</code> 目录下，这个文件里面有一个数据就是 A 的值，Zookeeper 启动时读取此文件，拿到服务器编号与 <code>zoo.cfg</code> 里面的配置信息比较，从而判断当前服务器的相关信息。</li>
<li>  B 是当前服务器的地址。</li>
<li>  C 是当前服务器与集群中的 Leader 进行数据交换时使用的端口。</li>
<li>D 是万一集群中的 Leader 服务器挂了，那么 C 就无法继续使用了，这时需要一个端口来进行重新选举新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</li>
</ul>
</li>
<li><p><strong>集群操作</strong></p>
<p> （1）分别启动三个节点上的 ZooKeeper</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ bin/zkServer.sh start</span><br><span class="line">[lvnengdong@hadoop103 apache-zookeeper-3.5.7-bin]$ bin/zkServer.sh start</span><br><span class="line">[lvnengdong@hadoop104 apache-zookeeper-3.5.7-bin]$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure>

<p> （2）查看ZK线程状态</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ xcall jps</span><br><span class="line">要执行的命令是jps</span><br><span class="line">---------------------hadoop102-----------------</span><br><span class="line">70690 Jps</span><br><span class="line">73576 QuorumPeerMain</span><br><span class="line">---------------------hadoop103-----------------</span><br><span class="line">116029 Jps</span><br><span class="line">115086 QuorumPeerMain</span><br><span class="line">---------------------hadoop104-----------------</span><br><span class="line">110019 Jps</span><br><span class="line">109931 QuorumPeerMain</span><br></pre></td></tr></table></figure>

<p> （3）查看 ZK 集群状态</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop104 apache-zookeeper-3.5.7-bin]$ bin/zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/apache-zookeeper-3.5.7-bin/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: leader	<span class="comment"># hadoop104节点被选举为leader</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[lvnengdong@hadoop103 apache-zookeeper-3.5.7-bin]$ bin/zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/apache-zookeeper-3.5.7-bin/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower	<span class="comment"># hadoop103和102节点都是follower</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ bin/zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/apache-zookeeper-3.5.7-bin/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h1 id="ZK-Shell-客户端操作"><a href="#ZK-Shell-客户端操作" class="headerlink" title="ZK Shell 客户端操作"></a>ZK Shell 客户端操作</h1><ol>
<li><p>启动 ZK 客户端（启动客户端前必须先启动 ZK 服务器端）</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ bin/zkCli.sh -server localhost:2181</span><br><span class="line"></span><br><span class="line"><span class="comment"># 命令分析</span></span><br><span class="line">bin/zkCli.sh -server localhost:2181</span><br><span class="line">[脚本名称] -server [ZK服务器地址:端口号]</span><br><span class="line"><span class="comment"># 如果不加 `-server localhost:2181`，默认走的就是 localhost:2181 这个地址，</span></span><br><span class="line"><span class="comment"># 如果你自己的ZK服务不是这个地址，在启动ZK客户端时一定要显式指定ZK服务器端的通讯地址</span></span><br><span class="line"><span class="comment"># ZK服务器集群中的任一节点都可以处理请求，所以Client将请求发送到集群中的任一节点都是OK的</span></span><br></pre></td></tr></table></figure></li>
<li><p>查看帮助文档</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] <span class="built_in">help</span></span><br><span class="line">ZooKeeper -server host:port cmd args</span><br><span class="line">	addauth scheme auth</span><br><span class="line">	close <span class="comment"># 退出客户端，并关闭session</span></span><br><span class="line">	config [-c] [-w] [-s]</span><br><span class="line">	connect host:port</span><br><span class="line">	create [-s] [-e] [-c] [-t ttl] path [data] [acl]	<span class="comment"># 增</span></span><br><span class="line">	delete [-v version] path	<span class="comment"># 删</span></span><br><span class="line">	deleteall path	<span class="comment"># 删</span></span><br><span class="line">	delquota [-n|-b] path</span><br><span class="line">	get [-s] [-w] path	<span class="comment"># 查</span></span><br><span class="line">	getAcl [-s] path</span><br><span class="line">	<span class="built_in">history</span> </span><br><span class="line">	listquota path	<span class="comment"># 查</span></span><br><span class="line">	<span class="built_in">ls</span> [-s] [-w] [-R] path	<span class="comment"># 查</span></span><br><span class="line">	ls2 path [watch]</span><br><span class="line">	printwatches on|off</span><br><span class="line">	quit <span class="comment"># 退出客户端，但不关闭session</span></span><br><span class="line">	reconfig [-s] [-v version] [[-file path] | [-members serverID=host:port1:port2;port3[,...]*]] | [-add serverId=host:port1:port2;port3[,...]]* [-remove serverId[,...]*]</span><br><span class="line">	redo cmdno	</span><br><span class="line">	removewatches path [-c|-d|-a] [-l]</span><br><span class="line">	rmr path	<span class="comment"># 删</span></span><br><span class="line">	<span class="built_in">set</span> [-s] [-v version] path data	<span class="comment"># 改</span></span><br><span class="line">	setAcl [-s] [-v version] [-R] path acl</span><br><span class="line">	setquota -n|-b val path</span><br><span class="line">	<span class="built_in">stat</span> [-w] path	<span class="comment"># 查</span></span><br><span class="line">	<span class="built_in">sync</span> path</span><br><span class="line"></span><br><span class="line"><span class="comment">#============================================================</span></span><br><span class="line">-w	表示注册监听，不带-w默认不会注册为监听器</span><br></pre></td></tr></table></figure></li>
<li><p>查看当前 znode 中所包含的内容</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] <span class="built_in">ls</span> /</span><br><span class="line">[zookeeper]</span><br></pre></td></tr></table></figure></li>
<li><p>查看当前节点详细数据</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] <span class="built_in">ls</span> -s /</span><br><span class="line">[zookeeper]cZxid = 0x0</span><br><span class="line">ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid = 0x0</span><br><span class="line">cversion = -1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure></li>
<li><p>分别创建2个普通节点</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] create /my_node1 <span class="string">&quot;hello&quot;</span></span><br><span class="line">Created /my_node1</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] create /my_node2 <span class="string">&quot;hi&quot;</span></span><br><span class="line">Created /my_node2</span><br></pre></td></tr></table></figure>

</li>
<li><p>获得节点的值</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] get /my_node1</span><br><span class="line">hello1</span><br></pre></td></tr></table></figure>

</li>
<li><p>创建<strong>临时</strong>节点</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 5] create -e /tmp <span class="string">&quot;This is a temporary node&quot;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>（1）在当前客户端是可以看到的</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 9] <span class="built_in">ls</span> /</span><br><span class="line">[my_node1, my_node2, tmp, zookeeper]</span><br></pre></td></tr></table></figure></li>
<li><p>（2）退出当前客户端后再重启客户端</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 10] quit</span><br><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ bin/zkCli.sh -server localhost:2181</span><br></pre></td></tr></table></figure></li>
<li><p>（3）再次查看发现该临时节点已经被删除了</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] <span class="built_in">ls</span> /</span><br><span class="line">[my_node1, my_node2, zookeeper]</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="常见的-Shell-客户端命令"><a href="#常见的-Shell-客户端命令" class="headerlink" title="常见的 Shell 客户端命令"></a>常见的 Shell 客户端命令</h3><table>
<thead>
<tr>
<th>命令基本语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>help</code></td>
<td>显示所有操作命令</td>
</tr>
<tr>
<td><code>ls path</code></td>
<td>使用 ls 命令来查看当前znode的子节点  -w 监听子节点变化  -s  附加次级信息</td>
</tr>
<tr>
<td><code>create</code></td>
<td>普通创建  -s 含有序列  -e 临时（重启或者超时消失）</td>
</tr>
<tr>
<td><code>get path</code></td>
<td>获得节点的值  -w 监听节点内容变化  -s  附加次级信息</td>
</tr>
<tr>
<td><code>set</code></td>
<td>设置节点的具体值</td>
</tr>
<tr>
<td><code>stat</code></td>
<td>查看节点状态</td>
</tr>
<tr>
<td><code>delete</code></td>
<td>删除节点</td>
</tr>
<tr>
<td><code>deleteall</code></td>
<td>递归删除节点</td>
</tr>
</tbody></table>
<h3 id="节点的状态和节点类型"><a href="#节点的状态和节点类型" class="headerlink" title="节点的状态和节点类型"></a>节点的状态和节点类型</h3><p>查看当前节点详细数据</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] <span class="built_in">ls</span> -s /my_node1</span><br><span class="line">[]cZxid = 0x6	</span><br><span class="line">ctime = Tue Nov 30 21:30:35 CST 2021</span><br><span class="line">mZxid = 0x6</span><br><span class="line">mtime = Tue Nov 30 21:30:35 CST 2021</span><br><span class="line">pZxid = 0x6</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 6</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li>  <code>c：create</code></li>
<li>  <code>Z：ZooKeeper</code></li>
<li>  <code>x：事务</code></li>
<li>  <code>m：modify，修改</code></li>
<li>  <code>p：parent</code></li>
</ul>
</blockquote>
<ul>
<li>  <code>cZxid</code>：创建该 ZooKeeper 节点时事务的 id（事务id是一个十六进制的数）。</li>
<li>  <code>ctime</code>：该 ZK 节点的创建时间</li>
<li>  <code>mZxid</code>：该 ZK 节点在修改时的事务 id</li>
<li>  <code>mtime</code>：该 ZK 节点的修改时间</li>
<li>  <code>pZxid</code>：当前节点中最新发生的创建子节点事务的 id。</li>
<li>  <code>cversion</code>：</li>
<li>  <code>dataVersion</code>：数据版本，当前节点中的数据发生变化后，该版本号就 +1。一般用于乐观锁。</li>
<li>  <code>aclVersion</code>：权限控制相关，每当节点的权限发生变化后，该版本号就 +1。</li>
<li>  <code>ephemeralOwner</code> ：该节点是否是一个临时节点。<code>0x0</code>表示当前节点是一个永久节点，非0表示当前节点是一个临时节点</li>
<li>  <code>numChildren</code>：当前节点的子节点个数</li>
</ul>
<h1 id="ZK-内部原理"><a href="#ZK-内部原理" class="headerlink" title="ZK 内部原理"></a>ZK 内部原理</h1><h2 id="监听器原理"><a href="#监听器原理" class="headerlink" title="监听器原理"></a>监听器原理</h2><p><img src="/2021/11/29/ZooKeeper/image-20211201102045752.png" alt="image-20211201102045752"></p>
<ol>
<li> 首先在 Main() 线程中创建 ZK 客户端；</li>
<li> ZK 客户端创建后会维护两个线程 <strong>Listener</strong> 和 <strong>connect</strong>，一个负责与 ZK 服务端进行网络连接通信（connect），一个负责监听（Listener）。</li>
<li> 客户端通过 connect 线程向 ZK 集群发送注册监听请求，比如 <code>getChildren(&quot;/&quot;, true)</code>，这个请求的含义是获取根目录 <code>/</code> 下的所有子节点，第二个 boolean 类型的参数表示是否注册成为监听器，true 表示注册成为监听器，这时 ZK 集群就会把客户端的 <code>ip、port、监听的path、以及客户端的名字等信息</code> 全部记录起来，当客户端监听的 <code>path</code> 发生变化时，就会通知客户端。</li>
<li> 对于服务器端而言，ZK 集群会将新注册的监听事件添加到监听器列表中，当监听列表中的监听事件被触发时（也就是监听路径下的数据或目录结构发生了变化），就会将变化情况发送给客户端的 Listener 线程。</li>
<li> Listener 线程内部再调用 <code>process()</code> 方法进行相应的处理。</li>
</ol>
<p><strong>常见的监听：</strong></p>
<ol>
<li><p>监听节点数据的变化</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get path[watch]</span><br></pre></td></tr></table></figure>

</li>
<li><p>监听子节点增减的变化</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> path[watch]</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>注意：</strong></p>
<ul>
<li>  观察者只是单次有效的。</li>
</ul>
<hr>
<h2 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h2><p><strong>半数机制：</strong>只有当集群中半数<strong>以上</strong>的机器存活时，集群才可用。</p>
<p>ZooKeeper 虽然没有在配置文件中指定 Master 和 Slaver，但是 ZK 集群在运行时只能有一个节点为 Leader，其余节点均为 Follower，这个 Leader 是 ZK 运行时通过内部选举机制产生的。</p>
<p>下面以一个简单的例子来说明选举 Leader 的过程：</p>
<p>假设有五台服务器组成的 Zookeeper 集群，它们的 id 为从1~5，同时它们都是新建的，没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器按照 id 从小到大<strong>依次启动</strong>，来看看会发生什么。</p>
<p><img src="/2021/11/29/ZooKeeper/image-20211201195906285.png" alt="image-20211201195906285"></p>
<ol>
<li> <code>Server1</code> 率先启动，发起一次选举，并且投自己一票（默认每个服务器投票时都会优先选自己）。此时集群中只有一台机器，<code>Server1</code> 得到一票，但是总票数小于3票，不够半数以上，选举无法完成，服务器保持 Looking 状态。</li>
<li> <code>Server2</code> 启动，再发起一次选举。<code>Server1</code> 和 <code>Server2</code> 首先分别投自己一票并交换选举信息。此时<code>Server1</code> 发现<code>Server2</code> 的ID比自己目前投票推举的服务器id大，更改选票为推举<code>Server2</code> 。此时<code>Server1</code> 得 0 票，<code>Server2</code> 得 2 票，没有半数以上的结果。选举无法完成。服务器1、2状态保持 Looking。</li>
<li> <code>Server3</code> 启动，发起一次选举，此时<code>Server1</code> 和 <code>Server2</code> 在交换完选票信息后都会更改选票为 <code>Server3</code> 。此次投票结束后：<code>Server1</code> 为0票，<code>Server2</code> 为0票，<code>Server3</code> 为3票，此时 <code>Server3</code>  的票数已经超过半数，<code>Server3</code> 当选为 Leader，服务器1、2更改状态为 Following，<code>Server3</code> 更改状态为 Leading。</li>
<li> <code>Server4</code> 启动，发起一次选举，此时服务器1、2、3已经不是 Looking 状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票，此时服务器4服从多数，更改选票信息为服务器3，并更改状态为Following。</li>
<li> 服务器5启动，同服务器4一样当小弟。</li>
</ol>
<hr>
<h2 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h2><p><img src="/2021/11/29/ZooKeeper/image-20211202102918703.png" alt="image-20211202102918703"></p>
<p>ZooKeeper 并没有实现读写分离。</p>
<ol>
<li> Client 向 ZK 集群中的 <code>Server1</code> 发送写请求，如果 <code>Server1</code> 不是 Leader，那么 <code>Server1</code> 首先会把接收到的请求进一步转发给 Leader。</li>
<li> Leader 会将写请求广播到集群中的每个节点，各个节点都会将这个写请求加入到待写队列，并向 Leader 发送写成功的 ACK。</li>
<li> 当 Leader 收到半数以上节点的成功信息，就说明该写操作可以执行成功。Leader 会继续向各个节点发送提交信息，各个 Server 收到该消息后会落实待写队列中的写请求，此时写成功。如果 Leader 没有收到半数以上的成功信息，各个节点中待写队列中对应的数据在一定时间后会被清除。</li>
<li> <code>Server1</code> 会进一步通知 Client 数据写成功了，这时候就认为整个写操作成功。</li>
</ol>
<hr>
<h1 id="ZK-Java客户端操作"><a href="#ZK-Java客户端操作" class="headerlink" title="ZK Java客户端操作"></a>ZK Java客户端操作</h1><h2 id="创建Maven项目，导入依赖"><a href="#创建Maven项目，导入依赖" class="headerlink" title="创建Maven项目，导入依赖"></a>创建Maven项目，导入依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.5.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h2 id="API-测试"><a href="#API-测试" class="headerlink" title="API 测试"></a>API 测试</h2><h3 id="创建-ZooKeeper客户端"><a href="#创建-ZooKeeper客户端" class="headerlink" title="创建 ZooKeeper客户端"></a>创建 ZooKeeper客户端</h3><p><strong>源码：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 构造函数</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> connectString ZK集群的地址</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> sessionTimeout client与ZK集群间的session的超时时间 </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> watcher 观察者实例</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="title function_">ZooKeeper</span><span class="params">(String connectString, <span class="type">int</span> sessionTimeout, Watcher watcher)</span></span><br><span class="line">    <span class="keyword">throws</span> IOException</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">this</span>(connectString, sessionTimeout, watcher, <span class="literal">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>测试</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ZKTest</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 格式：`IP地址:端口号`，如果ZK是一个集群，多个地址之间用逗号隔开</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">connectString</span> <span class="operator">=</span> <span class="string">&quot;hadoop101:2181, hadoop102:2181,&quot;</span>;</span><br><span class="line">    <span class="comment">// session的超时时间（单位：ms）</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">sessionTimeout</span> <span class="operator">=</span> <span class="number">10000</span>;</span><br><span class="line">    <span class="comment">// 观察者实例，一旦watcher观察的path发生了变更，服务端就会通知客户端，客户端收到通知后就会自动调用process()方法</span></span><br><span class="line">    <span class="type">Watcher</span> <span class="variable">watcher</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="type">ZooKeeper</span> <span class="variable">zkClient</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 创建ZK客户端对象</span></span><br><span class="line">    <span class="meta">@BeforeEach</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// 创建一个ZK客户端对象，创建 zkClient 需要依赖 connectString, sessionTimeout, watcher</span></span><br><span class="line">        zkClient = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(connectString, sessionTimeout, watcher);</span><br><span class="line">        System.out.println(zkClient);</span><br><span class="line">        Thread.sleep(<span class="number">3000</span>);	<span class="comment">// 主动让ZKClient睡眠一段时间，保证TCP连接可以成功建立</span></span><br><span class="line">        System.out.println(<span class="string">&quot;init&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 使用完毕后释放 zkClient</span></span><br><span class="line">    <span class="meta">@AfterEach</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="keyword">if</span> (zkClient != <span class="literal">null</span>)&#123;</span><br><span class="line">            zkClient.close();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">&quot;end&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="可能会出现的异常"><a href="#可能会出现的异常" class="headerlink" title="可能会出现的异常"></a>可能会出现的异常</h3><p>在创建了一个客户端对象后，就可以通过这个客户端对象去调用对应的 API 查看 ZK 中的一些信息了，但是在测试环境中，我们一般需要在创建完客户端之后等待几秒再去调用其它的 API。因为 <code>new ZooKeeper()</code> 语句只是创建出了一个 ZKClient 实例，只是建立了客户端与服务端之间的会话，但是此时 TCP 连接可能还未建立完成，如果这时向 ZooKeeper 集群发出操作命令的话就可能出现连接丢失异常，虽然这种概率相对较小。</p>
<p>所以一般情况下创建完 ZKClient 实例后，我们需要等待几秒钟再去执行其它操作就不会出问题了。</p>
<h3 id="创建子节点"><a href="#创建子节点" class="headerlink" title="创建子节点"></a>创建子节点</h3><p><strong>源码：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> path 节点的位置</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> data[] 节点中的数据，以 byte[] 数组存储</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> acl 权限列表</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> createMode 节点的类型</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> String <span class="title function_">create</span><span class="params">(<span class="keyword">final</span> String path, <span class="type">byte</span> data[], List&lt;ACL&gt; acl,CreateMode createMode)</span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>测试：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException, IOException &#123;</span><br><span class="line">   </span><br><span class="line">    <span class="type">String</span> <span class="variable">path</span> <span class="operator">=</span> <span class="string">&quot;/idea&quot;</span>;</span><br><span class="line">    <span class="type">byte</span>[] data = <span class="string">&quot;HelloZooKeeper&quot;</span>.getBytes();</span><br><span class="line">    List&lt;ACL&gt; acl = ZooDefs.Ids.OPEN_ACL_UNSAFE;    <span class="comment">// 这是一个枚举类，从中选择了最高权限`OPEN_ACL_UNSAFE`赋值给acl</span></span><br><span class="line">    <span class="type">CreateMode</span> <span class="variable">createMode</span> <span class="operator">=</span> CreateMode.PERSISTENT;  <span class="comment">// 也是一个枚举类，这里选择了节点类型为永久节点</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> zkClient.create(path, data, acl, createMode);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="获取子节点并监听节点变化"><a href="#获取子节点并监听节点变化" class="headerlink" title="获取子节点并监听节点变化"></a>获取子节点并监听节点变化</h3><p><strong>源码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment">	getChildren方法用于查看当前ZNode下的所有节点，</span></span><br><span class="line"><span class="comment">	调用该方法时可以传递一个监听器 watcher 对象，</span></span><br><span class="line"><span class="comment">	如果传递了 watcher 对象，在监听路径下的数据发生了</span></span><br><span class="line"><span class="comment">	变化时，服务器（ZK集群）就会回调watcher对象的process()方法。</span></span><br><span class="line"><span class="comment">	这个监听器对象可以使用创建ZK客户端时创建的对象，也可以重新定义</span></span><br><span class="line"><span class="comment">	一个新的监听器对象。</span></span><br><span class="line"><span class="comment">	</span></span><br><span class="line"><span class="comment">	该方法存在多个重载方法</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment">	如果第二个参数传递一个 Boolean 类型的值，如果为true监听器对象就会</span></span><br><span class="line"><span class="comment">	自动注册成为ZKClient创建时指定的监听器，如果为 false 监听器对象就</span></span><br><span class="line"><span class="comment">	为 null，表示不需要监听路径path</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> List&lt;String&gt; <span class="title function_">getChildren</span><span class="params">(String path, <span class="type">boolean</span> watch)</span></span><br><span class="line">    <span class="keyword">throws</span> KeeperException, InterruptedException &#123;</span><br><span class="line">    <span class="keyword">return</span> getChildren(path, watch ? watchManager.defaultWatcher : <span class="literal">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果想要使用自定义的监听器，也可以直接传入一个监听器对象</span></span><br><span class="line"><span class="keyword">public</span> List&lt;String&gt; <span class="title function_">getChildren</span><span class="params">(<span class="keyword">final</span> String path, Watcher watcher)</span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>测试：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ls</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">ls</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException, IOException &#123;</span><br><span class="line">   </span><br><span class="line">    System.out.println(zkClient);</span><br><span class="line">    List&lt;String&gt; children = zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="literal">null</span>);</span><br><span class="line">    System.out.println(children);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="判断-ZNode-是否存在"><a href="#判断-ZNode-是否存在" class="headerlink" title="判断 ZNode 是否存在"></a>判断 ZNode 是否存在</h3><p><strong>源码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用 ZKClient 的监听器</span></span><br><span class="line"><span class="keyword">public</span> Stat <span class="title function_">exists</span><span class="params">(String path, <span class="type">boolean</span> watch)</span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用自定义的监听器，返回结果直接回调给观察者</span></span><br><span class="line"><span class="keyword">public</span> Stat <span class="title function_">exists</span><span class="params">(<span class="keyword">final</span> String path, Watcher watcher)</span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>测试</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">exist</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">ZooKeeper</span> <span class="variable">zkClient</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(connectString, sessionTimeout, watcher);</span><br><span class="line">    <span class="type">Stat</span> <span class="variable">stat</span> <span class="operator">=</span> zkClient.exists(<span class="string">&quot;/idea&quot;</span>, <span class="literal">false</span>);</span><br><span class="line">    System.out.println(stat == <span class="literal">null</span> ? <span class="string">&quot;不存在&quot;</span> : <span class="string">&quot;存在&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="使用-JavaAPI-设置观察者"><a href="#使用-JavaAPI-设置观察者" class="headerlink" title="使用 JavaAPI 设置观察者"></a>使用 JavaAPI 设置观察者</h2><p>在 ZK 提供的客户端 API 中，有一些可以设置观察者，有一些不能设置观察者。我们以部分方法为例展示观察者模式的使用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testObserver</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">// 由connect线程调用</span></span><br><span class="line">    zkClient = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(connectString, sessionTimeout, watcher);</span><br><span class="line"></span><br><span class="line">    List&lt;String&gt; list = zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">        <span class="comment">// 由Listener线程调用</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line">            <span class="comment">// 当 `/` 路径下的内容发生变化时，就会触发该方法</span></span><br><span class="line">            System.out.println(<span class="string">&quot;路径==&quot;</span> + event.getPath() + <span class="string">&quot;===发生了变化&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 为了保证客户端能收到回调信息，ZKClient进程要一直运行，不能关闭</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line">        Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;我还活着&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>此时，当 ZK 中根目录 <code>/</code> 下的内容发生变化时，就会触发回调，客户端的回调函数 <code>process()</code> 方法就会被执行。</p>
<p><strong>注意：</strong>一般情况下，我们不会使用 ZKClient 中默认的监听器，因为让多种不同的操作执行同一个监听器的逻辑是不合理的，一般情况下我们会针对每种情况都设置自己的监听器。</p>
<hr>
<h2 id="持续监听"><a href="#持续监听" class="headerlink" title="持续监听"></a>持续监听</h2><p>在 ZK 中，观察者默认模式是<strong>单次有效</strong>的，也就是说，当监听路径下的内容第一次发生改变时，会触发监听事件，但是当监听事件触发过之后，该路径下的内容再次发生变化时就不会触发监听事件了。如果想要每次内容发生变化时都触发监听事件，就需要设置持续监听。</p>
<p><strong>持续监听原理：</strong></p>
<ol>
<li> ZKClient 在向服务器发送请求时，可以设置一个观察者 watcher；</li>
<li> 当 ZKClient 监听的路径发生变化时，服务器会调用 ZKClient 的回调方法 <code>process()</code></li>
<li> 如果我们想要实现持续监听功能的话，只需要在 <code>process()</code> 中重新向 ZK 服务器发送监听请求并设置一个新的监听器就可以了。</li>
</ol>
<p><strong>错误写法：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 持续监听【错误写法】</span></span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testObserving</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">       <span class="comment">// 由connect线程调用</span></span><br><span class="line">       List&lt;String&gt; list = zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">           <span class="comment">// 由Listener线程调用</span></span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line">               <span class="comment">// 当 `/` 路径下的内容发生变化时，就会触发该方法</span></span><br><span class="line">               System.out.println(<span class="string">&quot;路径==&quot;</span> + event.getPath() + <span class="string">&quot;===发生了变化&quot;</span>);</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   <span class="comment">// 递归调用，重新设置观察者</span></span><br><span class="line">                   testObserving();</span><br><span class="line">               &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       <span class="comment">// 为了保证客户端能收到回调信息，ZKClient进程要一直运行，不能关闭</span></span><br><span class="line">       <span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line">           Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">           System.out.println(<span class="string">&quot;我还活着&quot;</span>);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p>我们来分析一下上面这段代码：</p>
<ul>
<li>  当 <code>/</code> 路径下的内容发生变化时，就会触发回调函数 <code>process()</code> 方法执行，在该方法中会递归地调用 <code>testObserving()</code> 方法，进行重新向 ZK 服务器发送一个带有新的观察者的请求。</li>
<li>  看起来一切都很美好，持续监听的功能也能够实现。但是这段代码在实际运行时并不能得到想要的结果。</li>
</ul>
<p>原因如下：</p>
<ol>
<li> 第一次调用 <code>testObserving()</code> 方法的线程是 connect 线程，在本例中也就是 main 线程，为了保证客户端能够收到回调信息，main线程会一直保持运行。</li>
<li> 但是 <code>process()</code> 方法是由 Listener 线程执行的，也就是说第二次调用 <code>testObserving()</code> 方法是由 Listener 线程执行的，那么 Listener 线程在执行到第 22 行的时候，就会陷入死循环。</li>
<li> ZK 服务器端数据发生变化时需要通知 Listener 线程来执行 <code>process()</code> 方法，而此时的 Listener 线程陷入了死循环中，也就是说无法收到服务器的回调，自然也就无法继续执行 <code>process()</code> 方法了，所以上面的代码并不能实现持续监听的效果。</li>
</ol>
<p><strong>正确写法：</strong></p>
<p>想要解决上面的问题，我们只要让 Listen 线程不陷入死循环就可以解决了。</p>
<p>解决方法很简单，只需要把包含 Listener 线程的方法单独抽取到一个独立方法中就可以了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 持续监听【正确写法】</span></span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testObserving</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">       observing();</span><br><span class="line">       </span><br><span class="line">       <span class="comment">// 为了保证客户端能收到回调信息，ZKClient进程要一直运行，不能关闭</span></span><br><span class="line">       <span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line">           Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">           System.out.println(<span class="string">&quot;我还活着&quot;</span>);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">observing</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException &#123;</span><br><span class="line">       <span class="comment">// 由connect线程调用</span></span><br><span class="line">       zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">           <span class="comment">// 由Listener线程调用</span></span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line">               <span class="comment">// 当 `/` 路径下的内容发生变化时，就会触发该方法</span></span><br><span class="line">               System.out.println(<span class="string">&quot;路径==&quot;</span> + event.getPath() + <span class="string">&quot;===发生了变化&quot;</span>);</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   <span class="comment">// 递归调用，重新设置观察者</span></span><br><span class="line">                   testObserving();</span><br><span class="line">               &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>





<h2 id="监听服务器节点动态上下线案例"><a href="#监听服务器节点动态上下线案例" class="headerlink" title="监听服务器节点动态上下线案例"></a>监听服务器节点动态上下线案例</h2><p><strong>需求</strong>：某分布式系统中，有多台节点，可以动态上下线，要求任意一台客户端都能实时感知到主节点服务器的上下线。</p>
<p><strong>需求分析</strong></p>
<ol>
<li> 想要客户端能动态监控服务器节点的上线、下线，那么要求服务器节点一定不能是永久节点，对于永久节点来说，一旦创建了就会被持久化，即使下线了在 ZK 集群中也不会失去这个节点的信息。所以需要把服务器节点设置称为临时节点，这样当服务器上/下线的时候，ZK 集群就会动态的创建/删除节点。</li>
</ol>
<p><img src="/2021/11/29/ZooKeeper/image-20211213115812008.png" alt="image-20211213115812008"></p>
<p><strong>具体实现</strong></p>
<ol>
<li><p>先在集群上创建 <code>servers</code> 节点</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 5] create /servers &quot;servers&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>服务器端向 Zookeeper 注册的代码</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.xsyu.zoo_keeper_demo.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> lnd</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2021/12/13 12:06</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Version</span> 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 服务器节点</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 需求：</span></span><br><span class="line"><span class="comment"> *      每次启动后，在执行自己的核心业务之前，需要先向ZK集群注册一个临时节点，</span></span><br><span class="line"><span class="comment"> *      并向临时节点中保存一些关键信息</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Server</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 格式：`IP地址:端口号`，如果ZK是一个集群，多个地址之间用逗号隔开</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">connectString</span> <span class="operator">=</span> <span class="string">&quot;hadoop102:2181, hadoop103:2181,&quot;</span>;</span><br><span class="line">    <span class="comment">// session的超时时间（单位：ms）</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">sessionTimeout</span> <span class="operator">=</span> <span class="number">10000</span>;</span><br><span class="line">    <span class="comment">// 监听器线程对象，一旦watcher观察的path发生了变更，服务端就会通知客户端，客户端收到通知后就会自动调用process()方法</span></span><br><span class="line">    <span class="type">Watcher</span> <span class="variable">watcher</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="type">String</span> <span class="variable">basePath</span> <span class="operator">=</span> <span class="string">&quot;/servers&quot;</span>;</span><br><span class="line">    <span class="type">ZooKeeper</span> <span class="variable">zkClient</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化客户端对象</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建一个ZK客户端对象</span></span><br><span class="line">        zkClient = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(connectString, sessionTimeout, watcher);</span><br><span class="line">        System.out.println(zkClient);</span><br><span class="line">        System.out.println(<span class="string">&quot;init&quot;</span>);</span><br><span class="line">        Thread.sleep(<span class="number">3000</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;创建ZKClient成功&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 使用zkClient在ZK集群上为Server注册临时节点，</span></span><br><span class="line"><span class="comment">     * 并向临时节点中保存一些关键信息</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">register</span><span class="params">(String info)</span> <span class="keyword">throws</span> KeeperException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 创建带有序号的临时节点</span></span><br><span class="line">        <span class="comment">// 创建出来的节点类似于： /servers/server1;    /servers/server2 这种</span></span><br><span class="line">        zkClient.create(basePath+<span class="string">&quot;/server&quot;</span>, info.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line">        System.out.println(<span class="string">&quot;注册节点成功&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Server节点其它的业务功能</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doOthers</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 让Server持续运行</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;working....&quot;</span>);</span><br><span class="line">            Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 主方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Server</span> <span class="variable">server</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Server</span>();</span><br><span class="line">        <span class="comment">// 创建ZK客户端对象</span></span><br><span class="line">        server.init();</span><br><span class="line">        <span class="comment">// 将Server节点注册到ZK中</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">info</span> <span class="operator">=</span> <span class="string">&quot;我是Sever1，我的库存服务节点&quot;</span>;</span><br><span class="line">        server.register(info);</span><br><span class="line">        <span class="comment">// Server执行其它的业务功能</span></span><br><span class="line">        server.doOthers();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
<li><p>客户端代向 Zookeeper 监听服务器上下线情况的代码</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.xsyu.zoo_keeper_demo.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.KeeperException;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> lnd</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2021/12/13 15:42</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Version</span> 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 从ZK集群持续监听Server节点的变化，一旦有变化，重新获取Server进程的信息</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Client</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 格式：`IP地址:端口号`，如果ZK是一个集群，多个地址之间用逗号隔开</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">connectString</span> <span class="operator">=</span> <span class="string">&quot;hadoop102:2181, hadoop103:2181,&quot;</span>;</span><br><span class="line">    <span class="comment">// session的超时时间（单位：ms）</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">sessionTimeout</span> <span class="operator">=</span> <span class="number">10000</span>;</span><br><span class="line">    <span class="comment">// 监听器线程对象，一旦watcher观察的path发生了变更，服务端就会通知客户端，客户端收到通知后就会自动调用process()方法</span></span><br><span class="line">    <span class="type">Watcher</span> <span class="variable">watcher</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="type">String</span> <span class="variable">basePath</span> <span class="operator">=</span> <span class="string">&quot;/servers&quot;</span>;</span><br><span class="line">    <span class="type">ZooKeeper</span> <span class="variable">zkClient</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化客户端对象</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建一个ZK客户端对象</span></span><br><span class="line">        zkClient = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(connectString, sessionTimeout, watcher);</span><br><span class="line">        System.out.println(zkClient);</span><br><span class="line">        System.out.println(<span class="string">&quot;init&quot;</span>);</span><br><span class="line">        Thread.sleep(<span class="number">3000</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;创建ZKClient成功&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 持续监听当前启动的Server进程有哪些，获取到Server进程的信息</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;String&gt; <span class="title function_">getInfo</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        ArrayList&lt;String&gt; info = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; children = zkClient.getChildren(basePath, <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line">                System.out.println(event.getPath() + <span class="string">&quot;路径下发生了以下事件&quot;</span> + event.getType());</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">// 递归，持续监听</span></span><br><span class="line">                    getInfo();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取每个节点（server）中保存的信息</span></span><br><span class="line">        <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">            <span class="type">byte</span>[] data = zkClient.getData(basePath + <span class="string">&quot;/&quot;</span> + child, <span class="literal">null</span>, <span class="literal">null</span>);</span><br><span class="line">            info.add(<span class="keyword">new</span> <span class="title class_">String</span>(data));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;最新读到的信息是:&quot;</span> + info);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> info;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 其它业务功能</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doOthers</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 让Client持续运行</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;working....&quot;</span>);</span><br><span class="line">            Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Client</span> <span class="variable">client</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Client</span>();</span><br><span class="line">        <span class="comment">// 同样首先也要先创建ZKClient，并向ZK集群注册监听事件</span></span><br><span class="line">        client.init();</span><br><span class="line">        <span class="comment">// 获取数据</span></span><br><span class="line">        client.getInfo();</span><br><span class="line">        <span class="comment">// client 的其它工作（保持client不会关闭）</span></span><br><span class="line">        client.doOthers();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/28/Kafka/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/11/28/Kafka/" class="post-title-link" itemprop="url">Kafka</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-11-28 20:33:27" itemprop="dateCreated datePublished" datetime="2021-11-28T20:33:27+08:00">2021-11-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-11-29 11:21:19" itemprop="dateModified" datetime="2021-11-29T11:21:19+08:00">2021-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/" itemprop="url" rel="index"><span itemprop="name">消息队列</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h1><h2 id="消息队列的应用场景"><a href="#消息队列的应用场景" class="headerlink" title="消息队列的应用场景"></a>消息队列的应用场景</h2><ul>
<li>  异步处理</li>
</ul>
<h2 id="消息队列的两种模式"><a href="#消息队列的两种模式" class="headerlink" title="消息队列的两种模式"></a>消息队列的两种模式</h2><h3 id="点对点模式"><a href="#点对点模式" class="headerlink" title="点对点模式"></a>点对点模式</h3><p><strong>特点：</strong></p>
<ol>
<li> 一对一，</li>
<li> 消费者主动从消费队列中拉取数据，</li>
<li> MQ 会自动把消费者已经收到的消息清除。【因为每条 message 只存在一个消费者】</li>
</ol>
<p><strong>流程：</strong></p>
<img src="/2021/11/28/Kafka/image-20211128204651428.png" alt="image-20211128204651428" style="zoom:150%;">

<ol>
<li> 生产者 Producer 生产消息发送到 MessageQueue 中，然后消费者 Consumer 从 MQ 中取出并且消费消息。</li>
<li> 消息被消费以后，Consumer 会向 MQ 发送确认，当 MQ 收到确认后，就会自动从 queue 中删除该条消息。所以消息消费者不可能消费到已经被消费的消息。</li>
</ol>
<p><strong>说明：</strong></p>
<p>在点对点模式下，MQ 支持存在多个消费者，但是对某一个消息而言，只会存在一个消费者可以消费该条消息。</p>
<hr>
<h3 id="发布-订阅模式"><a href="#发布-订阅模式" class="headerlink" title="发布/订阅模式"></a>发布/订阅模式</h3><p><strong>特点：</strong></p>
<ul>
<li>  一对多，</li>
<li>  消费者消费数据之后不会清除消息【因为每条 message 可能存在多个消费者，所以该条 message 的删除时机是不确定的，一般是在消息队列满了之后由开发人员自行设置删除策略的】</li>
</ul>
<p><strong>流程：</strong></p>
<p>消息生产者 Producer 将消息发布到 Topic 中，同时有多个消费者 Consumer（订阅）消费该消息。和点对点方式不同，发布到 Topic 的消息会被所有订阅者消费。</p>
<p> <img src="/2021/11/28/Kafka/image-20211128205611922.png" alt="image-20211128205611922"></p>
<hr>
<h1 id="Kafka-简介"><a href="#Kafka-简介" class="headerlink" title="Kafka 简介"></a>Kafka 简介</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>Kafka 是一个分布式的基于<strong>发布/订阅模式</strong>的消息队列（Message Queue），主要应用于大数据实时处理领域。</p>
<p>Kafka 是一个分布式的数据流式传输平台。在流式计算中，Kafka 一般用来缓存数据，Spark 通过消费 Kafka 中的数据来进行计算。</p>
<ol>
<li> Apache Kafka 是由 Scala 语言编写的，Scala 语言编译后的文件可以运行在 JVM 虚拟机上。</li>
<li> Kafka 的目标是为处理<strong>实时数据</strong>提供一个统一、高通量、低等待的平台。</li>
<li> Kafka 是一个分布式消息队列。</li>
<li> Kafka 保存消息时会根据消息的 Topic 进行归类。</li>
<li> Kafka 集群由多个 kafka 实例组成，每个实例（server）称为 <strong>broker</strong>。</li>
<li> 无论是 kafka 集群，还是 consumer 都依赖于 <strong>zookeeper</strong> 集群保存一些meta信息，来保证系统可用性。</li>
</ol>
<hr>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>作为一个数据流式传输平台，kafka有以下三大特点：</p>
<ol>
<li> 类似于消息队列和商业的消息系统，kafka 提供对流式数据的发布和订阅</li>
<li> kafka 提供一种<strong>持久</strong>的<strong>容错</strong>的方式存储流式数据【持久：支持持久化。容错：采用多副本机制支持容错】</li>
<li> kafka 拥有良好的性能，可以近实时地处理流式数据</li>
</ol>
<p>基于以上三种特点，kafka 在以下两种应用之间流行：</p>
<ol>
<li> 需要在多个应用和系统间建立高可靠的实时数据通道</li>
<li> 一些需要实时传输数据及及时计算的应用</li>
</ol>
<p>此外，kafka 还有以下特点：</p>
<ul>
<li>  Kafka 集群将数据按照类别记录存储，这种类别在 kafka 中称为主题（Topic）</li>
<li>  每条记录由一个键，一个值和一个时间戳组成</li>
</ul>
<hr>
<h1 id="Kafka核心概念"><a href="#Kafka核心概念" class="headerlink" title="Kafka核心概念"></a>Kafka核心概念</h1><h2 id="Broker"><a href="#Broker" class="headerlink" title="Broker"></a>Broker</h2><blockquote>
<p>  <code>broker，代理</code></p>
</blockquote>
<p>Kafka 集群中的每台服务器都是 broker，一个集群由多个 broker 组成。</p>
<hr>
<h2 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h2><p>Topic 就是数据的主题，kafka 建议将不同的数据存放在不同的 Topic 中。</p>
<p>Topic 是对于数据的逻辑分类，即一个 Topic 的数据可以分布式存储在不同 broker 上。</p>
<p>一个 Topic 可以拥有一个或者多个消费者来订阅它的数据。</p>
<p>Topic 可以类比为数据库中的库。</p>
<hr>
<h2 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h2><p>Partition 是对数据的物理分区，每个 Topic 中的数据可以存储在多个分区上。通过分区的设计，Topic 可以不断进行扩展，即一个 Topic 的多个分区分布式存储在多个 broker 上。</p>
<p>此外通过分区还可以让一个 Topic 中的数据被多个 consumer 并行的进行消费。</p>
<p>Kafka 只能保证一个 partition 中的消息顺序地发给 consumer，不保证一个 topic 的整体（多个partition间）的顺序。</p>
<p>分区可以类比为数据库中的表。</p>
<hr>
<h2 id="Offset"><a href="#Offset" class="headerlink" title="Offset"></a>Offset</h2><p>Partition 是 Kafka 中数据的存储的物理单位，每个分区中的数据会按照写入的时间顺序按顺序保存，这个顺序是通过一个称之为 <code>offset</code> 的 id 来唯一标识的，因此也可以认为offset是有序且不可变的。</p>
<p>数据会按照时间顺序被不断第追加到分区中的一个结构化的 commit log中。每个分区中存储的记录都是有序的，且顺序不可变。</p>
<p>在每个消费者端，会唯一保存的元数据是 <code>offset（偏移量）</code>，即消费在 log 中的位置（消费者消费到了第几条数据），偏移量由消费者控制。通常在读取记录后，消费者会以线性的方式增加偏移量，但是实际上，由于这个位置由消费者控制，所以消费者可以采用任何顺序来消费记录。例如，一个消费者可以重置到一个旧的偏移量，从而重新处理过去的数据；也可以跳过最近的记录，从”现在”开始消费。</p>
<p>这些细节说明 Kafka 消费者是非常廉价的——消费者的增加和减少，对集群或者其他消费者没有多大的影响。比如，你可以使用命令行工具，对一些 topic 内容执行 tail 操作，并不会影响已存在的消费者消费数据。</p>
<blockquote>
<p>  图1 Topic拓扑结构</p>
</blockquote>
<p> <img src="/2021/11/28/Kafka/image-20211128213939763.png" alt="image-20211128213939763"></p>
<blockquote>
<p>  图2 数据流</p>
</blockquote>
<p><img src="/2021/11/28/Kafka/image-20211128213950934.png" alt="image-20211128213950934"></p>
<hr>
<h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><p>Kafka 集群会保留所有发布的记录，无论它们是否已被消费。</p>
<p>如果想要删除发布记录，可以通过配置保留期限参数来控制。举个例子，如果保留策略设置为2天，一条记录发布后两天内，可以随时被消费，两天过后这条记录会被清除并释放磁盘空间。</p>
<p>Kafka 的性能和数据大小无关，所以长时间存储数据没有什么问题。</p>
<hr>
<h2 id="副本机制"><a href="#副本机制" class="headerlink" title="副本机制"></a>副本机制</h2><p>Kafka 通过副本机制来保证高可用。</p>
<p>日志的分区 partition （分布）在Kafka集群的服务器上。每个服务器在处理数据和请求时，共享这些分区。每一个分区都会在已配置的服务器上进行备份，确保容错性。</p>
<p>每个分区都有一台 server 作为 “leader”，零台或者多台 server 作为 follwers。leader server 处理一切对 partition （分区）的读写请求，而 follwers 只需被动的同步 leader 上的数据。当 leader 宕机了，followers 中的一台服务器会自动成为新的 leader。通过这种机制，既可以保证数据有多个副本，也实现了一个高可用的机制！</p>
<p>基于安全考虑，每个分区的Leader和follower一般会错在在不同的broker!</p>
<h2 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a>Producer</h2><p>消息生产者，就是向 <code>kafka broker</code> 发消息的客户端，生产者负责将消息分配到 topic 的指定 partition（分区）中。</p>
<h2 id="Consumer"><a href="#Consumer" class="headerlink" title="Consumer"></a>Consumer</h2><p>消息消费者，向 <code>kafka broker</code> 取消息的客户端。每个消费者都要维护自己读取数据的 offset。</p>
<p>0.9 之前的低版本 kafka 将 offset 保存在 Zookeeper 中，0.9及之后保存在 Kafka 的 <code>__consumer_offsets</code> 主题中。</p>
<hr>
<h2 id="Consumer-Group"><a href="#Consumer-Group" class="headerlink" title="Consumer Group"></a>Consumer Group</h2><p>每个消费者都会使用一个消费组名称来进行标识。同一个组中的不同的消费者实例，可以分布在多个进程或多个机器上！</p>
<p>如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例（单播）。即每个消费者可以同时读取一个topic的不同分区！</p>
<p>如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程(广播)。</p>
<p>如果需要实现广播，只要每个consumer有一个独立的组就可以了。要实现单播只要所有的consumer在同一个组。</p>
<p>一个topic可以有多个consumer group。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个partion只会把消息发给该CG中的一个consumer。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/25/HttpServletRequest%E5%92%8CHttpServletResponse/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/11/25/HttpServletRequest%E5%92%8CHttpServletResponse/" class="post-title-link" itemprop="url">HttpServletRequest和HttpServletResponse</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-11-25 22:24:14" itemprop="dateCreated datePublished" datetime="2021-11-25T22:24:14+08:00">2021-11-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-11-26 18:03:37" itemprop="dateModified" datetime="2021-11-26T18:03:37+08:00">2021-11-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/JavaWeb/" itemprop="url" rel="index"><span itemprop="name">JavaWeb</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li>Request 对象和 Response 对象是 TomCat 服务器在监听到请求后由 Servlet 程序自动创建的，我们可以在 Servlet 的实现类中直接使用。</li>
<li>Request 对象是用来获取请求消息的，Response 对象是用来返回响应消息的。</li>
</ul>
<h1 id="1-Request"><a href="#1-Request" class="headerlink" title="1    Request"></a>1    Request</h1><h3 id="6-1-1-Request对象的继承结构"><a href="#6-1-1-Request对象的继承结构" class="headerlink" title="6.1.1    Request对象的继承结构"></a>6.1.1    Request对象的继承结构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ServletRequest		-- 接口</span><br><span class="line">	|（继承）</span><br><span class="line">HttpServletRequest		-- 接口</span><br><span class="line">	|（实现）</span><br><span class="line">org.apache.catalina.connector.RequestFacade 	--类（TomCat创建的）</span><br></pre></td></tr></table></figure>

<h3 id="6-1-2-HttpServletRequest"><a href="#6-1-2-HttpServletRequest" class="headerlink" title="6.1.2    HttpServletRequest"></a>6.1.2    HttpServletRequest</h3><h4 id="1、HttpServletRequest-有什么作用？"><a href="#1、HttpServletRequest-有什么作用？" class="headerlink" title="1、HttpServletRequest 有什么作用？"></a>1、HttpServletRequest 有什么作用？</h4><ol>
<li>TomCat 服务器会一直监听客户端的请求，并把请求消息中携带的信息封装到 ServletRequest 接口的实现类对象中；</li>
<li>通过 Servlet 对象的 <code>service()</code>方法（或者 <code>doGet()</code> 和 <code>doPos()</code> 方法）可以获取到这个实现类对象；</li>
<li>通过<code>HttpServletRequest</code> 的子类实例对象，获取到客户端请求消息中包含的信息；</li>
<li>在<code>service()</code>方法中处理请求并作出响应，将响应结果封装到<code>HttpServletResponse</code>的实现类对象中，返回给客户端。</li>
</ol>
<h4 id="2、HttpServletRequest类中的方法："><a href="#2、HttpServletRequest类中的方法：" class="headerlink" title="2、HttpServletRequest类中的方法："></a>2、HttpServletRequest类中的方法：</h4><h5 id="1、获取请求消息中的数据的方法"><a href="#1、获取请求消息中的数据的方法" class="headerlink" title="1、获取请求消息中的数据的方法"></a>1、获取请求消息中的数据的方法</h5><ol>
<li><p><strong>获取请求行数据</strong>：</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="left">方法</th>
<th align="left">说明</th>
<th align="left">例子</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="left">获取请求消息的请求方式</td>
<td align="left"><code>String getMethod()</code></td>
<td align="left"><code>GET</code></td>
</tr>
<tr>
<td align="center">2</td>
<td align="left">获取访问路径(url-pattern)</td>
<td align="left"><code>String getServletPath()</code></td>
<td align="left"><code>/demo01</code></td>
</tr>
<tr>
<td align="center">3</td>
<td align="left">获取请求参数[只适用于GET请求]</td>
<td align="left"><code>String getQueryString()</code></td>
<td align="left"><code>name=zhangsan</code></td>
</tr>
<tr>
<td align="center">4</td>
<td align="left">获取URI（统一资源标识符）</td>
<td align="left"><code>String getRequestURI()</code></td>
<td align="left"><code>/day14/demo01</code></td>
</tr>
<tr>
<td align="center">5</td>
<td align="left">获取URL（统一资源定位符）</td>
<td align="left"><code>StringBuffer getRequestURL()</code></td>
<td align="left"><code>http://localhost/day14/demo01</code></td>
</tr>
<tr>
<td align="center">6</td>
<td align="left">获取协议及版本信息</td>
<td align="left"><code>String getProtocol()</code></td>
<td align="left">-</td>
</tr>
<tr>
<td align="center">7</td>
<td align="left">获取客户机的IP地址</td>
<td align="left"><code>String getRemoteHost()</code></td>
<td align="left">-</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"></td>
<td align="left"><code>String getRemoteAddr()</code></td>
<td align="left">-</td>
</tr>
</tbody></table>
<ul>
<li><code>ServletRequest</code> 接口中没有定义<code>getMethod()</code>方法，<code>HttpServletRequest</code>接口中才开始定义该方法，所以如果想要使用该方法，自定义的类必须实现 <code>HttpServletRequest</code>接口。</li>
</ul>
</li>
<li><p><strong>获取请求头数据</strong></p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="left">方法</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="left"><code>Enumeration&lt;String&gt; getHeaderNames()</code></td>
<td align="left">获取所有请求头的名称</td>
</tr>
<tr>
<td align="center">2</td>
<td align="left"><code>String getHeader(String name)</code></td>
<td align="left">通过请求头的名称获取请求头的值</td>
</tr>
</tbody></table>
</li>
<li><p><strong>获取请求体数据</strong>[<strong>POST请求专有</strong>]</p>
<p> <code>ServletRequest</code> 接口会将 Request 请求消息中的请求体封装到一个<strong>数据流对象</strong>中，只有获得了这个流对象，就可以从流对象中读取到<strong>请求体数据</strong>。所以说获取请求体的数据可分为2步实现：</p>
<ol>
<li>获取数据流对象</li>
<li>从流对象中读取请求体数据</li>
</ol>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="left">方法</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center"></td>
<td align="left">获取流对象</td>
<td align="left"></td>
</tr>
<tr>
<td align="center">1</td>
<td align="left"><code>BufferedReader getReader()</code></td>
<td align="left">获取字符输入流</td>
</tr>
<tr>
<td align="center">2</td>
<td align="left"><code>ServletInputStream getInputStream()</code></td>
<td align="left">获取字节输入流</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="center"></td>
<td align="left">从流对象中读取数据</td>
<td align="left">参考File对象</td>
</tr>
</tbody></table>
</li>
<li><p><strong>获取请求参数的通用方法</strong>：</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="left">方法</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="left"><code>String getParameter(String name)</code></td>
<td align="left">根据请求参数的key获取value</td>
</tr>
<tr>
<td align="center">2</td>
<td align="left"><code>String[] getParameterValues(String name)</code></td>
<td align="left">根据请求参数的key获取value数组</td>
</tr>
<tr>
<td align="center">3</td>
<td align="left"><code>Enumeration&lt;String&gt; getParameterNames()</code></td>
<td align="left">获取所有请求参数的key</td>
</tr>
<tr>
<td align="center">4</td>
<td align="left"><code>Map&lt;String,String[]&gt; getParameterMap()</code></td>
<td align="left">获取所有参数的Map集合</td>
</tr>
</tbody></table>
</li>
<li><p><strong>其它[☆]</strong></p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="left">方法</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="left"><code>getAtrribute(key)</code></td>
<td align="left">获取Request共享域中的数据</td>
</tr>
<tr>
<td align="center">2</td>
<td align="left"><code>getRequestDispatcher(String path)</code></td>
<td align="left">获取请求转发对象</td>
</tr>
</tbody></table>
</li>
</ol>
<h5 id="2、修改请求消息中数据的方法"><a href="#2、修改请求消息中数据的方法" class="headerlink" title="2、修改请求消息中数据的方法"></a>2、修改请求消息中数据的方法</h5><table>
<thead>
<tr>
<th align="center"></th>
<th align="left">方法</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="left"><code>setAttribute(key, value)</code></td>
<td align="left">向Request共享域中添加数据</td>
</tr>
<tr>
<td align="center">2</td>
<td align="left"><code>setCharacterEncoding(&quot;utf-8&quot;)</code></td>
<td align="left">设置请求体的编码方式为utf-8</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="center"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<ol>
<li><p><strong>中文乱码问题</strong></p>
<blockquote>
<p>GET方式</p>
<ul>
<li>TomCat 8++之后已经将GET提交中文乱码的问题解决了。</li>
</ul>
<p>POST方式：</p>
<ul>
<li>在获取请求体之前，先通过Request对象设置请求体的编码方式</li>
</ul>
</blockquote>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.先设置请求体的编码方式为utf-8</span></span><br><span class="line">request.setCharacterEncoding(<span class="string">&quot;utf-8&quot;</span>);</span><br><span class="line"><span class="comment">// 2.根据参数名称获取参数值</span></span><br><span class="line">String <span class="title function_">getParameter</span><span class="params">(String name)</span>;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Tip：一定要在获取请求参数之前调用该方法，加入你要获取的请求参数有多个，但是只有最后一个会出现乱码，也必须在获取获取第一个请求参数之前调用<code>setCharacterEncoding()</code>方法，否则该方法不会生效。</p>
</blockquote>
</li>
</ol>
<h4 id="3、请求转发"><a href="#3、请求转发" class="headerlink" title="3、请求转发"></a>3、请求转发</h4><blockquote>
<p><strong>请求转发</strong>是指：服务器收到请求后，从一个资源跳到另一个资源的操作。</p>
<ul>
<li>请求转发是一种服务内部的资源跳转方式。</li>
</ul>
</blockquote>
<h5 id="3-1、为什么要使用请求转发？"><a href="#3-1、为什么要使用请求转发？" class="headerlink" title="3.1、为什么要使用请求转发？"></a>3.1、为什么要使用请求转发？</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">客户端通过浏览器访问服务器资源； </span><br><span class="line">--&gt;&gt; </span><br><span class="line">在服务器内部，我们希望服务器是&quot;高内聚、低耦合&quot;的，尽量让每个类满足单一职责要求。	这种情况下，客户端要访问的资源就不可能只存储在一个类中，也就是说客户端一次请求所需要的资源需要在服务器的同一个工程下的多个类中获取。这时候就需要请求转发了；</span><br><span class="line">--&gt;&gt; </span><br><span class="line">请求转发规定了一块范围，这块范围内的所有数据可以共享。</span><br></pre></td></tr></table></figure>

<p><img src="/2021/11/25/HttpServletRequest%E5%92%8CHttpServletResponse/image-20200519150806643-1597652154014.png" alt="image-20200519150806643"></p>
<h5 id="3-2、请求转发的特点"><a href="#3-2、请求转发的特点" class="headerlink" title="3.2、请求转发的特点"></a>3.2、请求转发的特点</h5><blockquote>
<ul>
<li>浏览器的地址栏路径不会发生变化。</li>
<li>只能转发到当前Web工程的子资源中。</li>
<li>请求转发是一次请求。</li>
<li>请求转发可以访问到<code>WEB-INF</code>目录下。[<code>WEB-INF</code>是受保护的目录，浏览器不能直接访问到该目录]</li>
</ul>
</blockquote>
<h5 id="3-3、Request共享域"><a href="#3-3、Request共享域" class="headerlink" title="3.3、Request共享域"></a>3.3、Request共享域</h5><blockquote>
<ul>
<li><p><code>request共享域</code>：代表一次请求的范围，一般用于请求转发的多个资源间共享数据。</p>
</li>
<li><p>共享数据必须被保存到共享域中才能被该共享域中的其它对象访问到。</p>
</li>
</ul>
</blockquote>
<p>常用方法：</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="left">方法</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="left"><code>void setAttribute(String name, Object obj)</code></td>
<td align="left">添加Map数据到Request共享域中</td>
</tr>
<tr>
<td align="center">2</td>
<td align="left"><code>Object getAttribute(String name)</code></td>
<td align="left">从Request共享域中通过键获取值</td>
</tr>
<tr>
<td align="center">3</td>
<td align="left"><code>void removeAttribute(String name)</code></td>
<td align="left">通过键删除键值对</td>
</tr>
</tbody></table>
<h5 id="3-4-请求转发的步骤"><a href="#3-4-请求转发的步骤" class="headerlink" title="3.4    请求转发的步骤"></a>3.4    请求转发的步骤</h5><ol>
<li><p>通过Request对象获取<em>请求转发器</em>对象：</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">RequestDispatcher <span class="title function_">getRequestDispatcher</span><span class="params">(String path)</span></span><br><span class="line"><span class="comment">// 如：</span></span><br><span class="line">    req.getRequestDispatcher(String path)	<span class="comment">// 参数为要转发的地址的资源路径url-pattern</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">	请求转发必须要以 &quot;/&quot; 开头，斜杠表示绝对路径，地址为：</span></span><br><span class="line"><span class="comment">		http://ip:port/工程名/</span></span><br><span class="line"><span class="comment">	映射到IDEA代码的WEB目录</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></li>
<li><p>使用<code>RequestDispatcher</code>转发器对象来进行转发：</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">	请求转发是一次请求，但是要访问多个资源。所有的请求信息都包含在第一次请求中，所有的响应消息都在最后一次请求完毕后返回给用户。</span></span><br><span class="line"><span class="comment">	所以在转发的时候，要把	客户端的 请求和响应 同时转发给下一个服务器资源对象 </span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">forward(ServletRequest req, Servletresponse resp)</span><br></pre></td></tr></table></figure></li>
</ol>
<h5 id="3-5-lt-Base-gt-标签的作用"><a href="#3-5-lt-Base-gt-标签的作用" class="headerlink" title="3.5    &lt;Base&gt;标签的作用"></a>3.5    <code>&lt;Base&gt;</code>标签的作用</h5><ul>
<li><p><code>&lt;Base&gt;</code>标签可以设置当前页面中所有相对路径的参考路径</p>
</li>
<li><p>我们在WEB目录下新建两个HTML项目</p>
<ol>
<li><p><code>index.html</code>：这是一个首页，可以跳转到<code>c.html</code>，也可以跳转到转发器ForwardC，通过转发器转发到<code>c.html</code></p>
 <figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>首页<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;a/b/c.html&quot;</span>&gt;</span>跳转到网页C<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;http://localhost:8080/D1/demo06&quot;</span>&gt;</span>调到转发器，转发器转发到网页C<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p><code>a/b/c.html</code>：这是C页面，可以跳转回首页（<code>index.html</code>）【通过相对路径跳转】</p>
 <figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>这里是C网页<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--添加&lt;base&gt;标签--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">base</span> <span class="attr">href</span>=<span class="string">&quot;http://localhost:8080/D1/a/b/c.html&quot;</span>&gt;</span>	 </span><br><span class="line"><span class="comment">&lt;!--&lt;base href=&quot;http://localhost:8080/D1/a/b/&quot;&gt;--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--文件名可以省略，斜杠不能省略。[省略斜杠会让b默认是文件，而不是文件夹]--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;../../index.html&quot;</span>&gt;</span>跳转到首页<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>

</li>
<li><p><code>ForwardC.java</code>：这是一个Java类，里面有一个转发器，跳转到C网页（<code>c.html</code>）</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Demo06_ForwardC</span> <span class="keyword">extends</span> <span class="title class_">HttpServlet</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">doGet</span><span class="params">(HttpServletRequest req, HttpServletResponse resp)</span> <span class="keyword">throws</span> ServletException, IOException &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;这里是转发器ForwardC&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">            WEB目录下的首页 index.html --&gt;&gt; 转发器ForwardC --&gt;&gt; WEB/a/b/c.html</span></span><br><span class="line"><span class="comment">	   */</span></span><br><span class="line">        <span class="type">RequestDispatcher</span> <span class="variable">requestDispatcher</span> <span class="operator">=</span> req.getRequestDispatcher(<span class="string">&quot;/a/b/c.html&quot;</span>);</span><br><span class="line">        requestDispatcher.forward(req, resp);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ul>
<p><img src="/2021/11/25/HttpServletRequest%E5%92%8CHttpServletResponse/image-20200519154532869-1597652154015.png" alt="image-20200519154532869"></p>
<ul>
<li><p>如果不使用<code>&lt;base&gt;</code>，经过转发器转发后，<code>c.html</code>返回首页的链接就会以转发器所在的<code>ForwardC.java</code>类的绝对路径为标准进行相对查找，这样找到的路径是不对的。</p>
</li>
<li><p>所以我们必须设置一个具有参考意义的绝对路径，让每次执行跳转到首页的操作时参考该路径，就不会出现转发器转发后路径错误的现象。</p>
</li>
<li><p><code>&lt;Base&gt;</code>标签可以设置当前页面中所有相对路径的参考路径，该参考路径为绝对路径。</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http:<span class="comment">//ip:port/工程路径/[资源路径] </span></span><br><span class="line"><span class="comment">// 资源路径可以省略</span></span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h1 id="2-Response"><a href="#2-Response" class="headerlink" title="2    Response"></a>2    Response</h1><h2 id="2-1-HttpServletResopnse"><a href="#2-1-HttpServletResopnse" class="headerlink" title="2.1    HttpServletResopnse"></a>2.1    HttpServletResopnse</h2><p>TomCat 服务器在运行时会维护 <code>HttpServletResponse</code> 接口和 <code>HttpServletRequest</code> 接口的实现类对象，每当服务器监听到请求消息时，就会为该请求创建一对 Request 的实现类对象和 Response 的实现类类对象。</p>
<p><code>HttpServletRequest</code> 对象用于封装客户端的请求数据，<code>HttpServletResponse</code> 对象则用于封装服务器的响应数据。</p>
<p>TomCat 会进一步把 <code>HttpServletResponse</code> 对象响应给客户端。</p>
<hr>
<h2 id="HttpServletResopnse-源码分析"><a href="#HttpServletResopnse-源码分析" class="headerlink" title="HttpServletResopnse 源码分析"></a>HttpServletResopnse 源码分析</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> javax.servlet.http;	<span class="comment">// Servlet包下的API</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Collection;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.function.Supplier;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.servlet.ServletResponse;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">1、继承自 ServletResponse 接口</span></span><br><span class="line"><span class="comment">2、实现了基于 HTTP 协议的响应相关的功能</span></span><br><span class="line"><span class="comment">3、分析：ServletResponse 是一个更为宽泛的概念，所有的响应（不仅仅局限于HTTP，包括RPC响应等等）</span></span><br><span class="line"><span class="comment">消息都需要实现这个接口，而 HttpServletResponse 只是 HTTP 协议关于响应消息的一个实现</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">HttpServletResponse</span> <span class="keyword">extends</span> <span class="title class_">ServletResponse</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 1、添加指定的cookie到响应体。</span></span><br><span class="line"><span class="comment">     * 2、可以多次调用此方法添加多个cookie。 </span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addCookie</span><span class="params">(Cookie cookie)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 作用：返回一个Boolean值，判断响应头 response header 中是否含有某个值，</span></span><br><span class="line"><span class="comment">     * 常用于判断响应头中是否含有某个值，如果有就删除/替换（没有就添加）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">containsHeader</span><span class="params">(String name)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Encodes the specified URL by including the session ID in it, or, if</span></span><br><span class="line"><span class="comment">     * encoding is not needed, returns the URL unchanged. The implementation of</span></span><br><span class="line"><span class="comment">     * this method includes the logic to determine whether the session ID needs</span></span><br><span class="line"><span class="comment">     * to be encoded in the URL. For example, if the browser supports cookies,</span></span><br><span class="line"><span class="comment">     * or session tracking is turned off, URL encoding is unnecessary.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * For robust session tracking, all URLs emitted by a servlet should be run</span></span><br><span class="line"><span class="comment">     * through this method. Otherwise, URL rewriting cannot be used with</span></span><br><span class="line"><span class="comment">     * browsers which do not support cookies.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> url</span></span><br><span class="line"><span class="comment">     *            the url to be encoded.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> the encoded URL if encoding is needed; the unchanged URL</span></span><br><span class="line"><span class="comment">     *         otherwise.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">encodeURL</span><span class="params">(String url)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Encodes the specified URL for use in the &lt;code&gt;sendRedirect&lt;/code&gt; method</span></span><br><span class="line"><span class="comment">     * or, if encoding is not needed, returns the URL unchanged. The</span></span><br><span class="line"><span class="comment">     * implementation of this method includes the logic to determine whether the</span></span><br><span class="line"><span class="comment">     * session ID needs to be encoded in the URL. Because the rules for making</span></span><br><span class="line"><span class="comment">     * this determination can differ from those used to decide whether to encode</span></span><br><span class="line"><span class="comment">     * a normal link, this method is separated from the &lt;code&gt;encodeURL&lt;/code&gt;</span></span><br><span class="line"><span class="comment">     * method.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * All URLs sent to the &lt;code&gt;HttpServletResponse.sendRedirect&lt;/code&gt; method</span></span><br><span class="line"><span class="comment">     * should be run through this method. Otherwise, URL rewriting cannot be</span></span><br><span class="line"><span class="comment">     * used with browsers which do not support cookies.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> url</span></span><br><span class="line"><span class="comment">     *            the url to be encoded.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> the encoded URL if encoding is needed; the unchanged URL</span></span><br><span class="line"><span class="comment">     *         otherwise.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #sendRedirect</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #encodeUrl</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">encodeRedirectURL</span><span class="params">(String url)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> url</span></span><br><span class="line"><span class="comment">     *            the url to be encoded.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> the encoded URL if encoding is needed; the unchanged URL</span></span><br><span class="line"><span class="comment">     *         otherwise.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@deprecated</span> As of version 2.1, use encodeURL(String url) instead</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Deprecated</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">encodeUrl</span><span class="params">(String url)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> url</span></span><br><span class="line"><span class="comment">     *            the url to be encoded.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> the encoded URL if encoding is needed; the unchanged URL</span></span><br><span class="line"><span class="comment">     *         otherwise.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@deprecated</span> As of version 2.1, use encodeRedirectURL(String url) instead</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Deprecated</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">encodeRedirectUrl</span><span class="params">(String url)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sends an error response to the client using the specified status code and</span></span><br><span class="line"><span class="comment">     * clears the output buffer. The server defaults to creating the response to</span></span><br><span class="line"><span class="comment">     * look like an HTML-formatted server error page containing the specified</span></span><br><span class="line"><span class="comment">     * message, setting the content type to &quot;text/html&quot;, leaving cookies and</span></span><br><span class="line"><span class="comment">     * other headers unmodified. If an error-page declaration has been made for</span></span><br><span class="line"><span class="comment">     * the web application corresponding to the status code passed in, it will</span></span><br><span class="line"><span class="comment">     * be served back in preference to the suggested msg parameter.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * If the response has already been committed, this method throws an</span></span><br><span class="line"><span class="comment">     * IllegalStateException. After using this method, the response should be</span></span><br><span class="line"><span class="comment">     * considered to be committed and should not be written to.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> sc</span></span><br><span class="line"><span class="comment">     *            the error status code</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msg</span></span><br><span class="line"><span class="comment">     *            the descriptive message</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@exception</span> IOException</span></span><br><span class="line"><span class="comment">     *                If an input or output exception occurs</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@exception</span> IllegalStateException</span></span><br><span class="line"><span class="comment">     *                If the response was committed</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendError</span><span class="params">(<span class="type">int</span> sc, String msg)</span> <span class="keyword">throws</span> IOException;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sends an error response to the client using the specified status code and</span></span><br><span class="line"><span class="comment">     * clears the buffer. This is equivalent to calling &#123;<span class="doctag">@link</span> #sendError(int,</span></span><br><span class="line"><span class="comment">     * String)&#125; with the same status code and &lt;code&gt;null&lt;/code&gt; for the message.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> sc</span></span><br><span class="line"><span class="comment">     *            the error status code</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@exception</span> IOException</span></span><br><span class="line"><span class="comment">     *                If an input or output exception occurs</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@exception</span> IllegalStateException</span></span><br><span class="line"><span class="comment">     *                If the response was committed before this method call</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendError</span><span class="params">(<span class="type">int</span> sc)</span> <span class="keyword">throws</span> IOException;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sends a temporary redirect response to the client using the specified</span></span><br><span class="line"><span class="comment">     * redirect location URL. This method can accept relative URLs; the servlet</span></span><br><span class="line"><span class="comment">     * container must convert the relative URL to an absolute URL before sending</span></span><br><span class="line"><span class="comment">     * the response to the client. If the location is relative without a leading</span></span><br><span class="line"><span class="comment">     * &#x27;/&#x27; the container interprets it as relative to the current request URI.</span></span><br><span class="line"><span class="comment">     * If the location is relative with a leading &#x27;/&#x27; the container interprets</span></span><br><span class="line"><span class="comment">     * it as relative to the servlet container root.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * If the response has already been committed, this method throws an</span></span><br><span class="line"><span class="comment">     * IllegalStateException. After using this method, the response should be</span></span><br><span class="line"><span class="comment">     * considered to be committed and should not be written to.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> location</span></span><br><span class="line"><span class="comment">     *            the redirect location URL</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@exception</span> IOException</span></span><br><span class="line"><span class="comment">     *                If an input or output exception occurs</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@exception</span> IllegalStateException</span></span><br><span class="line"><span class="comment">     *                If the response was committed or if a partial URL is given</span></span><br><span class="line"><span class="comment">     *                and cannot be converted into a valid URL</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendRedirect</span><span class="params">(String location)</span> <span class="keyword">throws</span> IOException;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sets a response header with the given name and date-value. The date is</span></span><br><span class="line"><span class="comment">     * specified in terms of milliseconds since the epoch. If the header had</span></span><br><span class="line"><span class="comment">     * already been set, the new value overwrites the previous one. The</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;containsHeader&lt;/code&gt; method can be used to test for the presence</span></span><br><span class="line"><span class="comment">     * of a header before setting its value.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">     *            the name of the header to set</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> date</span></span><br><span class="line"><span class="comment">     *            the assigned date value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #containsHeader</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #addDateHeader</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setDateHeader</span><span class="params">(String name, <span class="type">long</span> date)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Adds a response header with the given name and date-value. The date is</span></span><br><span class="line"><span class="comment">     * specified in terms of milliseconds since the epoch. This method allows</span></span><br><span class="line"><span class="comment">     * response headers to have multiple values.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">     *            the name of the header to set</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> date</span></span><br><span class="line"><span class="comment">     *            the additional date value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setDateHeader</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addDateHeader</span><span class="params">(String name, <span class="type">long</span> date)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sets a response header with the given name and value. If the header had</span></span><br><span class="line"><span class="comment">     * already been set, the new value overwrites the previous one. The</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;containsHeader&lt;/code&gt; method can be used to test for the presence</span></span><br><span class="line"><span class="comment">     * of a header before setting its value.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">     *            the name of the header</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     *            the header value If it contains octet string, it should be</span></span><br><span class="line"><span class="comment">     *            encoded according to RFC 2047</span></span><br><span class="line"><span class="comment">     *            (http://www.ietf.org/rfc/rfc2047.txt)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #containsHeader</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #addHeader</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setHeader</span><span class="params">(String name, String value)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Adds a response header with the given name and value. This method allows</span></span><br><span class="line"><span class="comment">     * response headers to have multiple values.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">     *            the name of the header</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     *            the additional header value If it contains octet string, it</span></span><br><span class="line"><span class="comment">     *            should be encoded according to RFC 2047</span></span><br><span class="line"><span class="comment">     *            (http://www.ietf.org/rfc/rfc2047.txt)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setHeader</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addHeader</span><span class="params">(String name, String value)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sets a response header with the given name and integer value. If the</span></span><br><span class="line"><span class="comment">     * header had already been set, the new value overwrites the previous one.</span></span><br><span class="line"><span class="comment">     * The &lt;code&gt;containsHeader&lt;/code&gt; method can be used to test for the</span></span><br><span class="line"><span class="comment">     * presence of a header before setting its value.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">     *            the name of the header</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     *            the assigned integer value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #containsHeader</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #addIntHeader</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setIntHeader</span><span class="params">(String name, <span class="type">int</span> value)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Adds a response header with the given name and integer value. This method</span></span><br><span class="line"><span class="comment">     * allows response headers to have multiple values.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">     *            the name of the header</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     *            the assigned integer value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setIntHeader</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addIntHeader</span><span class="params">(String name, <span class="type">int</span> value)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sets the status code for this response. </span></span><br><span class="line"><span class="comment">     * 设置响应状态码</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setStatus</span><span class="params">(<span class="type">int</span> sc)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sets the status code and message for this response.</span></span><br><span class="line"><span class="comment">     * 设置响应状态码和响应消息</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Deprecated</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setStatus</span><span class="params">(<span class="type">int</span> sc, String sm)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Get the HTTP status code for this Response.</span></span><br><span class="line"><span class="comment">     * 获取响应状态码</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getStatus</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Return the value for the specified header, or &lt;code&gt;null&lt;/code&gt; if this</span></span><br><span class="line"><span class="comment">     * header has not been set.  If more than one value was added for this</span></span><br><span class="line"><span class="comment">     * name, only the first is returned; use &#123;<span class="doctag">@link</span> #getHeaders(String)&#125; to</span></span><br><span class="line"><span class="comment">     * retrieve all of them.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> name Header name to look up</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 1、根据参数name获取响应头中对应的值；</span></span><br><span class="line"><span class="comment">     * 2、如果值不存在，返回null；</span></span><br><span class="line"><span class="comment">     * 3、如果存在多个值，只返回第一个。</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@since</span> Servlet 3.0</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getHeader</span><span class="params">(String name)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Return a Collection of all the header values associated with the</span></span><br><span class="line"><span class="comment">     * specified header name.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 1、根据参数name获取响应头中对应的所有值</span></span><br><span class="line"><span class="comment">     * 2、弥补上一个方法的缺陷，返回的是一个集合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> Collection&lt;String&gt; <span class="title function_">getHeaders</span><span class="params">(String name)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Get the header names set for this HTTP response.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> The header names set for this HTTP response.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@since</span> Servlet 3.0</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> Collection&lt;String&gt; <span class="title function_">getHeaderNames</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Configure the supplier of the trailer headers. The supplier will be</span></span><br><span class="line"><span class="comment">     * called in the scope of the thread that completes the response.</span></span><br><span class="line"><span class="comment">     * &lt;br&gt;</span></span><br><span class="line"><span class="comment">     * Trailers that don&#x27;t meet the requirements of RFC 7230, section 4.1.2 will</span></span><br><span class="line"><span class="comment">     * be ignored.</span></span><br><span class="line"><span class="comment">     * &lt;br&gt;</span></span><br><span class="line"><span class="comment">     * The default implementation is a NO-OP.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> supplier The supplier for the trailer headers</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IllegalStateException if this method is called when the</span></span><br><span class="line"><span class="comment">     *         underlying protocol does not support trailer headers or if using</span></span><br><span class="line"><span class="comment">     *         HTTP/1.1 and the response has already been committed</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@since</span> Servlet 4.0</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">default</span> <span class="keyword">void</span> <span class="title function_">setTrailerFields</span><span class="params">(Supplier&lt;Map&lt;String, String&gt;&gt; supplier)</span> &#123;</span><br><span class="line">        <span class="comment">// NO-OP</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Obtain the supplier of the trailer headers.</span></span><br><span class="line"><span class="comment">     * &lt;br&gt;</span></span><br><span class="line"><span class="comment">     * The default implementation returns null.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> The supplier for the trailer headers</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@since</span> Servlet 4.0</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">default</span> Supplier&lt;Map&lt;String, String&gt;&gt; <span class="title function_">getTrailerFields</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * Server status codes; see RFC 2068.</span></span><br><span class="line"><span class="comment">     * 响应状态码</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_CONTINUE</span> <span class="operator">=</span> <span class="number">100</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_SWITCHING_PROTOCOLS</span> <span class="operator">=</span> <span class="number">101</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_OK</span> <span class="operator">=</span> <span class="number">200</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_CREATED</span> <span class="operator">=</span> <span class="number">201</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_ACCEPTED</span> <span class="operator">=</span> <span class="number">202</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_NON_AUTHORITATIVE_INFORMATION</span> <span class="operator">=</span> <span class="number">203</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_NO_CONTENT</span> <span class="operator">=</span> <span class="number">204</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_RESET_CONTENT</span> <span class="operator">=</span> <span class="number">205</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_PARTIAL_CONTENT</span> <span class="operator">=</span> <span class="number">206</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_MULTIPLE_CHOICES</span> <span class="operator">=</span> <span class="number">300</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_MOVED_PERMANENTLY</span> <span class="operator">=</span> <span class="number">301</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_MOVED_TEMPORARILY</span> <span class="operator">=</span> <span class="number">302</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_FOUND</span> <span class="operator">=</span> <span class="number">302</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_SEE_OTHER</span> <span class="operator">=</span> <span class="number">303</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_NOT_MODIFIED</span> <span class="operator">=</span> <span class="number">304</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_USE_PROXY</span> <span class="operator">=</span> <span class="number">305</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_TEMPORARY_REDIRECT</span> <span class="operator">=</span> <span class="number">307</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_BAD_REQUEST</span> <span class="operator">=</span> <span class="number">400</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_UNAUTHORIZED</span> <span class="operator">=</span> <span class="number">401</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_PAYMENT_REQUIRED</span> <span class="operator">=</span> <span class="number">402</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_FORBIDDEN</span> <span class="operator">=</span> <span class="number">403</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_NOT_FOUND</span> <span class="operator">=</span> <span class="number">404</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_METHOD_NOT_ALLOWED</span> <span class="operator">=</span> <span class="number">405</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_NOT_ACCEPTABLE</span> <span class="operator">=</span> <span class="number">406</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_PROXY_AUTHENTICATION_REQUIRED</span> <span class="operator">=</span> <span class="number">407</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_REQUEST_TIMEOUT</span> <span class="operator">=</span> <span class="number">408</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_CONFLICT</span> <span class="operator">=</span> <span class="number">409</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_GONE</span> <span class="operator">=</span> <span class="number">410</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_LENGTH_REQUIRED</span> <span class="operator">=</span> <span class="number">411</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_PRECONDITION_FAILED</span> <span class="operator">=</span> <span class="number">412</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_REQUEST_ENTITY_TOO_LARGE</span> <span class="operator">=</span> <span class="number">413</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_REQUEST_URI_TOO_LONG</span> <span class="operator">=</span> <span class="number">414</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_UNSUPPORTED_MEDIA_TYPE</span> <span class="operator">=</span> <span class="number">415</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_REQUESTED_RANGE_NOT_SATISFIABLE</span> <span class="operator">=</span> <span class="number">416</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_EXPECTATION_FAILED</span> <span class="operator">=</span> <span class="number">417</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_INTERNAL_SERVER_ERROR</span> <span class="operator">=</span> <span class="number">500</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_NOT_IMPLEMENTED</span> <span class="operator">=</span> <span class="number">501</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_BAD_GATEWAY</span> <span class="operator">=</span> <span class="number">502</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_SERVICE_UNAVAILABLE</span> <span class="operator">=</span> <span class="number">503</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_GATEWAY_TIMEOUT</span> <span class="operator">=</span> <span class="number">504</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">SC_HTTP_VERSION_NOT_SUPPORTED</span> <span class="operator">=</span> <span class="number">505</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="1、设置响应行"><a href="#1、设置响应行" class="headerlink" title="1、设置响应行"></a>1、设置响应行</h3><p><strong>响应行格式：</strong></p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">HTTP/1.1</span> <span class="number">200</span> OK		</span><br></pre></td></tr></table></figure>

<p><strong>设置响应状态码</strong></p>
<ul>
<li>  在响应行中，协议名称、版本号以及协议状态都不用手动设置，唯一可以手动设置的只有状态码了。</li>
</ul>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="left">方法</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="left"><code>setStatus(int sc)</code></td>
<td align="left">设置响应状态码</td>
</tr>
</tbody></table>
<hr>
<h3 id="2、设置响应头"><a href="#2、设置响应头" class="headerlink" title="2、设置响应头"></a>2、设置响应头</h3><ul>
<li>  响应头都是一些键值对数据</li>
</ul>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="left">方法</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="left"><code>setHeader(String name, String value)</code></td>
<td align="left">设置响应头消息（重置）</td>
</tr>
<tr>
<td align="center">2</td>
<td align="left"><code>addHeader(String name, String value)</code></td>
<td align="left">添加响应头（追加）</td>
</tr>
</tbody></table>
<hr>
<h3 id="3、设置响应体（重要）"><a href="#3、设置响应体（重要）" class="headerlink" title="3、设置响应体（重要）"></a>3、设置响应体（重要）</h3><blockquote>
<p>  <strong>ServletResponse 源码</strong></p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> javax.servlet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.PrintWriter;</span><br><span class="line"><span class="keyword">import</span> java.util.Locale;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 定义一个对象用于帮助 servlet 服务器向客户端发送响应。</span></span><br><span class="line"><span class="comment"> * 这个 Servlet 容器会创建一个 ServletResponse 对象，</span></span><br><span class="line"><span class="comment"> * 并且将它作为参数传递给 Servlet 的 service 方法。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 1、如果想要返回二进制（binary）的MIME类型的响应数据，请使用</span></span><br><span class="line"><span class="comment"> * ServletOutputStream 类中的 getOutputStream 方法返回的对象； </span></span><br><span class="line"><span class="comment"> * 2、如果想要返回 character 类型的数据，请使用 PrintWriter中的#getWriter方法</span></span><br><span class="line"><span class="comment"> * 3、如果想要返回二进制和文本的混合类型数据（mix binary and text data），可以</span></span><br><span class="line"><span class="comment"> * 使用 ServletOutputStream 创建一个 multipart response 对象，并手动管理character</span></span><br><span class="line"><span class="comment"> * 类型的部分数据。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The charset for the MIME body response can be specified explicitly or</span></span><br><span class="line"><span class="comment"> * implicitly. The priority order for specifying the response body is:</span></span><br><span class="line"><span class="comment"> * &lt;ol&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;explicitly per request using &#123;<span class="doctag">@link</span> #setCharacterEncoding&#125; and</span></span><br><span class="line"><span class="comment"> *    &#123;<span class="doctag">@link</span> #setContentType&#125;&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;implicitly per request using &#123;<span class="doctag">@link</span> #setLocale&#125;&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;per web application via the deployment descriptor or</span></span><br><span class="line"><span class="comment"> *     &#123;<span class="doctag">@link</span> ServletContext#setRequestCharacterEncoding(String)&#125;&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;container default via vendor specific configuration&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;ISO-8859-1&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;/ol&gt;</span></span><br><span class="line"><span class="comment"> * The &lt;code&gt;setCharacterEncoding&lt;/code&gt;, &lt;code&gt;setContentType&lt;/code&gt;, or</span></span><br><span class="line"><span class="comment"> * &lt;code&gt;setLocale&lt;/code&gt; method must be called before &lt;code&gt;getWriter&lt;/code&gt;</span></span><br><span class="line"><span class="comment"> * and before committing the response for the character encoding to be used.</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * See the Internet RFCs such as &lt;a href=&quot;http://www.ietf.org/rfc/rfc2045.txt&quot;&gt;</span></span><br><span class="line"><span class="comment"> * RFC 2045&lt;/a&gt; for more information on MIME. Protocols such as SMTP and HTTP</span></span><br><span class="line"><span class="comment"> * define profiles of MIME, and those standards are still evolving.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@see</span> ServletOutputStream</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">ServletResponse</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns the name of the character encoding (MIME charset) used for the</span></span><br><span class="line"><span class="comment">     * body sent in this response.</span></span><br><span class="line"><span class="comment">     * The charset for the MIME body response can be specified explicitly or</span></span><br><span class="line"><span class="comment">     * implicitly. The priority order for specifying the response body is:</span></span><br><span class="line"><span class="comment">     * &lt;ol&gt;</span></span><br><span class="line"><span class="comment">     * &lt;li&gt;explicitly per request using &#123;<span class="doctag">@link</span> #setCharacterEncoding&#125; and</span></span><br><span class="line"><span class="comment">     *    &#123;<span class="doctag">@link</span> #setContentType&#125;&lt;/li&gt;</span></span><br><span class="line"><span class="comment">     * &lt;li&gt;implicitly per request using &#123;<span class="doctag">@link</span> #setLocale&#125;&lt;/li&gt;</span></span><br><span class="line"><span class="comment">     * &lt;li&gt;per web application via the deployment descriptor or</span></span><br><span class="line"><span class="comment">     *     &#123;<span class="doctag">@link</span> ServletContext#setRequestCharacterEncoding(String)&#125;&lt;/li&gt;</span></span><br><span class="line"><span class="comment">     * &lt;li&gt;container default via vendor specific configuration&lt;/li&gt;</span></span><br><span class="line"><span class="comment">     * &lt;li&gt;ISO-8859-1&lt;/li&gt;</span></span><br><span class="line"><span class="comment">     * &lt;/ol&gt;</span></span><br><span class="line"><span class="comment">     * Calls made to &#123;<span class="doctag">@link</span> #setCharacterEncoding&#125;, &#123;<span class="doctag">@link</span> #setContentType&#125; or</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@link</span> #setLocale&#125; after &lt;code&gt;getWriter&lt;/code&gt; has been called or after</span></span><br><span class="line"><span class="comment">     * the response has been committed have no effect on the character encoding.</span></span><br><span class="line"><span class="comment">     * If no character encoding has been specified, &lt;code&gt;ISO-8859-1&lt;/code&gt; is</span></span><br><span class="line"><span class="comment">     * returned.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * See RFC 2047 (http://www.ietf.org/rfc/rfc2047.txt) for more information</span></span><br><span class="line"><span class="comment">     * about character encoding and MIME.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> a &lt;code&gt;String&lt;/code&gt; specifying the name of the character</span></span><br><span class="line"><span class="comment">     *         encoding, for example, &lt;code&gt;UTF-8&lt;/code&gt;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getCharacterEncoding</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns the content type used for the MIME body sent in this response.</span></span><br><span class="line"><span class="comment">     * The content type proper must have been specified using</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@link</span> #setContentType&#125; before the response is committed. If no content</span></span><br><span class="line"><span class="comment">     * type has been specified, this method returns null. If a content type has</span></span><br><span class="line"><span class="comment">     * been specified and a character encoding has been explicitly or implicitly</span></span><br><span class="line"><span class="comment">     * specified as described in &#123;<span class="doctag">@link</span> #getCharacterEncoding&#125;, the charset</span></span><br><span class="line"><span class="comment">     * parameter is included in the string returned. If no character encoding</span></span><br><span class="line"><span class="comment">     * has been specified, the charset parameter is omitted.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> a &lt;code&gt;String&lt;/code&gt; specifying the content type, for example,</span></span><br><span class="line"><span class="comment">     *         &lt;code&gt;text/html; charset=UTF-8&lt;/code&gt;, or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@since</span> 2.4</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getContentType</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns a &#123;<span class="doctag">@link</span> ServletOutputStream&#125; suitable for writing binary data in</span></span><br><span class="line"><span class="comment">     * the response. The servlet container does not encode the binary data.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * Calling flush() on the ServletOutputStream commits the response. Either</span></span><br><span class="line"><span class="comment">     * this method or &#123;<span class="doctag">@link</span> #getWriter&#125; may be called to write the body, not</span></span><br><span class="line"><span class="comment">     * both.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> a &#123;<span class="doctag">@link</span> ServletOutputStream&#125; for writing binary data</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@exception</span> IllegalStateException</span></span><br><span class="line"><span class="comment">     *                if the &lt;code&gt;getWriter&lt;/code&gt; method has been called on</span></span><br><span class="line"><span class="comment">     *                this response</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@exception</span> IOException</span></span><br><span class="line"><span class="comment">     *                if an input or output exception occurred</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #getWriter</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> ServletOutputStream <span class="title function_">getOutputStream</span><span class="params">()</span> <span class="keyword">throws</span> IOException;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns a &lt;code&gt;PrintWriter&lt;/code&gt; object that can send character text to</span></span><br><span class="line"><span class="comment">     * the client. The &lt;code&gt;PrintWriter&lt;/code&gt; uses the character encoding</span></span><br><span class="line"><span class="comment">     * returned by &#123;<span class="doctag">@link</span> #getCharacterEncoding&#125;. If the response&#x27;s character</span></span><br><span class="line"><span class="comment">     * encoding has not been specified as described in</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;getCharacterEncoding&lt;/code&gt; (i.e., the method just returns the</span></span><br><span class="line"><span class="comment">     * default value &lt;code&gt;ISO-8859-1&lt;/code&gt;), &lt;code&gt;getWriter&lt;/code&gt; updates it</span></span><br><span class="line"><span class="comment">     * to &lt;code&gt;ISO-8859-1&lt;/code&gt;.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * Calling flush() on the &lt;code&gt;PrintWriter&lt;/code&gt; commits the response.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * Either this method or &#123;<span class="doctag">@link</span> #getOutputStream&#125; may be called to write the</span></span><br><span class="line"><span class="comment">     * body, not both.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> a &lt;code&gt;PrintWriter&lt;/code&gt; object that can return character data</span></span><br><span class="line"><span class="comment">     *         to the client</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@exception</span> java.io.UnsupportedEncodingException</span></span><br><span class="line"><span class="comment">     *                if the character encoding returned by</span></span><br><span class="line"><span class="comment">     *                &lt;code&gt;getCharacterEncoding&lt;/code&gt; cannot be used</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@exception</span> IllegalStateException</span></span><br><span class="line"><span class="comment">     *                if the &lt;code&gt;getOutputStream&lt;/code&gt; method has already</span></span><br><span class="line"><span class="comment">     *                been called for this response object</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@exception</span> IOException</span></span><br><span class="line"><span class="comment">     *                if an input or output exception occurred</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #getOutputStream</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setCharacterEncoding</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> PrintWriter <span class="title function_">getWriter</span><span class="params">()</span> <span class="keyword">throws</span> IOException;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sets the character encoding (MIME charset) of the response being sent to</span></span><br><span class="line"><span class="comment">     * the client, for example, to UTF-8. If the character encoding has already</span></span><br><span class="line"><span class="comment">     * been set by container default, ServletContext default,</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@link</span> #setContentType&#125; or &#123;<span class="doctag">@link</span> #setLocale&#125;, this method overrides it.</span></span><br><span class="line"><span class="comment">     * Calling &#123;<span class="doctag">@link</span> #setContentType&#125; with the &lt;code&gt;String&lt;/code&gt; of</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;text/html&lt;/code&gt; and calling this method with the</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;String&lt;/code&gt; of &lt;code&gt;UTF-8&lt;/code&gt; is equivalent with calling</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;setContentType&lt;/code&gt; with the &lt;code&gt;String&lt;/code&gt; of</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;text/html; charset=UTF-8&lt;/code&gt;.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * This method can be called repeatedly to change the character encoding.</span></span><br><span class="line"><span class="comment">     * This method has no effect if it is called after &lt;code&gt;getWriter&lt;/code&gt;</span></span><br><span class="line"><span class="comment">     * has been called or after the response has been committed.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * Containers must communicate the character encoding used for the servlet</span></span><br><span class="line"><span class="comment">     * response&#x27;s writer to the client if the protocol provides a way for doing</span></span><br><span class="line"><span class="comment">     * so. In the case of HTTP, the character encoding is communicated as part</span></span><br><span class="line"><span class="comment">     * of the &lt;code&gt;Content-Type&lt;/code&gt; header for text media types. Note that</span></span><br><span class="line"><span class="comment">     * the character encoding cannot be communicated via HTTP headers if the</span></span><br><span class="line"><span class="comment">     * servlet does not specify a content type; however, it is still used to</span></span><br><span class="line"><span class="comment">     * encode text written via the servlet response&#x27;s writer.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> charset</span></span><br><span class="line"><span class="comment">     *            a String specifying only the character set defined by IANA</span></span><br><span class="line"><span class="comment">     *            Character Sets</span></span><br><span class="line"><span class="comment">     *            (http://www.iana.org/assignments/character-sets)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setContentType #setLocale</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@since</span> 2.4</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setCharacterEncoding</span><span class="params">(String charset)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sets the length of the content body in the response In HTTP servlets,</span></span><br><span class="line"><span class="comment">     * this method sets the HTTP Content-Length header.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> len</span></span><br><span class="line"><span class="comment">     *            an integer specifying the length of the content being returned</span></span><br><span class="line"><span class="comment">     *            to the client; sets the Content-Length header</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setContentLength</span><span class="params">(<span class="type">int</span> len)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sets the length of the content body in the response In HTTP servlets,</span></span><br><span class="line"><span class="comment">     * this method sets the HTTP Content-Length header.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> length</span></span><br><span class="line"><span class="comment">     *            an integer specifying the length of the content being returned</span></span><br><span class="line"><span class="comment">     *            to the client; sets the Content-Length header</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@since</span> Servlet 3.1</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setContentLengthLong</span><span class="params">(<span class="type">long</span> length)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sets the content type of the response being sent to the client, if the</span></span><br><span class="line"><span class="comment">     * response has not been committed yet. The given content type may include a</span></span><br><span class="line"><span class="comment">     * character encoding specification, for example,</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;text/html;charset=UTF-8&lt;/code&gt;. The response&#x27;s character encoding</span></span><br><span class="line"><span class="comment">     * is only set from the given content type if this method is called before</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;getWriter&lt;/code&gt; is called.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * This method may be called repeatedly to change content type and character</span></span><br><span class="line"><span class="comment">     * encoding. This method has no effect if called after the response has been</span></span><br><span class="line"><span class="comment">     * committed. It does not set the response&#x27;s character encoding if it is</span></span><br><span class="line"><span class="comment">     * called after &lt;code&gt;getWriter&lt;/code&gt; has been called or after the response</span></span><br><span class="line"><span class="comment">     * has been committed.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * Containers must communicate the content type and the character encoding</span></span><br><span class="line"><span class="comment">     * used for the servlet response&#x27;s writer to the client if the protocol</span></span><br><span class="line"><span class="comment">     * provides a way for doing so. In the case of HTTP, the</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;Content-Type&lt;/code&gt; header is used.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> type</span></span><br><span class="line"><span class="comment">     *            a &lt;code&gt;String&lt;/code&gt; specifying the MIME type of the content</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setLocale</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setCharacterEncoding</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #getOutputStream</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #getWriter</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setContentType</span><span class="params">(String type)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sets the preferred buffer size for the body of the response. The servlet</span></span><br><span class="line"><span class="comment">     * container will use a buffer at least as large as the size requested. The</span></span><br><span class="line"><span class="comment">     * actual buffer size used can be found using &lt;code&gt;getBufferSize&lt;/code&gt;.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * A larger buffer allows more content to be written before anything is</span></span><br><span class="line"><span class="comment">     * actually sent, thus providing the servlet with more time to set</span></span><br><span class="line"><span class="comment">     * appropriate status codes and headers. A smaller buffer decreases server</span></span><br><span class="line"><span class="comment">     * memory load and allows the client to start receiving data more quickly.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * This method must be called before any response body content is written;</span></span><br><span class="line"><span class="comment">     * if content has been written or the response object has been committed,</span></span><br><span class="line"><span class="comment">     * this method throws an &lt;code&gt;IllegalStateException&lt;/code&gt;.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> size</span></span><br><span class="line"><span class="comment">     *            the preferred buffer size</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@exception</span> IllegalStateException</span></span><br><span class="line"><span class="comment">     *                if this method is called after content has been written</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #getBufferSize</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #flushBuffer</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #isCommitted</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #reset</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setBufferSize</span><span class="params">(<span class="type">int</span> size)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns the actual buffer size used for the response. If no buffering is</span></span><br><span class="line"><span class="comment">     * used, this method returns 0.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> the actual buffer size used</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setBufferSize</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #flushBuffer</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #isCommitted</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #reset</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getBufferSize</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Forces any content in the buffer to be written to the client. A call to</span></span><br><span class="line"><span class="comment">     * this method automatically commits the response, meaning the status code</span></span><br><span class="line"><span class="comment">     * and headers will be written.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException if an I/O occurs during the flushing of the response</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setBufferSize</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #getBufferSize</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #isCommitted</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #reset</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flushBuffer</span><span class="params">()</span> <span class="keyword">throws</span> IOException;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Clears the content of the underlying buffer in the response without</span></span><br><span class="line"><span class="comment">     * clearing headers or status code. If the response has been committed, this</span></span><br><span class="line"><span class="comment">     * method throws an &lt;code&gt;IllegalStateException&lt;/code&gt;.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setBufferSize</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #getBufferSize</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #isCommitted</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #reset</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@since</span> 2.3</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">resetBuffer</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns a boolean indicating if the response has been committed. A</span></span><br><span class="line"><span class="comment">     * committed response has already had its status code and headers written.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> a boolean indicating if the response has been committed</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setBufferSize</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #getBufferSize</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #flushBuffer</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #reset</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">isCommitted</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Clears any data that exists in the buffer as well as the status code and</span></span><br><span class="line"><span class="comment">     * headers. If the response has been committed, this method throws an</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;IllegalStateException&lt;/code&gt;.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@exception</span> IllegalStateException</span></span><br><span class="line"><span class="comment">     *                if the response has already been committed</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setBufferSize</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #getBufferSize</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #flushBuffer</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #isCommitted</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reset</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sets the locale of the response, if the response has not been committed</span></span><br><span class="line"><span class="comment">     * yet. It also sets the response&#x27;s character encoding appropriately for the</span></span><br><span class="line"><span class="comment">     * locale, if the character encoding has not been explicitly set using</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@link</span> #setContentType&#125; or &#123;<span class="doctag">@link</span> #setCharacterEncoding&#125;,</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;getWriter&lt;/code&gt; hasn&#x27;t been called yet, and the response hasn&#x27;t</span></span><br><span class="line"><span class="comment">     * been committed yet. If the deployment descriptor contains a</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;locale-encoding-mapping-list&lt;/code&gt; element, and that element</span></span><br><span class="line"><span class="comment">     * provides a mapping for the given locale, that mapping is used. Otherwise,</span></span><br><span class="line"><span class="comment">     * the mapping from locale to character encoding is container dependent.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * This method may be called repeatedly to change locale and character</span></span><br><span class="line"><span class="comment">     * encoding. The method has no effect if called after the response has been</span></span><br><span class="line"><span class="comment">     * committed. It does not set the response&#x27;s character encoding if it is</span></span><br><span class="line"><span class="comment">     * called after &#123;<span class="doctag">@link</span> #setContentType&#125; has been called with a charset</span></span><br><span class="line"><span class="comment">     * specification, after &#123;<span class="doctag">@link</span> #setCharacterEncoding&#125; has been called, after</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;getWriter&lt;/code&gt; has been called, or after the response has been</span></span><br><span class="line"><span class="comment">     * committed.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * Containers must communicate the locale and the character encoding used</span></span><br><span class="line"><span class="comment">     * for the servlet response&#x27;s writer to the client if the protocol provides</span></span><br><span class="line"><span class="comment">     * a way for doing so. In the case of HTTP, the locale is communicated via</span></span><br><span class="line"><span class="comment">     * the &lt;code&gt;Content-Language&lt;/code&gt; header, the character encoding as part</span></span><br><span class="line"><span class="comment">     * of the &lt;code&gt;Content-Type&lt;/code&gt; header for text media types. Note that</span></span><br><span class="line"><span class="comment">     * the character encoding cannot be communicated via HTTP headers if the</span></span><br><span class="line"><span class="comment">     * servlet does not specify a content type; however, it is still used to</span></span><br><span class="line"><span class="comment">     * encode text written via the servlet response&#x27;s writer.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> loc</span></span><br><span class="line"><span class="comment">     *            the locale of the response</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #getLocale</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setContentType</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setCharacterEncoding</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setLocale</span><span class="params">(Locale loc)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns the locale specified for this response using the</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@link</span> #setLocale&#125; method. Calls made to &lt;code&gt;setLocale&lt;/code&gt; after the</span></span><br><span class="line"><span class="comment">     * response is committed have no effect.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> The locale specified for this response using the</span></span><br><span class="line"><span class="comment">     *          &#123;<span class="doctag">@link</span> #setLocale&#125; method. If no locale has been specified, the</span></span><br><span class="line"><span class="comment">     *          container&#x27;s default locale is returned.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #setLocale</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> Locale <span class="title function_">getLocale</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>设置响应体文本</strong>：</p>
<ol>
<li>先通过<code>resp</code>对象获取<code>输出流对象</code></li>
<li>调用<code>输出流对象</code>中的<code>write(String s)</code>方法可以将字符串<code>s</code>添加到<code>Response缓冲区</code>中，之后TomCat会将<code>Response缓冲区</code>中的内容封装到响应消息的响应体中返回给客户端浏览器。</li>
</ol>
<ol>
<li><p>通过<code>resp</code>对象获取<em>输出流对象</em></p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.获取输出流对象的方法： </span></span><br><span class="line">PrintWriter <span class="title function_">getWriter</span><span class="params">()</span>;	<span class="comment">// 获取字符输出流</span></span><br><span class="line">ServletOutputStream <span class="title function_">getOutputStream</span><span class="params">()</span>;	<span class="comment">// 获取字节输出流</span></span><br></pre></td></tr></table></figure></li>
<li><p>调用<em>输出流对象</em>中的<code>write()</code>方法</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2.使用输出流，将数据输出到客户端浏览器</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h5 id="两个输出流的说明"><a href="#两个输出流的说明" class="headerlink" title="两个输出流的说明"></a>两个输出流的说明</h5><table>
<thead>
<tr>
<th align="center"></th>
<th align="center">获取</th>
<th align="center">功能</th>
</tr>
</thead>
<tbody><tr>
<td align="center">字节流</td>
<td align="center"><code>getOutputStream()</code></td>
<td align="center">常用于下载（传递二进制数据）</td>
</tr>
<tr>
<td align="center">字符流</td>
<td align="center"><code>getWriter()</code></td>
<td align="center">常用于回传字符串</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
<blockquote>
<p>Tip：两个流同时只能使用一个，否则访问服务器时会出现500以上的错误。</p>
</blockquote>
<h4 id="4、如何解决客户端输出到浏览器上的中文数据乱码问题？"><a href="#4、如何解决客户端输出到浏览器上的中文数据乱码问题？" class="headerlink" title="4、如何解决客户端输出到浏览器上的中文数据乱码问题？"></a>4、如何解决客户端输出到浏览器上的中文数据乱码问题？</h4><ol>
<li><strong>乱码产生的原因</strong></li>
</ol>
<blockquote>
<p>乱码问题产生的原因：编码&amp;解码使用的标准不一致。</p>
<ul>
<li>Response缓冲区的默认编码是<code>ISO-8859-1</code>。</li>
<li>如果服务器的响应消息中不告诉用户浏览器使用什么方式解码，那么用户浏览器就会使用默认的解码方式解码【默认解码方式与用户系统一致，比如安装的是中文系统的Windows，那么默认使用GBK编码】</li>
<li>那么拿着ISO-8859-1编码的数据，使用GBK解码，显然会出现乱码的现象。</li>
</ul>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Demo05_Response</span> <span class="keyword">extends</span> <span class="title class_">HttpServlet</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">doGet</span><span class="params">(HttpServletRequest req, HttpServletResponse resp)</span> <span class="keyword">throws</span> ServletException, IOException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取输出流对象</span></span><br><span class="line">        <span class="type">PrintWriter</span> <span class="variable">writer</span> <span class="operator">=</span> resp.getWriter();</span><br><span class="line">        <span class="comment">// 2.调用输出流对象中的write()方法，将字符串写到Response缓冲区中(并使用ISO编码编译)</span></span><br><span class="line">        writer.write(<span class="string">&quot;你好，世界&quot;</span>);</span><br><span class="line">        <span class="comment">// 3.出现问题，输出到浏览器页面的中文内容无法被解析</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 4.查看resp对象的编码类型 【ISO-8859-1】</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">encoding</span> <span class="operator">=</span> resp.getCharacterEncoding();</span><br><span class="line">        System.out.println(encoding);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 5.原因：浏览器默认使用的GBK编码，TomCat服务器通过resp对象传给浏览器的数据默认是ISO-8859-1编码，是浏览器无法解析的数据类型，所以会出现乱码。</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<ol start="2">
<li><p><strong>解决方案</strong>（3个方法）：</p>
<ul>
<li><p>1)、在Response获取输出流对象之前，设置该流使用的编码方式与浏览器默认的解码方式一致</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//方法1:</span></span><br><span class="line"><span class="meta">@WebServlet(&quot;/demo03response&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Demo05_Response</span> <span class="keyword">extends</span> <span class="title class_">HttpServlet</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">doPost</span><span class="params">(HttpServletRequest req, HttpServletResponse resp)</span> <span class="keyword">throws</span> ServletException, IOException &#123;</span><br><span class="line">        <span class="comment">// 0.设置TomCat的字符输出流的编码方式与系统相同（GBK）【耦合度高】</span></span><br><span class="line">        resp.setCharacterEncoding(<span class="string">&quot;gbk&quot;</span>);</span><br><span class="line">        <span class="comment">// 1..获取字节/字符输出流对象【通过TomCat创建的resp对象来获取】</span></span><br><span class="line">        <span class="type">PrintWriter</span> <span class="variable">pw</span> <span class="operator">=</span> resp.getWriter();</span><br><span class="line">        <span class="comment">// 2.使用输出流，把数据输出到客户端浏览器</span></span><br><span class="line">        pw.write(<span class="string">&quot;&lt;h1&gt;hello 世界&lt;/h1&gt;&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
<li><p>2)、服务器在Response消息的响应头Headers中告诉客户端浏览器我使用的编码方式是什么，让浏览器选择对应的解码方式。[我们已知了计算机系统的默认编码方式为GBK，但是如果把这个程序放到默认以UTF-8编码的计算机上以方法1运行仍然会报错，所以我们需要把    设置输出流编码方式  &amp;  告诉浏览器输出流的编码方式  结合起来使用。]</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@WebServlet(&quot;/demo03response&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Demo03_Response</span> <span class="keyword">extends</span> <span class="title class_">HttpServlet</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">doPost</span><span class="params">(HttpServletRequest req, HttpServletResponse resp)</span> <span class="keyword">throws</span> ServletException, IOException &#123;</span><br><span class="line">        <span class="comment">// 0. 首先要设置服务器中字符输出流的编码格式</span></span><br><span class="line">        resp.setCharacterEncoding(<span class="string">&quot;utf-8&quot;</span>);</span><br><span class="line">	   	<span class="comment">// 1.服务器告诉浏览器我使用哪种编码，让浏览器按照对应的码表解码	</span></span><br><span class="line">        resp.setHeader(<span class="string">&quot;content-type&quot;</span>, <span class="string">&quot;text/html;charset=utf-8&quot;</span>);</span><br><span class="line">        <span class="comment">// 2.获取字节/字符输出流【通过TomCat创建的resp对象来获取】</span></span><br><span class="line">        <span class="type">PrintWriter</span> <span class="variable">pw</span> <span class="operator">=</span> resp.getWriter();</span><br><span class="line">        <span class="comment">// 2.使用输出流，把数据输出到客户端浏览器</span></span><br><span class="line">        pw.write(<span class="string">&quot;&lt;h1&gt;hello 世界&lt;/h1&gt;&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>3)、resoponse对象中的<code>setContentType()</code>方法不仅设置了服务器使用utf-8字符集，还设置了响应头中的<code>ContentType</code>的值为<code>&quot;text/html;utf-8&quot;</code>。简化了方法2的书写</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@WebServlet(&quot;/demo03response&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Demo03_Response</span> <span class="keyword">extends</span> <span class="title class_">HttpServlet</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">doPost</span><span class="params">(HttpServletRequest req, HttpServletResponse resp)</span> <span class="keyword">throws</span> ServletException, IOException &#123;</span><br><span class="line">       </span><br><span class="line">	   	<span class="comment">// 1.同时设置服务器可客户端使用utf-8字符集	</span></span><br><span class="line">        resp.setContentType(<span class="string">&quot;text/html;utf-8&quot;</span>);</span><br><span class="line">        <span class="comment">// 2.获取字节/字符输出流【通过TomCat创建的resp对象来获取】</span></span><br><span class="line">        <span class="type">PrintWriter</span> <span class="variable">pw</span> <span class="operator">=</span> resp.getWriter();</span><br><span class="line">        <span class="comment">// 3.使用输出流，把数据输出到客户端浏览器</span></span><br><span class="line">        pw.write(<span class="string">&quot;&lt;h1&gt;hello 世界&lt;/h1&gt;&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Tip：此方法一定要在获取流对象之前调用才有效</p>
</blockquote>
</li>
</ul>
</li>
</ol>
<h3 id="6-2-2-重定向-vs-转发"><a href="#6-2-2-重定向-vs-转发" class="headerlink" title="6.2.2    重定向 vs. 转发"></a>6.2.2    重定向 vs. 转发</h3><p><img src="/2021/11/25/HttpServletRequest%E5%92%8CHttpServletResponse/%E9%87%8D%E5%AE%9A%E5%90%91-1597652154015.bmp"></p>
<p><img src="/2021/11/25/HttpServletRequest%E5%92%8CHttpServletResponse/image-20200519183919703-1597652154016.png" alt="image-20200519183919703"></p>
<ul>
<li><p>重定向：<code>redirect</code></p>
<ol>
<li><p>重定向是一种资源跳转的方式。</p>
<ul>
<li>用户希望访问一些数据，先通过URL找到了A资源，A告诉用户我满足不了你的需求，但是我知道B能满足你的需求，所以A的Response中包含了302的响应状态码以及B的URL。告诉用户的浏览器去重新访问B资源。其中用户的浏览器对A和B都发送了Request请求，这是2次请求。</li>
</ul>
</li>
<li><p>重定向的特点：</p>
<ol>
<li>地址栏发生变化。</li>
<li>重定向可以访问其他站点（服务器）的资源。</li>
<li>重定向是两次请求。</li>
<li>不能共享Request域中的数据。</li>
<li>不能访问WEB-INF目录下的资源</li>
</ol>
</li>
<li><p>请求重定向的2种方法：</p>
<ul>
<li>方法1：<code>resp.setStatus(302)</code> + <code>resp.setHeader(&quot;location&quot;,&quot;/D4/demo02response&quot;)</code></li>
</ul>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@WebServlet(&quot;/demo01response&quot;)</span>	<span class="comment">// 当前资源路径</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Demo01_Response</span> <span class="keyword">extends</span> <span class="title class_">HttpServlet</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">doGet</span><span class="params">(HttpServletRequest req, HttpServletResponse resp)</span> <span class="keyword">throws</span> ServletException, IOException &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;demo01重定向到demo02&quot;</span>);		<span class="comment">// 本路径的资源</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        	重定向</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="comment">// 1. 给客户端返回302状态码。</span></span><br><span class="line">        resp.setStatus(<span class="number">302</span>);</span><br><span class="line">        <span class="comment">// 2. 给客户端返回跳转的地址【键值对】保存在响应头中返回。</span></span><br><span class="line">        resp.setHeader(<span class="string">&quot;location&quot;</span>,<span class="string">&quot;/D4/demo02response&quot;</span>);	<span class="comment">// 第一个斜杠表示绝对地址（给浏览器解析）</span></span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>方法2(推荐使用)：<code> resp.sendRedirect(&quot;http://www.baidu.com&quot;)</code></li>
</ul>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@WebServlet(&quot;/demo01response&quot;)</span>	<span class="comment">// 当前资源路径</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Demo01_Response</span> <span class="keyword">extends</span> <span class="title class_">HttpServlet</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">doGet</span><span class="params">(HttpServletRequest req, HttpServletResponse resp)</span> <span class="keyword">throws</span> ServletException, IOException &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;demo01重定向到demo02&quot;</span>);		<span class="comment">// 本路径的资源	</span></span><br><span class="line">         <span class="comment">/*</span></span><br><span class="line"><span class="comment">        	重定向</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">		<span class="comment">// 1.动态的获取[绝对]工程路径</span></span><br><span class="line">     <span class="type">String</span> <span class="variable">contextPath</span> <span class="operator">=</span> req.getContextPath(	);</span><br><span class="line">        <span class="comment">// 2.调用重定向方法（该方法实现了返回302状态码 &amp; 返回跳转的地址）</span></span><br><span class="line">     <span class="comment">// resp.sendRedirect(contextPath+&quot;/demo02response&quot;);</span></span><br><span class="line">        resp.sendRedirect(<span class="string">&quot;http://www.baidu.com&quot;</span>);     <span class="comment">// 重定向可以访问其它服务器上的资源</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
</li>
<li><p>转发：forward</p>
<ol>
<li>转发时地址栏路径不变</li>
<li>转发只能访问当前服务器下的资源</li>
<li>转发是一次请求，可以共享数据。</li>
</ol>
</li>
</ul>
<hr>
<ul>
<li>访问缓存（304）<ol>
<li>用户浏览器会把用户机暂时访问过的部分资源缓存到客户机中，未清空缓存的情况下，用户再次访问该资源时，客户端会自动检测到这些缓存的存在，就会给用户浏览器返回304响应状态码，让用户使用缓存的资源。从而节省了传输时间，减轻了服务器的压力。</li>
</ol>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/22/Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/11/22/Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/" class="post-title-link" itemprop="url">Hadoop完全分布式集群搭建</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-11-22 21:21:29" itemprop="dateCreated datePublished" datetime="2021-11-22T21:21:29+08:00">2021-11-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-02-16 10:36:51" itemprop="dateModified" datetime="2022-02-16T10:36:51+08:00">2022-02-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="完全分布式集群的搭建（重点）"><a href="#完全分布式集群的搭建（重点）" class="headerlink" title="完全分布式集群的搭建（重点）"></a>完全分布式集群的搭建（重点）</h1><p><strong>是什么？</strong>Hadoop 中各个组件（HDFS、Yarn等）的各个进程（HDFS 的 NameNode 和 DataNode；Yarn 的ResourceManager 和 DataManager 等）分布在多台机器上运行。</p>
<h2 id="进程规划"><a href="#进程规划" class="headerlink" title="进程规划"></a>进程规划</h2><p>进程规划原则：</p>
<ol>
<li> 核心进程尽量分散。</li>
<li> 同质进程尽量分散。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">解析：</span><br><span class="line">    1、核心进程尽量分散：</span><br><span class="line">        在 Hadoop 集群中，HDFS 中的 NM 和 YARN 中的 RM 都属于核心进程，一个集群中只能有一个。    </span><br><span class="line">        对于这些进程来说，一旦集群中的某个节点上的机器突然宕机，整个集群就会立即停止运转，所以我们在进程规划时尽量会把 NM 和 RM 分配到不同的节点上，这样能有效的避免单点故障。</span><br><span class="line">        </span><br><span class="line">    2、同质进程尽量分散：</span><br><span class="line">        所谓同质进程，就是指 运行某两个进程需求的相同或类似的资源。</span><br><span class="line">        比如 NN 和 2NN ，在运行时都需要分配大量的内存空间，所以我们一般把这两个进程分配到不同的节点上去，这样可以提高内存资源的利用率。</span><br></pre></td></tr></table></figure>



<h2 id="执行群起"><a href="#执行群起" class="headerlink" title="执行群起"></a>执行群起</h2><ul>
<li>  <code>start-dfs.sh</code> 命令可以在集群中的任意一台机器上执行。</li>
<li>  <code>start-yarn.sh</code> 如果在没有配置 RM 的机器上执行，那么不会启动 RM，只能启动 NM！</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tip：</span><br><span class="line">    1、建议所有的群起脚本都在 RM 所在的机器上执行！</span><br><span class="line">    2、只需要配置 RM 所在的机器到集群中其它机器的 SSH 免密登录即可。</span><br></pre></td></tr></table></figure>

<ul>
<li>  在执行群起脚本时，默认读取当前机器中的 <code>$HADOOP_HOME/etc/hadoop/slaves</code> 文件，所以要在该文件中配置当前集群中所有的主机IP地址。</li>
</ul>
<h2 id="集群的时间同步"><a href="#集群的时间同步" class="headerlink" title="集群的时间同步"></a>集群的时间同步</h2><blockquote>
<p>  一个集群中，每台机器的时间必须保证是同步的！</p>
</blockquote>
<hr>
<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><h2 id="scp"><a href="#scp" class="headerlink" title="scp"></a>scp</h2><p><strong>scp（secure copy）：安全拷贝</strong></p>
<p><strong>作用：</strong></p>
<ul>
<li>  <code>scp</code> 可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>  <code>scp</code> 命令执行的是全量复制，每次复制时都会把一份完整的源文件重新复制拷贝到目的文件夹中。如果经常需要执行增量复制的话，<code>scp</code> 命令就不太合适了。</li>
<li>  正常情况下，一个文件第一次从A机器拷贝到B机器需要执行一次全量复制，但是当A机器中该文件数据发生变化后，下一次复制只需执行一次增量复制就可以了。但是如果使用 <code>scp</code> 命令的话，每次执行的都是全量复制。</li>
</ul>
<p><strong>基本语法</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r 源文件属主A@主机名1:path1    目标文件属主B@主机名2:path2</span><br></pre></td></tr></table></figure>

<ul>
<li>  <strong>作用：</strong>在主机1上，使用 A 用户读取 <code>path1</code> 的文件，再使用用户B登录到主机2上，在主机2的 <code>path2</code> 路径执行写入操作。</li>
<li>  <code>-r</code>：递归，如果要拷贝的是一个目录，且目录还有其它文件，就一定要使用递归。</li>
<li><strong>要求：</strong><ol>
<li> 主机名1上的用户名A对 <code>path1</code> 有读权限</li>
<li> 主机名2上的用户名B对 <code>path2</code> 有写权限</li>
</ol>
</li>
<li>  <strong>小技巧</strong>：如果从本机执行读取或写入，<code>用户名B@主机名2:</code> 可以省略！</li>
</ul>
<p><strong>测试</strong></p>
<ol>
<li><p> 在 hadoop102 的 <code>/opt/module/</code> 目录下有一个 testUpload 文件，其属主是用户 <code>lvnengdong</code></p>
</li>
<li><p>现在在 hadoop103 上执行命令，将 hadoop102 上的文件 <code>/opt/module/testUpload</code> 拷贝到 hadoop103 的 <code>/opt/module/</code> 目录下，执行的脚本为：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop103 etc]$ scp -r lvnengdong@hadoop102:/opt/module/testUpload /opt/module/</span><br><span class="line">testUpload             									100%   19    12.9KB/s   00:00 </span><br></pre></td></tr></table></figure></li>
<li><p> 在 hadoop103 上查看是否拷贝成功。</p>
</li>
</ol>
<hr>
<h2 id="rsync"><a href="#rsync" class="headerlink" title="rsync"></a>rsync</h2><p><strong>rsync（remote sync）    远程同步</strong></p>
<p><strong>作用 &amp; 特点：</strong></p>
<ul>
<li>  <code>rsync</code> 也用于执行多个服务器间数据的拷贝，但是该命令采用的是<strong>增量复制</strong>策略。</li>
<li>  增量复制的原理是对比文件的修改时间，只同步修改时间不一致的文件。</li>
</ul>
<p><strong>基本语法：</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync -rvlt 源文件属主A@主机名1:path1    目标文件属主B@主机名2:path2</span><br></pre></td></tr></table></figure>

<ul>
<li>  <strong>作用：</strong>在主机1上，使用 A 用户读取 <code>path1</code> 的文件，再使用用户B登录到主机2上，在主机2的 <code>path2</code> 路径执行写入操作。但是由于 <code>rsync</code> 命令只支持将本机的文件或目录同步到其它机器上，所以 <code>源文件属主A@主机名1:</code> 就可以省略，一般命令写为：</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync -rvlt path1  目标文件属主B@主机名2:path2</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>选项参数</strong></p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-r:	 递归，复制目录时使用</span><br><span class="line">-v： 显示复制的过程</span><br><span class="line">-l:  即使是软连接，也会将软连接同步</span><br><span class="line">-t:  基于文件的修改时间进行对比，只同步发生变化的文件</span><br></pre></td></tr></table></figure></li>
<li><p><strong>注意事项：</strong></p>
<ol>
<li> 只能将本机的文件同步到其他机器！</li>
<li> 如果 <code>path1</code> 是个目录，目录以 <code>/</code> 结尾，表示只会同步目录中的内容，不会同步目录本身；如果目录不以 <code>/</code> 结尾，不进会同步目录中的内容，也会同步目录本身！</li>
</ol>
</li>
</ul>
<hr>
<h1 id="SSH-免密登录"><a href="#SSH-免密登录" class="headerlink" title="SSH 免密登录"></a>SSH 免密登录</h1><p>举例：  A机器的a用户，希望在A机器上，使用b用户的身份登录到B机器！（A机器上的A用户，使用b用户远程连接到B机器）</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line"></span><br><span class="line">ssh-keygen  生成秘钥的指令 </span><br><span class="line">-t rsa	-t表示指定生成的算法  rsa 是一种算法</span><br></pre></td></tr></table></figure>





<hr>
<h1 id="编写数据同步脚本"><a href="#编写数据同步脚本" class="headerlink" title="编写数据同步脚本"></a>编写数据同步脚本</h1><ol>
<li><p> <strong>需求：</strong>循环复制文件到所有节点的相同目录下</p>
</li>
<li><p><strong>分析</strong>：</p>
<ul>
<li><p><code>rsync</code> 命令原始拷贝：</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync  -rvl     /opt/module  		 root@hadoop103:/opt/</span><br></pre></td></tr></table></figure></li>
<li><p>期望脚本：</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync 要同步的文件名称</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p> <strong>说明：</strong>在 <code>/home/&#123;用户名&#125;/bin</code> 这个目录下存放的脚本，用户可以在系统任何地方直接执行。</p>
</li>
<li><p><strong>脚本实现</strong></p>
<ol>
<li><p>在 <code>/home/&#123;用户名&#125;</code> 目录下创建 <code>bin</code> 目录，并在 <code>bin</code> 目录下创建脚本文件 <code>xsync</code> ，脚本文件内容如下：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=<span class="variable">$#</span></span><br><span class="line"><span class="keyword">if</span>((pcount==0)); <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> no args;</span><br><span class="line"><span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2 获取文件名称</span></span><br><span class="line">p1=<span class="variable">$1</span></span><br><span class="line">fname=`<span class="built_in">basename</span> <span class="variable">$p1</span>`</span><br><span class="line"><span class="built_in">echo</span> fname=<span class="variable">$fname</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`<span class="built_in">cd</span> -P $(<span class="built_in">dirname</span> <span class="variable">$p1</span>); <span class="built_in">pwd</span>`</span><br><span class="line"><span class="built_in">echo</span> pdir=<span class="variable">$pdir</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4 获取当前用户名称</span></span><br><span class="line">user=`<span class="built_in">whoami</span>`</span><br><span class="line"></span><br><span class="line"><span class="comment">#5 循环</span></span><br><span class="line"><span class="keyword">for</span>((host=103; host&lt;105; host++)); <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> ------------------- hadoop<span class="variable">$host</span> --------------</span><br><span class="line">        rsync -rvl <span class="variable">$pdir</span>/<span class="variable">$fname</span> <span class="variable">$user</span>@hadoop<span class="variable">$host</span>:<span class="variable">$pdir</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></li>
<li><p>修改脚本文件 <code>xsync</code> 具有执行权限</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop103 ~]$ <span class="built_in">chmod</span> 777 xsync</span><br></pre></td></tr></table></figure></li>
<li><p>调用脚本形式：<code>xsync 文件名称</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop103 ~]$ xsync /home/lvnengdong/bin</span><br><span class="line"><span class="comment"># 将 /bin 目录及其子目录或文件分发到集群中的其它机器上</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p> <strong>注意：</strong>如果将 <code>xsync</code> 放到 <code>/home/atguigu/bin</code> 目录下仍然不能实现全局使用，可以将 <code>xsync</code> 移动到 <code>/usr/local/bin</code> 目录下。</p>
</li>
</ol>
<hr>
<h1 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h1><h2 id="1、集群部署规划"><a href="#1、集群部署规划" class="headerlink" title="1、集群部署规划"></a>1、集群部署规划</h2><table>
<thead>
<tr>
<th align="left"></th>
<th align="left">hadoop102</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td align="left">HDFS</td>
<td align="left">NameNode<br>DataNode</td>
<td>DataNode</td>
<td>SecondaryNameNode<br>DataNode</td>
</tr>
<tr>
<td align="left">YARN</td>
<td align="left">NodeManager<br>Jobhistory</td>
<td>ResourceManager<br>NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<hr>
<h2 id="2、配置集群"><a href="#2、配置集群" class="headerlink" title="2、配置集群"></a>2、配置集群</h2><h3 id="核心配置文件"><a href="#核心配置文件" class="headerlink" title="核心配置文件"></a>核心配置文件</h3><p><strong>core-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="HDFS配置文件"><a href="#HDFS配置文件" class="headerlink" title="HDFS配置文件"></a>HDFS配置文件</h3><p><strong>hdfs-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 副本数量（默认配置就是3.所以可以无需在自定义的配置文件中重写）--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop辅助节点（2nn）的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="YARN配置文件"><a href="#YARN配置文件" class="headerlink" title="YARN配置文件"></a>YARN配置文件</h3><p><strong>yarn-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>





<h3 id="MapReduce配置文件"><a href="#MapReduce配置文件" class="headerlink" title="MapReduce配置文件"></a>MapReduce配置文件</h3><p><strong>mapred-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定mr运行在yarn上 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="在集群上分发配置好的Hadoop配置文件"><a href="#在集群上分发配置好的Hadoop配置文件" class="headerlink" title="在集群上分发配置好的Hadoop配置文件"></a>在集群上分发配置好的Hadoop配置文件</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync /opt/module/hadoop-2.7.2/</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="在集群中的每一台机器上都配置JAVA-HOME和HADOOP-HOME-环境变量"><a href="#在集群中的每一台机器上都配置JAVA-HOME和HADOOP-HOME-环境变量" class="headerlink" title="在集群中的每一台机器上都配置JAVA_HOME和HADOOP_HOME 环境变量"></a>在集群中的每一台机器上都配置JAVA_HOME和HADOOP_HOME 环境变量</h3><hr>
<h2 id="3、集群单点启动"><a href="#3、集群单点启动" class="headerlink" title="3、集群单点启动"></a>3、集群单点启动</h2><h2 id="4、集群群起"><a href="#4、集群群起" class="headerlink" title="4、集群群起"></a>4、集群群起</h2><h3 id="配置群起脚本"><a href="#配置群起脚本" class="headerlink" title="配置群起脚本"></a>配置群起脚本</h3><p><strong>作用：</strong>在集群中的所有机器上批量执行同一条命令</p>
<ol>
<li><p>在当前用户的家目录<code>~</code>下的 <code>bin</code> 目录下创建一个 <code>xcall</code> 脚本文件，文件内容如下：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 在集群中的所有机器上批量执行同一条命令</span></span><br><span class="line"><span class="keyword">if</span>((<span class="variable">$#</span>==0))</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">	<span class="built_in">echo</span> 请输入您要操作的命令！</span><br><span class="line">	<span class="built_in">exit</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 要执行的命令是$*</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环执行此命令</span></span><br><span class="line"><span class="keyword">for</span>((i=102;i&lt;=104;i++))</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">	<span class="built_in">echo</span> ---------------------hadoop<span class="variable">$i</span>-----------------</span><br><span class="line">	ssh hadoop<span class="variable">$i</span> $*</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></li>
<li><p>修改脚本文件 <code>xsync</code> 具有执行权限</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop103 ~]$ <span class="built_in">chmod</span> 777 xsync</span><br></pre></td></tr></table></figure></li>
<li><p>测试脚本执行效果</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 bin]$ xcall jps</span><br><span class="line">要执行的命令是jps</span><br><span class="line">---------------------hadoop102-----------------</span><br><span class="line">34660 Jps</span><br><span class="line">---------------------hadoop103-----------------</span><br><span class="line">7595 Jps</span><br><span class="line">---------------------hadoop104-----------------</span><br><span class="line">26590 Jps</span><br></pre></td></tr></table></figure>

</li>
<li><p>将该脚本文件分发到集群的所有节点上</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 bin]$ xsync ./xcall </span><br><span class="line">==================== hadoop102 ====================</span><br><span class="line">sending incremental file list</span><br><span class="line"></span><br><span class="line">sent 68 bytes  received 12 bytes  53.33 bytes/sec</span><br><span class="line">total size is 295  speedup is 3.69</span><br><span class="line">==================== hadoop103 ====================</span><br><span class="line">sending incremental file list</span><br><span class="line">xcall</span><br><span class="line"></span><br><span class="line">sent 410 bytes  received 35 bytes  890.00 bytes/sec</span><br><span class="line">total size is 295  speedup is 0.66</span><br><span class="line">==================== hadoop104 ====================</span><br><span class="line">sending incremental file list</span><br><span class="line">xcall</span><br><span class="line"></span><br><span class="line">sent 410 bytes  received 35 bytes  890.00 bytes/sec</span><br><span class="line">total size is 295  speedup is 0.66</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h1 id="启动-Hadoop"><a href="#启动-Hadoop" class="headerlink" title="启动 Hadoop"></a>启动 Hadoop</h1><h2 id="启动-HDFS"><a href="#启动-HDFS" class="headerlink" title="启动 HDFS"></a>启动 HDFS</h2><ol>
<li><p>启动 NameNode</p>
<ul>
<li><p>如果 NameNode 是第一次启动，需要在 NameNode 所在的节点上先执行格式化。</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 bin]$ hadoop namenode -format</span><br></pre></td></tr></table></figure></li>
</ul>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动 namenode</span></span><br><span class="line">[lvnengdong@hadoop102 bin]$ hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure></li>
<li><p>启动 DataNode</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><strong>测试</strong></p>
<ol>
<li><p> 在 hadoop102 上启动 NameNode</p>
</li>
<li><p> 在 hadoop102 上启动 DataNode</p>
</li>
<li><p> 在 hadoop103 上启动 DataNode</p>
</li>
<li><p>在 hadoop104 上启动 DataNode</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 hadoop102 上启动 NameNode</span></span><br><span class="line">[lvnengdong@hadoop102 bin]$ hadoop-daemon.sh start namenode</span><br><span class="line"></span><br><span class="line"><span class="comment"># 群起 DataNode</span></span><br><span class="line">[lvnengdong@hadoop102 bin]$ xcall hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure></li>
<li><p>查看结果</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 bin]$ xcall jps</span><br><span class="line">要执行的命令是jps</span><br><span class="line">---------------------hadoop102-----------------</span><br><span class="line">52104 NameNode</span><br><span class="line">59609 Jps</span><br><span class="line">57549 DataNode</span><br><span class="line">---------------------hadoop103-----------------</span><br><span class="line">33104 Jps</span><br><span class="line">31189 DataNode</span><br><span class="line">---------------------hadoop104-----------------</span><br><span class="line">27345 Jps</span><br><span class="line">27148 DataNode</span><br><span class="line">[lvnengdong@hadoop102 bin]$ </span><br></pre></td></tr></table></figure></li>
<li><p>在 hadoop104 上启动 2nn</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop104 ~]$ hadoop-daemon.sh start secondarynamenode</span><br></pre></td></tr></table></figure></li>
<li><p>通过浏览器<code>http://hadoop102:50070</code>查看，三个 DataNode 都已经成功启动了</p>
<p> <img src="/2021/11/22/Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211123121936190.png" alt="image-20211123121936190"></p>
</li>
</ol>
<hr>
<h2 id="启动-Yarn"><a href="#启动-Yarn" class="headerlink" title="启动 Yarn"></a>启动 Yarn</h2><p>Yarn 由 ResourceManager 和 DataManager 组成。我们规划在 Hadoop103 上启动 ResourceManager，在每个节点上都启动一个 DataManager。 </p>
<ol>
<li><p>在 Hadoop103 启动一个 ResourceManager</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop103 ~]$ yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure>

</li>
<li><p>在每台服务器都启动一个DataManager </p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop103 ~]$ xcall yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure></li>
<li><p>查看</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop103 ~]$ xcall jps</span><br><span class="line">要执行的命令是jps</span><br><span class="line">---------------------hadoop102-----------------</span><br><span class="line">88133 NodeManager</span><br><span class="line">52104 NameNode</span><br><span class="line">89115 Jps</span><br><span class="line">57549 DataNode</span><br><span class="line">---------------------hadoop103-----------------</span><br><span class="line">62512 NodeManager</span><br><span class="line">31189 DataNode</span><br><span class="line">60693 ResourceManager</span><br><span class="line">63484 Jps</span><br><span class="line">---------------------hadoop104-----------------</span><br><span class="line">28049 Jps</span><br><span class="line">27148 DataNode</span><br><span class="line">27484 SecondaryNameNode</span><br><span class="line">27871 NodeManager</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>通过浏览器 <code>http://hadoop103:8088/</code> 查看</p>
<p> <img src="/2021/11/22/Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211123122829328.png" alt="image-20211123122829328"></p>
</li>
</ol>
<h2 id="测试集群能否正常运行"><a href="#测试集群能否正常运行" class="headerlink" title="测试集群能否正常运行"></a>测试集群能否正常运行</h2><ol>
<li><p>在 HDFS 文件系统下创建一个目录 <code>/wcinput</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 bin]$ hadoop fs -<span class="built_in">mkdir</span> /wcinput</span><br></pre></td></tr></table></figure></li>
<li><p>然后随便上传一个测试文件到 <code>/wcinput</code> 目录下</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 bin]$ hadoop fs -put xcall /wcinput</span><br></pre></td></tr></table></figure></li>
<li><p> 可以证明分布式 HDFS 是 OK 的。</p>
</li>
<li><p>再运行一个 MapReduce 程序来测试 Yarn 能否正常运行。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 bin]$ <span class="built_in">cd</span> /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/</span><br><span class="line">[lvnengdong@hadoop102 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount /wcinput /wcoutput</span><br></pre></td></tr></table></figure></li>
<li><p> 通过网页查看 Yarn 也是可以正常执行的。</p>
</li>
</ol>
<hr>
<h1 id="集群群起"><a href="#集群群起" class="headerlink" title="集群群起"></a>集群群起</h1><p>上一章节启动 Hadoop 的方式是逐个启动进程来实现的，但是 Hadoop 也为我们提供了群起集群的脚本，保存在 <code>$HADOOP_HOME/sbin</code> 目录下。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 sbin]$ ll <span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line">总用量 120</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 2752 5月  22 2017 distribute-exclude.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 6452 5月  22 2017 hadoop-daemon.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1360 5月  22 2017 hadoop-daemons.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1427 5月  22 2017 hdfs-config.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 2291 5月  22 2017 httpfs.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 3128 5月  22 2017 kms.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 4080 5月  22 2017 mr-jobhistory-daemon.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1648 5月  22 2017 refresh-namenodes.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 2145 5月  22 2017 slaves.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1471 5月  22 2017 start-all.sh	<span class="comment"># 群起脚本</span></span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1128 5月  22 2017 start-balancer.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 3734 5月  22 2017 start-dfs.sh	<span class="comment"># 只启动dfs集群</span></span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1357 5月  22 2017 start-secure-dns.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1347 5月  22 2017 start-yarn.sh	<span class="comment"># 只启动yarn集群</span></span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1462 5月  22 2017 stop-all.sh	<span class="comment"># 群关脚本</span></span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1179 5月  22 2017 stop-balancer.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 3206 5月  22 2017 stop-dfs.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1340 5月  22 2017 stop-secure-dns.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1340 5月  22 2017 stop-yarn.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 4295 5月  22 2017 yarn-daemon.sh</span><br><span class="line">-rwxr-xr-x. 1 lvnengdong lvnengdong 1353 5月  22 2017 yarn-daemons.sh</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="群起脚本的执行原理"><a href="#群起脚本的执行原理" class="headerlink" title="群起脚本的执行原理"></a>群起脚本的执行原理</h2><p>Hadoop 为我们提供的群起脚本的执行原理是：</p>
<ol>
<li> 读取当前机器下的 <code>$HADOOP_HOME/etc/hadoop/slaves</code> 文件，获取集群中所有节点的 IP 地址</li>
<li> 循环执行 <code>ssh IP地址 xxx-daemon.sh start xxx</code> 来启动各种 NameNode、DataNode、ResourceManager、NodeManager 等各种进程。</li>
</ol>
<h2 id="执行群起脚本的前提条件"><a href="#执行群起脚本的前提条件" class="headerlink" title="执行群起脚本的前提条件"></a>执行群起脚本的前提条件</h2><p>所以如果想要执行群起脚本的话，</p>
<ol>
<li> 首先需要将集群中所有节点的主机名（已经做过IP地址映射了）配置到 <code>$HADOOP_HOME/etc/hadoop/slaves</code> 文件中；</li>
<li> 因为群起脚本中执行的 SSH 操作，所以还需要保证当前机器到集群中的其它机器已经配置了 SSH 免密登录。</li>
<li> 因为群里脚本执行的属于 SSH 的 <strong>NonLogin</strong> 登录，所以还需要保证集群中所有用户的家目录 <code>~</code> 下的 <code>./bashrc</code> 文件中，已经配置了 <code>source /etc/profile</code> 属性。</li>
</ol>
<h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>在 Hadoop103 配置 <code>$HADOOP_HOME/etc/hadoop/slaves</code> 文件 </p>
<ol>
<li><p>修改<code>$HADOOP_HOME/etc/hadoop/slaves</code> 文件 </p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 sbin]$ vim <span class="variable">$HADOOP_HOME</span>/etc/hadoop/slaves</span><br></pre></td></tr></table></figure></li>
<li><p> 删除文件中原先的内容 localhost</p>
</li>
<li><p>将集群中的主机名配置到该文件中</p>
 <figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>注意事项：</strong></p>
<ul>
<li>  <code>start-all.sh</code> 命令，其实在底层分别调用了 <code>start-dfs.sh</code> 和 <code>start-yarn.sh</code> 命令</li>
<li>  在集群中的任意一台机器上调用 <code>start-dfs.sh</code> 命令，都可以启动 HDFS 中的所有进程。</li>
<li>  但是对于 <code>start-yarn.sh</code> ，如果在非 ResourceManager 所在的机器上调用，可以启动所有的 NodeManager，但是不会启动 ResourceManager。同理，群停命令 <code>stop-yarn.sh</code> 命令也一样。</li>
<li>  <strong>建议：</strong>1️⃣只需要配置 ResourceManager 所在机器到其它机器的 SSH 免密登录即可。2️⃣只在 ResourceManager 所在的机器上执行群起和群停脚本。3️⃣自定义的 <code>xsync</code> 和 <code>xcall</code> 脚本只放在 ResourceManager 所在的机器即可。</li>
</ul>
<hr>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol>
<li><p>当前 Hadoop 集群（HDFS 和 Yarn）都处于启动状态，先测试一下群停</p>
<ul>
<li>  停止所有 yarn 相关的进程</li>
</ul>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 sbin]$ stop-yarn.sh</span><br></pre></td></tr></table></figure>

<ul>
<li><p>停止所有 dfs 进程</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 sbin]$ stop-dfs.sh</span><br><span class="line">Stopping namenodes on [hadoop102]</span><br><span class="line">hadoop102: stopping namenode</span><br><span class="line">hadoop103: stopping datanode</span><br><span class="line">hadoop102: stopping datanode</span><br><span class="line">hadoop104: stopping datanode</span><br><span class="line">Stopping secondary namenodes [hadoop104]</span><br><span class="line">hadoop104: stopping secondarynamenode</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
<li><p>测试群起</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 ~]$ start-all.sh</span><br><span class="line">This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh</span><br><span class="line">Starting namenodes on [hadoop102]</span><br><span class="line">hadoop102: starting namenode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-lvnengdong-namenode-hadoop102.out</span><br><span class="line">hadoop102: starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-lvnengdong-datanode-hadoop102.out</span><br><span class="line">hadoop103: starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-lvnengdong-datanode-hadoop103.out</span><br><span class="line">hadoop104: starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-lvnengdong-datanode-hadoop104.out</span><br><span class="line">Starting secondary namenodes [hadoop104]</span><br><span class="line">hadoop104: starting secondarynamenode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-lvnengdong-secondarynamenode-hadoop104.out</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-lvnengdong-resourcemanager-hadoop102.out</span><br><span class="line">hadoop103: starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-lvnengdong-nodemanager-hadoop103.out</span><br><span class="line">hadoop104: starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-lvnengdong-nodemanager-hadoop104.out</span><br><span class="line">hadoop102: starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-lvnengdong-nodemanager-hadoop102.out</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h1 id="配置历史日志服务器"><a href="#配置历史日志服务器" class="headerlink" title="配置历史日志服务器"></a>配置历史日志服务器</h1><p>Yarn 在启动后，主要作用是分配资源进行计算。相关程序在计算的过程中，不管是成功还是失败，都会产生一些日志数据。这些日志数据保存在专门的历史日志服务器中，配置历史服务器的步骤如下：</p>
<ol>
<li><p>配置 <code>mapred-site.xml</code>，在该文件里面增加如下配置：</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 表示将MapReduce在运行时产生的日志数据交给 hadoop102:10020 进程来处理 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 表示可以通过浏览器上访问 hadoop102:19888 来查看历史日志服务器上的数据 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--第三方框架使用yarn计算的日志聚集功能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://hadoop102:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>将更新后的配置文件分发到整个集群</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ xsync ./hadoop-2.7.2/</span><br></pre></td></tr></table></figure></li>
<li><p>启动历史服务器</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure></li>
<li><p>查看历史服务器是否启动</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ jps</span><br></pre></td></tr></table></figure>

</li>
<li><p>查看 JobHistory</p>
<p> <code>http://hadoop102:19888/jobhistory</code></p>
<p> <img src="/2021/11/22/Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211123173615772.png" alt="image-20211123173615772"></p>
</li>
</ol>
<hr>
<h1 id="配置日志的聚集"><a href="#配置日志的聚集" class="headerlink" title="配置日志的聚集"></a>配置日志的聚集</h1><p>在上一章我们已经配置好了历史日志服务器，并且可以通过历史日志服务器的IP地址来查看历史日志信息，但是每次都通过 IP 地址查看历史日志信息是非常不方便的。</p>
<p>在 Yarn 的 ResourceManager 界面上有一个超链接可以直接跳转到历史日志服务器，这个链接默认是不生效的，只有配置了 <code>yarn-site.xml</code> 后，才可以通过这个超链接直接跳转到历史日志服务界面。</p>
<p><img src="/2021/11/22/Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211123172955343.png" alt="image-20211123172955343"></p>
<ol>
<li><p>配置 <code>yarn-site.xml</code></p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 开启日志聚集功能（通过RM跳转到历史日志服务器） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置日志保留时间为7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>将配置信息分发到集群中的各个节点上</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop103 module]$ xsync ./hadoop-2.7.2/</span><br></pre></td></tr></table></figure>

</li>
<li><p> 注意：修改配置文件后，需要重启 NodeManager 、ResourceManager 和 HistoryManager 等，重新加载配置文件。</p>
</li>
</ol>
<h2 id="测试："><a href="#测试：" class="headerlink" title="测试："></a>测试：</h2><ol>
<li><p> 停止 NodeManager、ResourceManager、NameNode、DataNode、HistoryManager</p>
</li>
<li><p>启动 NodeManager、ResourceManager、NameNode、DataNode、</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop103 module]$ start-all.sh</span><br></pre></td></tr></table></figure></li>
<li><p>启动 HistoryManager</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop103 module]$ mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure></li>
<li><p>测试：执行 wordcount 程序</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop103 mapreduce]$ <span class="built_in">cd</span> /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/</span><br><span class="line">[lvnengdong@hadoop103 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount /wcinput /wcoutput2</span><br></pre></td></tr></table></figure></li>
<li><p> 测试通过网页跳转，OK。</p>
</li>
</ol>
<hr>
<h1 id="集群时间同步"><a href="#集群时间同步" class="headerlink" title="集群时间同步"></a>集群时间同步</h1>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/" class="post-title-link" itemprop="url">Hadoop集群搭建</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-11-21 11:57:51" itemprop="dateCreated datePublished" datetime="2021-11-21T11:57:51+08:00">2021-11-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-02-16 10:32:21" itemprop="dateModified" datetime="2022-02-16T10:32:21+08:00">2022-02-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Hadoop-运行环境搭建"><a href="#Hadoop-运行环境搭建" class="headerlink" title="Hadoop 运行环境搭建"></a>Hadoop 运行环境搭建</h1><h2 id="安装前的注意事项"><a href="#安装前的注意事项" class="headerlink" title="安装前的注意事项"></a>安装前的注意事项</h2><ol>
<li><p>Hadoop项目是用Java语言实现的，Hadoop运行时依赖JDK相关的类库，所以安装Hadoop前必须保证当前机器已经安装了JDK，并且配置了 <code>JAVA_HOME</code> 全局变量。</p>
</li>
<li><p>Hadoop 集群本质上通过多个不同类型的进程配合工作，如 NameNode，DataNode，ResourceManager，NodeManager 等，这些进程往往部署在不同的机器上，并且进程之间需要互相通信。既然要通信时就需要知道彼此的 IP 地址和端口号，为了方便记忆和书写，我们往往会选择使用主机名进行通信，这时候就需要修改 <code>hosts</code> 文件配置主机名到 IP 的映射了。当然直接使用 IP 进行通信的话就无需配置主机名到 IP 的映射了。</p>
<ul>
<li>  host 文件在不同系统下的位置：</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="quote">&gt; Linux：/etc/hosts</span></span><br><span class="line"><span class="quote">&gt; Windows：C:\Windows\System32\drivers\etc\hosts</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>文件权限</strong>：Hadoop 在运行时会产生很多数据（包括 HDFS 存储的数据和每个节点的日志文件等），对于保存这些数据的目录，我们必须设置让当前启动 Hadoop 进程的用户拥有写权限。</p>
</li>
<li><p>关闭防火墙，并且关闭防火墙开机自启。</p>
</li>
</ol>
<hr>
<h2 id="使用普通用户操作-Hadoop"><a href="#使用普通用户操作-Hadoop" class="headerlink" title="使用普通用户操作 Hadoop"></a>使用普通用户操作 Hadoop</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. 创建普通用户 Xxx</span><br><span class="line">    useradd Xxx</span><br><span class="line">2. 为 Xxx 设置密码</span><br><span class="line">    passwd 123456</span><br><span class="line">3. 赋予 Xxx 用户 root 权限</span><br><span class="line">    vim /etc/sudoers</span><br><span class="line">4. 将 /opt 目录下创建的 software 目录和 module 目录的属主改为 Xxx</span><br><span class="line">    <span class="built_in">chown</span> -R Xxx:Xxx /opt/software /opt/module</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="安装-Hadoop"><a href="#安装-Hadoop" class="headerlink" title="安装 Hadoop"></a>安装 Hadoop</h2><ol>
<li><p> 将 <code>hadoop-2.7.2.tar.gz</code> 导入到 <code>opt/software</code> 目录下；</p>
</li>
<li><p>解压安装文件到 <code>/opt/module</code> 下面；</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.7.2.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li>
<li><p><strong>将 HADOOP_HOME 添加到环境变量</strong></p>
<ul>
<li>  （1）获取 Hadoop 安装路径</li>
</ul>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure>

<ul>
<li><p>（2）编辑 <code>/etc/profile.d/my_env.sh</code> 文件</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure></li>
<li><p>（3）在文件末尾添加 <code>HADOOP_HOME</code></p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>测试配置是否生效</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop version</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h1 id="Hadoop-的目录介绍"><a href="#Hadoop-的目录介绍" class="headerlink" title="Hadoop 的目录介绍"></a>Hadoop 的目录介绍</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 module]$ ll hadoop-2.7.2/</span><br><span class="line">总用量 28</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong   194 5月  22 2017 bin</span><br><span class="line">drwxr-xr-x. 3 lvnengdong lvnengdong    20 5月  22 2017 etc</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong   106 5月  22 2017 include</span><br><span class="line">drwxr-xr-x. 3 lvnengdong lvnengdong    20 5月  22 2017 lib</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong   239 5月  22 2017 libexec</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong 15429 5月  22 2017 LICENSE.txt</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   101 5月  22 2017 NOTICE.txt</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong  1366 5月  22 2017 README.txt</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong  4096 5月  22 2017 sbin</span><br><span class="line">drwxr-xr-x. 4 lvnengdong lvnengdong    31 5月  22 2017 share</span><br></pre></td></tr></table></figure>

<ul>
<li>  <strong>bin</strong>：存放对 Hadoop 相关服务（HDFS、MR 和 YARN ）进行操作的脚本。</li>
<li>  <strong>sbin</strong> ：存放管理员启动和停止集群时使用的命令脚本。</li>
<li>  <strong>etc</strong>：存放 Hadoop 的配置文件。</li>
<li>  <strong>lib</strong>：存放 Hadoop 的本地库</li>
<li>  <strong>share</strong>：存放 Hadoop 的依赖 jar 包、文件和官方案例。</li>
</ul>
<hr>
<h1 id="Hadoop-运行模式"><a href="#Hadoop-运行模式" class="headerlink" title="Hadoop 运行模式"></a>Hadoop 运行模式</h1><p>Hadoop 提供了两类三种运行模式，分别是：</p>
<ul>
<li>  <strong>本地模式</strong></li>
<li><strong>分布式模式</strong><ul>
<li>  伪分布式模式：如果 NN 和 DN 都在一台机器，且只有一个 DN 节点，称为伪分布式！</li>
<li>  完全分布式模式</li>
</ul>
</li>
</ul>
<p>这两种运行模式本质上其实就是 HDFS 的运行模式，分别对应了：</p>
<ul>
<li>  1️⃣在本地机器上使用 HDFS，即使用本地机器的文件系统。 </li>
<li>  2️⃣在多台机器上使用 HDFS，即使用了一个分布式的文件系统。</li>
</ul>
<p>Hadoop 安装后默认使用的本地模式，如果想要使用分布式模式，需要修改对应的配置文件中的配置参数。</p>
<h2 id="HDFS-运行模式配置"><a href="#HDFS-运行模式配置" class="headerlink" title="HDFS 运行模式配置"></a>HDFS 运行模式配置</h2><p>HDFS 采用哪种运行模式，取决于配置参数 <strong>fs.defaultFS</strong>，该参数要在 <code>core-default.xml</code> 配置文件中进行配置。默认使用的是本地模式。</p>
<p><strong>本地模式</strong></p>
<ul>
<li><p>  概念：使用当前计算机的文件系统作为 HDFS 的文件系统。</p>
</li>
<li><p>参数：</p>
  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- core-default.xml --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- fs.defaultFS=file:///    （默认）【file:/// 是一种本地文件系统协议】 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>分布式模式</strong></p>
<ul>
<li><p>  概念：要使用的文件系统是一个分布式的文件系统！一个分布式的文件系统，必须由 NameNode，DataNode 等若干进程共同运行完成文件系统的读写操作！</p>
</li>
<li><p>参数：</p>
  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- core-default.xml --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- fs.defaultFS=hdfs://     【hdfs:// 代表一种分布式文件系统协议】 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h2 id="MapReduce-的运行模式"><a href="#MapReduce-的运行模式" class="headerlink" title="MapReduce 的运行模式"></a>MapReduce 的运行模式</h2><ol>
<li><p>按照 MR 的规范编写一个程序；</p>
</li>
<li><p>将程序打成一个 jar 包</p>
</li>
<li><p>运行 jar 包。</p>
<p>运行 jar 包可以选择两种模式，分别是：1️⃣本地模式；2️⃣在 Yarn 上运行（将 jar 包提交给 Yarn，由 Yarn 调度资源完成运算）。具体使用哪一种模式由参数由参数 <code>mapreduce.framework.name</code> 决定，该参数也需要在 <code>core-default.xml</code> 配置文件中进行配置。默认值是：<code>mapreduce.framework.name=local</code>，表示在本地模式下运行。</p>
</li>
</ol>
<p><strong>MapReduce 的两种运行方式</strong></p>
<ol>
<li><p><strong>本地模式</strong>：在本地运行 MR，即在本机使用多线程的方式模拟多个 Task 的运行。对应的在  <code>mapred-default.xml</code> 配置文件中的配置信息为：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- mapred-default.xml --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- mapreduce.framework.name=local --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>local<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>在 YARN 上运行</strong>：需要启动 YARN，YARN 由 RM 和 NM 进程组成，所以还需要启动 RM 和 NM。将 MR 生成的 Job 提交给 YARN ，由 YARN 负责将 Job 中拆分成多个 Task 并分配到多台机器中运算。对应的在  <code>mapred-default.xml</code> 配置文件中的配置信息为：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- mapred-default.xml --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- mapreduce.framework.name=yarn --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>local<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h1 id="Hadoop-的配置文件"><a href="#Hadoop-的配置文件" class="headerlink" title="Hadoop 的配置文件"></a>Hadoop 的配置文件</h1><p>同大多数的项目一样，Hadoop 安装后，为我们提供了 4 个默认的配置文件，通过加载默认配置文件已经可以正常启动一个 Hadoop 项目。但是，如果想要使用自定义配置信息的话，有两种方法：</p>
<ul>
<li>  <strong>方式一</strong>：Hadoop 同时也提供了 4 个自定义配置文件支持用户使用自定义的配置文件覆盖默认配置信息。Hadoop 在启动时，会先加载 4 个默认的配置文件，再加载用户自定义的配置文件，如果自定义的配置文件中出现了和默认配置文件中同名的参数，就会覆盖默认配置文件中的值。</li>
<li>  <strong>方式二</strong>：通过显式指定配置参数的方法启动Hadoop，这种方法的弊端是仅在当前次有效，当集群重启后还是会以配置文件中的配置信息为准。</li>
</ul>
<h2 id="四个默认配置文件"><a href="#四个默认配置文件" class="headerlink" title="四个默认配置文件"></a>四个默认配置文件</h2><p> <strong>默认配置文件所在目录：</strong>默认配置文件直接内嵌在Hadoop依赖的类库中，具体目录为：</p>
<ul>
<li>  <code>$HADOOP_HOME/share</code>：该目录下保存 着 Hadoop 启动时需要加载的所有 jar 包。</li>
<li>  <code>$HADOOP_HOME/share/hadoop/.../xxx.jar/xxx-default.xml</code> ：默认配置文件的保存路径，4个配置文件分别保存在4个不同的 jar 包的类路径下</li>
</ul>
<p><strong>默认配置文件：</strong></p>
<ul>
<li><strong>core-default.xml</strong>    保存 Hadoop 最核心的配置参数</li>
<li><strong>hdfs-default.xml</strong>    保存 HDFS 相关的参数</li>
<li><strong>mapred-default.xm</strong>l   保存 MapReduce 相关的参数</li>
<li><strong>yarn-default.xml</strong>    保存 YARN 相关的参数</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop-common-2.7.7.jar  ==&gt; core-default.xml <span class="comment"># core-default.xml 配置文件保存在  hadoop-common-2.7.2.jar 这个jar包中</span></span><br><span class="line">hadoop-hdfs-2.7.2.jar ==&gt; hdfs-default.xml</span><br><span class="line">hadoop-mapreduce-client-core-2.7.2.jar ==&gt; mapred-default.xml</span><br><span class="line">hadoop-yarn-common-2.7.2.jar ==&gt; yarn-default.xml</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="四个用户自定义配置文件"><a href="#四个用户自定义配置文件" class="headerlink" title="四个用户自定义配置文件"></a>四个用户自定义配置文件</h2><p><strong>注意事项：</strong></p>
<ul>
<li>  4个用户自定义配置文件必须命名为 <code>xxx-site.xml</code>，只有这样<strong>用户自定义的配置文件才会覆盖默认配置文件中的同名参数。</strong></li>
<li>  Hadoop 在启动时，首先会加载 4 个默认的配置文件，再加载用户自定义的配置文件，如果用户自定义的配置文件中有和默认配置文件中同名的参数，可以覆盖之前已经加载的值！</li>
</ul>
<p><strong>自定义配置文件所在目录：</strong><code>$HADOOP_HOME/etc/hadoop/</code> </p>
<p><strong>自定义配置文件：</strong></p>
<ul>
<li>  <strong>core-site.xml</strong>    保存用户自定义的 Hadoop 最核心的配置参数</li>
<li>  <strong>hdfs-site.xml</strong>    保存用户自定义的 HDFS 相关的参数</li>
<li>  <strong>mapred-site.xml</strong>    保存用户自定义的 MapReduce 相关的参数</li>
<li>  <strong>yarn-site.xml</strong>    保存用户自定义的 YARN 相关的参数</li>
</ul>
<hr>
<h2 id="配置文件使用小技巧"><a href="#配置文件使用小技巧" class="headerlink" title="配置文件使用小技巧"></a>配置文件使用小技巧</h2><p><strong>小技巧1：</strong></p>
<p>在 <code>$HADOOP_HOME/etc/hadoop/</code> 目录下保存了四个自定义配置信息的模板，我们在需要使用自定义配置的时候可以直接拷贝这几个配置文件并加以修改使用。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ ll <span class="variable">$HADOOP_HOME</span>/etc/hadoop/ | grep site.xml</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   774 5月  22 2017 core-site.xml</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   775 5月  22 2017 hdfs-site.xml</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   620 5月  22 2017 httpfs-site.xml</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong  5511 5月  22 2017 kms-site.xml</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   758 5月  22 2017 mapred-site.xml.template</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   690 5月  22 2017 yarn-site.xml</span><br></pre></td></tr></table></figure>





<p><strong>小技巧2</strong></p>
<ol>
<li><p><strong>在使用 Hadoop 命令时可以手动指定加载哪个目录下的配置文件</strong></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在Linux终端中键入 `hadoop`，会提示如下信息 </span></span><br><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop</span><br><span class="line">Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]</span><br><span class="line"></span><br><span class="line"><span class="comment"># [--config confdir] 表示在使用 hadoop 命令时可以手动指定要加载哪个目录中的自定义配置文件</span></span><br></pre></td></tr></table></figure>

<ul>
<li>  <strong>如果未显式指定配置文件的目录，默认读取 <code>$HADOOP_HOME/etc/hadoop</code> 目录下对应的自定义配置文件！</strong></li>
</ul>
</li>
</ol>
<hr>
<h1 id="本地-HDFS-使用测试"><a href="#本地-HDFS-使用测试" class="headerlink" title="本地 HDFS 使用测试"></a>本地 HDFS 使用测试</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用：hadoop fs 命令 文件路径</span><br><span class="line"><span class="comment"># 表示使用 hadoop 的 file system 执行操作</span></span><br></pre></td></tr></table></figure>

<ul>
<li>  <code>hadoop fs</code> 常见命令</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop fs</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-<span class="built_in">cat</span> [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-checksum &lt;src&gt; ...]</span><br><span class="line">	[-<span class="built_in">chgrp</span> [-R] GROUP PATH...]</span><br><span class="line">	[-<span class="built_in">chmod</span> [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">	[-<span class="built_in">chown</span> [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">	[-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-count [-q] [-h] &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">cp</span> [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">	[-<span class="built_in">df</span> [-h] [&lt;path&gt; ...]]</span><br><span class="line">	[-<span class="built_in">du</span> [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">	[-expunge]</span><br><span class="line">	[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">	[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">	[-getmerge [-<span class="built_in">nl</span>] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-<span class="built_in">help</span> [cmd ...]]</span><br><span class="line">	[-<span class="built_in">ls</span> [-d] [-h] [-R] [&lt;path&gt; ...]]</span><br><span class="line">	[-<span class="built_in">mkdir</span> [-p] &lt;path&gt; ...]</span><br><span class="line">	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-<span class="built_in">mv</span> &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]	<span class="comment"># 上传文件，&lt;localsrc&gt;表示文件源地址；&lt;dst&gt;表示目的地址</span></span><br><span class="line">	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">	[-<span class="built_in">rm</span> [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">	[-<span class="built_in">rmdir</span> [--ignore-fail-on-non-empty] &lt;<span class="built_in">dir</span>&gt; ...]</span><br><span class="line">	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--<span class="built_in">set</span> &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">stat</span> [format] &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">tail</span> [-f] &lt;file&gt;]</span><br><span class="line">	[-<span class="built_in">test</span> -[defsz] &lt;path&gt;]</span><br><span class="line">	[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-touchz &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">truncate</span> [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">	[-usage [cmd ...]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<ol>
<li><p>创建一个 testUpload 文件用于测试文件上传</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ vim /opt/module/hadoop-2.7.2/etc/hadoop/testUpload</span><br><span class="line"></span><br><span class="line"><span class="comment"># testUpload 文件内容为：</span></span><br><span class="line">测试文件上传</span><br></pre></td></tr></table></figure></li>
<li><p>在当前目录创建一个保存自定义配置文件的目录 <code>myconfig</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> myconfig</span><br></pre></td></tr></table></figure></li>
<li><p>运行 <code>hadoop fs</code> 命令上传文件</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop --config myconfig fs -put testUpload /</span><br><span class="line"><span class="comment"># 表示加载 myconfig 目录下的配置文件，完成将当前目录下的 testUpload 文件上传到 HDFS 的根目录（/）下</span></span><br><span class="line"><span class="comment"># 因为在本地模式下HDFS使用的就是当前Linux系统的文件系统，所以 / 就表示当前Linux系统的根目录</span></span><br></pre></td></tr></table></figure></li>
<li><p>报错信息分析</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># core-site.xml not found，因为我们创建的 myconfig 目录是一个空目录，没有任何的自定义配置文件，</span></span><br><span class="line"><span class="comment"># 而如果配置了 --config，则至少需要有一个自定义的 core-site.xml 文件。</span></span><br><span class="line"><span class="comment"># 解决办法是在 myconfig 目录下创建一个 core-site.xml 自定义配置文件</span></span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">&quot;main&quot;</span> java.lang.RuntimeException: core-site.xml not found</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2566)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2492)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.<span class="built_in">set</span>(Configuration.java:1143)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.<span class="built_in">set</span>(Configuration.java:1115)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1451)</span><br><span class="line">	at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:321)</span><br><span class="line">	at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:487)</span><br><span class="line">	at org.apache.hadoop.util.GenericOptionsParser.&lt;init&gt;(GenericOptionsParser.java:170)</span><br><span class="line">	at org.apache.hadoop.util.GenericOptionsParser.&lt;init&gt;(GenericOptionsParser.java:153)</span><br><span class="line">	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:64)</span><br><span class="line">	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)</span><br><span class="line">	at org.apache.hadoop.fs.FsShell.main(FsShell.java:340)</span><br></pre></td></tr></table></figure></li>
<li><p>将 <code>$HADOOP_HOME/etc/hadoop/</code> 目录下的 <code>core-site.xml</code> 配置文件复制到 <code>myconfig</code> 目录下</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ <span class="built_in">cp</span> <span class="variable">$HADOOP_HOME</span>/etc/hadoop/core-site.xml myconfig/</span><br></pre></td></tr></table></figure></li>
<li><p>重新执行文件上传命令</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop --config myconfig fs -put testUpload /</span><br><span class="line">put: /testUpload._COPYING_ (权限不够)</span><br></pre></td></tr></table></figure></li>
<li><p> 报错信息分析：因为当前使用的是本地模式，也就是说 HDFS 使用 Linux 的本地文件系统，而我们当前登录的用户对 HDFS 的根目录 <code>/</code> 是没有操作权限的，我们可以将该文件上传到一个当前登录用户拥有权限的目录下，该命令就可以正常执行了。</p>
</li>
<li><p>重新执行上传命令</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop --config myconfig fs -put testUpload /opt/module/</span><br><span class="line">[lvnengdong@hadoop102 hadoop]$ <span class="built_in">echo</span> $?</span><br><span class="line">0	<span class="comment"># 表示上一条命令执行成功</span></span><br></pre></td></tr></table></figure></li>
<li><p>执行结果验证</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ ll /opt/module/</span><br><span class="line">总用量 4</span><br><span class="line"><span class="comment"># ..</span></span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong  19 11月 21 20:47 testUpload</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 /opt/module/ 目录下多出了一个 testUpload，表示文件上传执行成功了</span></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h1 id="分布式-HDFS-使用测试"><a href="#分布式-HDFS-使用测试" class="headerlink" title="分布式 HDFS 使用测试"></a>分布式 HDFS 使用测试</h1><h2 id="分布式-HDFS-常用命令"><a href="#分布式-HDFS-常用命令" class="headerlink" title="分布式 HDFS 常用命令"></a>分布式 HDFS 常用命令</h2><p>在分布式 HDFS 中，需要额外管理 NameNode、DataNode 等，所以针对 NameNode、DataNode 等有额外的操作命令。</p>
<h3 id="启动与停止"><a href="#启动与停止" class="headerlink" title="启动与停止"></a>启动与停止</h3><p><strong>NameNode</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动Namenode	</span></span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止Namenode	</span></span><br><span class="line">hadoop-daemon.sh stop namenode</span><br></pre></td></tr></table></figure>



<p><strong>DataNode</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动datanode	</span></span><br><span class="line">hadoop-daemon.sh start datanode</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止datanode</span></span><br><span class="line">hadoop-daemon.sh stop datanode</span><br></pre></td></tr></table></figure>

<ul>
<li>  上述四条命令都是 sbin 目录中的脚本【sbin 已经被添加到环境变量中了】</li>
</ul>
<hr>
<h3 id="查看-NameNode-DataNode-进程状态"><a href="#查看-NameNode-DataNode-进程状态" class="headerlink" title="查看 NameNode/DataNode 进程状态"></a>查看 NameNode/DataNode 进程状态</h3><ol>
<li><p><strong>jps命令</strong>    </p>
<ul>
<li>  因为 Hadoop 是用 Java 实现的，所以可以直接用 <code>jps</code> 命令查看 NameNode 和 DataNode 进程的运行状态。</li>
</ul>
</li>
<li><p>通过<strong>浏览器访问</strong></p>
<ul>
<li>  Hadoop 给我们提供了可视化的管理界面，可以访问该页面查看 HDFS 中相关进程（包括 NameNode 和 DataNode）的运行状态。</li>
</ul>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 访问地址</span></span><br><span class="line">http://&#123;NameNode所在主机的IP地址&#125;:50070</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过浏览器访问是通过 http 协议访问的，而 NameNode 的 http 协议访问端口号是 50070</span></span><br><span class="line"><span class="comment"># 当前还可以通过 RPC 协议来访问 NameNode，RPC 访问端口号是我们在配置文件中自定义的，一般都设置为 9000</span></span><br><span class="line"><span class="comment"># DataNode 被 NameNode 管理，所以在 NameNode 的可视化管理界面可以查看 DataNode 线程的信息</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h3 id="格式化-Namenode"><a href="#格式化-Namenode" class="headerlink" title="格式化 Namenode"></a>格式化 Namenode</h3><p><strong>命令：</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure>

<p><strong>注意：格式化 Namenode 只需要执行一次</strong></p>
<p><strong>作用：</strong>格式化时，首先会根据配置文件 <code>core-default.xml</code> 或 <code>core-site.xml</code> 中的配置信息生成用于保存 NameNode 和 DataNode 数据的目录；并在这个目录中生成一些版本验证文件。</p>
<p><strong>Demo：</strong></p>
<ol>
<li><p> 使用 <code>hadoop namenode -format</code> 命令自动创建的保存 NameNode 和 DataNode 数据的目录；</p>
</li>
<li><p> 并使用 <code>tree</code> 命令查看这个命令的目录结构。</p>
</li>
<li><p>我们发现在这个目录下会多出一个 <strong>dfs</strong> 目录，<strong>name</strong> 目录用于保存 NameNode 节点的数据，并且继续深入还会有一些目录和文件，这些目录和文件用于进行版本验证，表明当前目录就是一个 NameNode 保存运行时数据的目录。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ tree /opt/module/hadoop-2.7.2/data/</span><br><span class="line">/opt/module/hadoop-2.7.2/data/</span><br><span class="line">└── dfs</span><br><span class="line">    └── name	<span class="comment"># name 目录用于保存 NameNode 节点的数据文件</span></span><br><span class="line">        ├── current</span><br><span class="line">        │   ├── edits_inprogress_0000000000000000001</span><br><span class="line">        │   ├── fsimage_0000000000000000000</span><br><span class="line">        │   ├── fsimage_0000000000000000000.md5</span><br><span class="line">        │   ├── seen_txid</span><br><span class="line">        │   └── VERSION</span><br><span class="line">        └── in_use.lock</span><br></pre></td></tr></table></figure></li>
<li><p>由于当前我们还没有启动 DataNode 节点，当启动 DataNode 节点并向该节点上传数据后，该目录下会新出现一个 <code>./dfs/data/...</code> 目录用于存储 DataNode 运行时产生的数据。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ tree /opt/module/hadoop-2.7.2/data/</span><br><span class="line">/opt/module/hadoop-2.7.2/data/</span><br><span class="line">└── dfs</span><br><span class="line">    ├── data	<span class="comment"># data 目录用于保存 DataNode 节点的数据文件</span></span><br><span class="line">    │   ├── current</span><br><span class="line">    │   │   ├── BP-401557870-192.168.1.102-1637506411781</span><br><span class="line">    │   │   │   ├── current</span><br><span class="line">    │   │   │   │   ├── finalized</span><br><span class="line">    │   │   │   │   │   └── subdir0</span><br><span class="line">    │   │   │   │   │       └── subdir0</span><br><span class="line">    │   │   │   │   │           ├── blk_1073741825	<span class="comment"># block块的id，与浏览器中展示的块的id一致</span></span><br><span class="line">    │   │   │   │   │           └── blk_1073741825_1001.meta</span><br><span class="line">    │   │   │   │   ├── rbw</span><br><span class="line">    │   │   │   │   └── VERSION</span><br><span class="line">    │   │   │   ├── scanner.cursor</span><br><span class="line">    │   │   │   └── tmp</span><br><span class="line">    │   │   └── VERSION</span><br><span class="line">    │   └── in_use.lock</span><br><span class="line">    └── name	<span class="comment"># name 目录用于保存 NameNode 节点的数据文件</span></span><br><span class="line">        ├── current</span><br><span class="line">        │   ├── edits_inprogress_0000000000000000001</span><br><span class="line">        │   ├── fsimage_0000000000000000000</span><br><span class="line">        │   ├── fsimage_0000000000000000000.md5</span><br><span class="line">        │   ├── seen_txid</span><br><span class="line">        │   └── VERSION</span><br><span class="line">        └── in_use.lock</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="NameNode-通信地址原理分析"><a href="#NameNode-通信地址原理分析" class="headerlink" title="NameNode 通信地址原理分析"></a>NameNode 通信地址原理分析</h2><p>HDFS（Hadoop分布式文件系统）就是一个服务器集群，HDFS 集群是由一个 NameNode 和多个 DataNode 组成的，其中 NameNode 负责保存元数据信息、<strong>与 DataNode 通信</strong>以及<strong>与客户端通信</strong>。既然要通信，那么就必须明确两点：</p>
<ol>
<li> <strong>通信协议：</strong>在 Hadoop 中，客户端与 NameNode（服务器端）， NameNode 和 DataNode 之间通信采用的都是 RPC 协议，并且如果使用 HDFS 文件传输的话，还需要满足 <code>hdfs://</code> 文件传输协议。</li>
<li> <strong>通信地址：</strong>NameNode 在这三者中处于通信的核心地位，既要和 DataNode 通信，又要和客户端通信，所以 NameNode 必须对外暴露自己的通信地址，让客户端和 NameNode 可以找到自己。又因为 NameNode 是一个线程，唯一定位一个线程需要 IP 地址和端口号两个因素。</li>
</ol>
<p>在 <code>core-site.xml</code> 配置文件中，<code>fs.defaultFS</code> 属性配置的其实就是 NameNode 的通信地址，用于让客户端和 DataNode 能够找到自己，NameNode 进程的通信地址由 IP 地址和端口号组成。如果不配置端口号的话每次启动线程时 NameNode 所在的服务器都会随机分配一个端口号给 NameNode 进程，不方便客户端和 DataNode 查找到 NameNode，所以我们一般会将 NameNode 的端口号写死，这样每次启动 NameNode 时使用的就都是同一端口号了。</p>
<hr>
<h2 id="NameNode-生成保存数据的目录原理分析"><a href="#NameNode-生成保存数据的目录原理分析" class="headerlink" title="NameNode 生成保存数据的目录原理分析"></a>NameNode 生成保存数据的目录原理分析</h2><p><strong>手动指定保存数据的目录</strong></p>
<p>Hadoop 在运行期间，HDFS 中的 NameNode 节点中需要一定的磁盘空间来保存元数据，DataNode 节点需要一定的磁盘空间来保存真正的数据，在默认配置文件 <code>core-default.xml</code> 中，设置了将每个节点会数据都保存在当前服务器的 <code>/tmp/hadoop-用户名/dfs</code> 目录下，而我们当前的服务器上可能并没有这个目录，所以一定要事先将这个目录创建好。</p>
<p>由于 <code>/tmp</code> 目录是 Linux 系统中的一个临时目录，每 15 天会自动清空一次，所以为了防止数据丢失，我们一般会重新指定这个目录。这时就需要在自定义的配置文件 <code>hdfs-core.xml</code> 中重写配置信息覆盖 <strong>hadoop.tmp.dir</strong> 属性。</p>
<ul>
<li>  默认配置文件 <code>core-default.xml</code> 中的配置信息</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">&lt;!-- core-default.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/hadoop-$&#123;user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>A base for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>  自定义配置文件 <code>core-site.xml</code> 中的配置信息</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">     配置 HDFS 框架在运行时产生的数据的存储目录。</span></span><br><span class="line"><span class="comment">	（1）NameNode 上保存的元数据等信息,</span></span><br><span class="line"><span class="comment">	（2）DataNode 上保存的真实数据信息</span></span><br><span class="line"><span class="comment">  都保存在该目录下。</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p><strong>使用格式化指令创建保存数据的目录</strong></p>
<p>在上面的配置文件中，我们设置了 HDFS 中数据的保存目录。所以我们必须事先在 NameNode 节点的服务器上创建这个目录后才能成功启动 NameNode。<strong>但是</strong>这个文件并不是一个空目录，也就是说如果我们通过 <code>mkdir</code> 命令创建的空目录是无法识别为能够保存 HDFS 数据的目录，能够保存 HDFS 数据的目录中必须有一个版本验证文件，如果想要创建一个包含版本验证文件的目录，我们需要通过 Hadoop 提供的格式化工具<strong>（hadoop namenode -format）</strong>生成一个目录，并且会自动在这个目录下生成一个版本号。</p>
<hr>
<h2 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h2><ol>
<li> 如果想要使用分布式的 HDFS，首先需要在自定义配置文件 <code>core-site.xml</code> 中重写默认配置文件中的  <code>fs.defaultFS</code> 属性为 <code>hdfs://</code>。它的含义是设置文件传输协议为 hdfs，表示 FileSystem 为 hdfs 分布式文件系统。</li>
<li> 同时要设置 NameNode 的唯一通信地址为：<code>hadoop102:9000</code>（IP地址 + 端口号）</li>
<li> 二者结合一下就变成了 <code>hdfs://hadoop102:9000</code></li>
<li> 在 <code>core-site.xml</code> 中重写 NameNode 和 DataNode 保存数据的目录</li>
<li> 使用 Hadoop 提供的格式化命令 <code>hadoop namenode -format</code> 创建这个保存数据的目录。</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">    1、使用分布式文件存储系统</span></span><br><span class="line"><span class="comment">    2、通知DataNode和客户端NameNode的唯一地址</span></span><br><span class="line"><span class="comment">    [Hadoop 集群中的各个进程之间需要互相通信，通过 RPC 方式通信：</span></span><br><span class="line"><span class="comment">        （1）客户端与 NN 直接通信；</span></span><br><span class="line"><span class="comment">        （2）DN 与 NN 也要进行通信。</span></span><br><span class="line"><span class="comment">    所以我们必须在集群中每台机器的配置文件 core-site.xml 中配置 NN 的通信地址]</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="comment">&lt;!-- hadoop102:主机名到IP地址的映射</span></span><br><span class="line"><span class="comment">	 9000：NameNode进程在RPC协议下的端口号（随便取，只要不被占用即可）</span></span><br><span class="line"><span class="comment"> --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">     配置 HDFS 框架在运行时产生的数据的存储目录。</span></span><br><span class="line"><span class="comment">	（1）NameNode 上保存的元数据等信息,</span></span><br><span class="line"><span class="comment">	（2）DataNode 上保存的真实数据信息</span></span><br><span class="line"><span class="comment">  都保存在该目录下。</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>







<hr>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol>
<li><p>首先，修改上一章节使用过的自定义配置文件 <code>./myconfig/core-site.xml</code> 。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ vim myconfig/core-site.xml</span><br></pre></td></tr></table></figure></li>
<li><p>添加自定义的 NameNode 配置信息。</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>使用自定义的配置文件启动 NameNode</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop-daemon.sh --config /opt/module/hadoop-2.7.2/etc/hadoop/myconfig/ start namenode</span><br><span class="line"><span class="comment"># 启动后的输出的信息</span></span><br><span class="line">starting namenode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-lvnengdong-namenode-hadoop102.out</span><br><span class="line"><span class="comment"># 这条信息表示，启动 namenode 后，所有的日志信息都会保存在这个目录下，目录的组成信息为：</span></span><br><span class="line"><span class="comment"># $HADOOP_HOME/logs/hadoop-用户名-进程名-主机名.out</span></span><br><span class="line"><span class="comment"># out 表示日志输出时的配置文件，真正的日志其实保存在 logs 目录下的 `xxx.log` 文件中</span></span><br><span class="line">2021-11-21 21:56:35,921 INFO  [main] namenode.NameNode (LogAdapter.java:info(47)) - STARTUP_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">STARTUP_MSG: Starting NameNode</span><br><span class="line">STARTUP_MSG:   host = hadoop102/192.168.1.102</span><br><span class="line">STARTUP_MSG:   args = []</span><br><span class="line">STARTUP_MSG:   version = 2.7.2</span><br><span class="line">......</span><br></pre></td></tr></table></figure></li>
<li><p>查看进程是否启动成功。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ jps</span><br><span class="line"><span class="comment"># 发现未启动成功</span></span><br></pre></td></tr></table></figure></li>
<li><p>查看日志信息</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印日志文件的后100行信息</span></span><br><span class="line">[lvnengdong@hadoop102 logs]$ <span class="built_in">tail</span> -n 100 hadoop-lvnengdong-namenode-hadoop102.out</span><br></pre></td></tr></table></figure></li>
<li><p>storage directory does not exist or is not accessible ==&gt;存储目录不存在或不可访问。</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lvnengdong/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:<span class="number">327</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:<span class="number">215</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:<span class="number">975</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:<span class="number">681</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:<span class="number">584</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:<span class="number">644</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:<span class="number">811</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:<span class="number">795</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:<span class="number">1488</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:<span class="number">1554</span>)</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">异常分析：</span></span><br><span class="line"><span class="comment">	因为我们在自定义的配置文件中未重写 NameNode 和 DataNode 保存数据的目录，所以在项目启动时</span></span><br><span class="line"><span class="comment">	使用的仍然是默认配置文件中的配置，也就是说会把数据保存在 /tmp/hadoop-用户名/dfs/ 目录下，</span></span><br><span class="line"><span class="comment">	但是该目录在当前服务器上并不存在，所以才会报错“storage directory does not exist or is not accessible”</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></li>
<li><p>在自定义的配置文件 <code>hdfs-core.xml</code> 中添加配置信息覆盖 <strong>hadoop.tmp.dir</strong> 属性。</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>并创建 <code>/opt/module/hadoop-2.7.2/data</code> 目录</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 myconfig]$ <span class="built_in">mkdir</span> /opt/module/hadoop-2.7.2/data</span><br></pre></td></tr></table></figure></li>
<li><p>重新启动 NameNode</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 myconfig]$ hadoop-daemon.sh --config /opt/module/hadoop-2.7.2/etc/hadoop/myconfig/ start namenode</span><br></pre></td></tr></table></figure></li>
<li><p> 使用 jps 命令查看发现仍然没有 NameNode 进程；</p>
</li>
<li><p>查看日志：发现错误信息仍然显示保存数据的目录没有创建，但是我们确实创建了。</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /opt/<span class="keyword">module</span>/hadoop-<span class="number">2.7</span><span class="number">.2</span>/data/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:<span class="number">327</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:<span class="number">215</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:<span class="number">975</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:<span class="number">681</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:<span class="number">584</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:<span class="number">644</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:<span class="number">811</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:<span class="number">795</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:<span class="number">1488</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:<span class="number">1554</span>)</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">异常分析：</span></span><br><span class="line"><span class="comment">	这个错误是因为目前我们虽然创建了和配置文件中一致的目录来保存NameNode和DataNode运行时产生的数据，</span></span><br><span class="line"><span class="comment">	但是由于这个目录是我们通过 mkdir 手动创建的，是一个空目录，该目录中并没有版本验证文件，所以无法</span></span><br><span class="line"><span class="comment">	证明这个目录就是用来保存 NameNode 和 DataNode 数据的目录。为了解决这个问题，我们还需要通过</span></span><br><span class="line"><span class="comment">	Hadoop提供的格式化工具来自动根据配置文件生成带版本号的可以保存NameNode生产数据的目录。</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></li>
<li><p>先删除我们自己创建的 <code>/opt/module/hadoop-2.7.2/data</code> 目录</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ <span class="built_in">rm</span> -rf data</span><br></pre></td></tr></table></figure></li>
<li><p>使用 <strong>hadoop namenode -format</strong> 格式化工具自动生成带版本号文件的目录</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop --config /opt/module/hadoop-2.7.2/etc/hadoop/myconfig/ namenode -format</span><br></pre></td></tr></table></figure></li>
<li><p>我们发现在执行完这条命令后会根据配置文件自动帮我们创建一个 <strong>data</strong> 目录，并在 <code>data</code> 目录下自动帮我们创建了一个 <strong>dfs</strong> 文件，这个 <code>dfs</code> 文件就是存储 NameNode 和 DataNode 运行时数据的目录。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ ll data/</span><br><span class="line">总用量 0</span><br><span class="line">drwxrwxr-x. 3 lvnengdong lvnengdong 18 11月 21 22:53 dfs</span><br></pre></td></tr></table></figure></li>
<li><p>重新启动 NameNode</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 myconfig]$ hadoop-daemon.sh --config /opt/module/hadoop-2.7.2/etc/hadoop/myconfig/ start namenode</span><br></pre></td></tr></table></figure></li>
<li><p>使用 jps 命令查看，发现此时 NameNode 进程已经成功启动了。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ jps</span><br><span class="line">94994 Jps</span><br><span class="line">94543 NameNode</span><br></pre></td></tr></table></figure></li>
<li><p>还可以通过浏览器界面来访问 NameNode</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122102444283.png" alt="image-20211122102444283"></p>
</li>
<li><p>使用当前配置文件启动一个 NameNode</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 myconfig] $ hadoop-daemon.sh --config /opt/module/hadoop-2.7.2/etc/hadoop/myconfig/ start datanode</span><br></pre></td></tr></table></figure></li>
<li><p>查看 DataNode 是否成功启动</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ jps</span><br><span class="line">50678 DataNode</span><br><span class="line">51053 Jps</span><br><span class="line">94543 NameNode</span><br></pre></td></tr></table></figure></li>
<li><p>或者在浏览器中查看 DataNode 的启动情况</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122104518498.png" alt="image-20211122104518498"></p>
</li>
<li><p>当 NameNode 和 DataNode 都启动成功后，执行上传文件命令</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop --config /opt/module/hadoop-2.7.2/etc/hadoop/myconfig/ fs -put /opt/module/hadoop-2.7.2/etc/hadoop/testUpload /</span><br><span class="line"><span class="comment"># 表示加载 myconfig 目录下的配置文件，完成将 testUpload 文件上传到 HDFS 的根目录（/）下</span></span><br></pre></td></tr></table></figure></li>
<li><p>上传完成后，通过浏览器查看是否上传成功</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122105055584.png" alt="image-20211122105055584"></p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122105640671.png" alt="image-20211122105640671"></p>
</li>
<li><p>也可以在 HDFS 指定的保存数据的目录下查看上传的文件信息</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ tree /opt/module/hadoop-2.7.2/data/</span><br><span class="line">/opt/module/hadoop-2.7.2/data/</span><br><span class="line">└── dfs</span><br><span class="line">    ├── data	<span class="comment"># data 目录用于保存 DataNode 节点的数据文件</span></span><br><span class="line">    │   ├── current</span><br><span class="line">    │   │   ├── BP-401557870-192.168.1.102-1637506411781</span><br><span class="line">    │   │   │   ├── current</span><br><span class="line">    │   │   │   │   ├── finalized</span><br><span class="line">    │   │   │   │   │   └── subdir0</span><br><span class="line">    │   │   │   │   │       └── subdir0</span><br><span class="line">    │   │   │   │   │           ├── blk_1073741825	<span class="comment"># block块的id，与浏览器中展示的块的id一致</span></span><br><span class="line">    │   │   │   │   │           └── blk_1073741825_1001.meta</span><br><span class="line">    │   │   │   │   ├── rbw</span><br><span class="line">    │   │   │   │   └── VERSION</span><br><span class="line">    │   │   │   ├── scanner.cursor</span><br><span class="line">    │   │   │   └── tmp</span><br><span class="line">    │   │   └── VERSION</span><br><span class="line">    │   └── in_use.lock</span><br><span class="line">    └── name	<span class="comment"># name 目录用于保存 NameNode 节点的数据文件</span></span><br><span class="line">        ├── current</span><br><span class="line">        │   ├── edits_inprogress_0000000000000000001</span><br><span class="line">        │   ├── fsimage_0000000000000000000</span><br><span class="line">        │   ├── fsimage_0000000000000000000.md5</span><br><span class="line">        │   ├── seen_txid</span><br><span class="line">        │   └── VERSION</span><br><span class="line">        └── in_use.lock</span><br></pre></td></tr></table></figure>

</li>
<li><p>继续查看 <code>blk_1073741825</code> 块文件的信息，发现内容与我们之前写的 testUpLoad 文件一致</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ <span class="built_in">cat</span> /opt/module/hadoop-2.7.2/data/dfs/data/current/BP-401557870-192.168.1.102-1637506411781/current/finalized/subdir0/subdir0/blk_1073741825</span><br><span class="line">测试上传文件</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h1 id="本地-MapReduce-测试"><a href="#本地-MapReduce-测试" class="headerlink" title="本地 MapReduce 测试"></a>本地 MapReduce 测试</h1><p>在 <code>$HADOOP_HOME/share/hadoop/mapreduce/</code> 目录下有一个自带的 MapReduce 测试案例 <code>hadoop-mapreduce-examples-2.7.2.jar</code>，我们可以直接调用这个 jar 包进行测试。</p>
<ol>
<li><p>进入到实例程序 jar 包所在的目录</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ <span class="built_in">cd</span> share/hadoop/mapreduce/</span><br></pre></td></tr></table></figure></li>
<li><p>尝试执行这个 jar 包</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.2.jar </span><br><span class="line">An example program must be given as the first argument. <span class="comment"># 实例程序必须给一个参数，这个参数就是要执行该jar包下的哪个方法</span></span><br><span class="line">Valid program names are:</span><br><span class="line">  aggregatewordcount: An Aggregate based map/reduce program that counts the words <span class="keyword">in</span> the input files.</span><br><span class="line">  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words <span class="keyword">in</span> the input files.</span><br><span class="line">  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.</span><br><span class="line">  dbcount: An example job that count the pageview counts from a database.</span><br><span class="line">  distbbp: A map/reduce program that uses a BBP-<span class="built_in">type</span> formula to compute exact bits of Pi.</span><br><span class="line">  grep: A map/reduce program that counts the matches of a regex <span class="keyword">in</span> the input.</span><br><span class="line">  <span class="built_in">join</span>: A job that effects a <span class="built_in">join</span> over sorted, equally partitioned datasets</span><br><span class="line">  multifilewc: A job that counts words from several files.</span><br><span class="line">  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.</span><br><span class="line">  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.</span><br><span class="line">  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.</span><br><span class="line">  randomwriter: A map/reduce program that writes 10GB of random data per node.</span><br><span class="line">  secondarysort: An example defining a secondary <span class="built_in">sort</span> to the reduce.</span><br><span class="line">  <span class="built_in">sort</span>: A map/reduce program that sorts the data written by the random writer.</span><br><span class="line">  sudoku: A sudoku solver.</span><br><span class="line">  teragen: Generate data <span class="keyword">for</span> the terasort</span><br><span class="line">  terasort: Run the terasort</span><br><span class="line">  teravalidate: Checking results of terasort</span><br><span class="line">  wordcount: A map/reduce program that counts the words <span class="keyword">in</span> the input files.	<span class="comment"># 单词统计方法</span></span><br><span class="line">  wordmean: A map/reduce program that counts the average length of the words <span class="keyword">in</span> the input files.</span><br><span class="line">  wordmedian: A map/reduce program that counts the median length of the words <span class="keyword">in</span> the input files.</span><br><span class="line">  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words <span class="keyword">in</span> the input files.</span><br></pre></td></tr></table></figure></li>
<li><p>尝试执行该 jar 包下的 wordcount 方法</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount</span><br><span class="line">Usage: wordcount &lt;<span class="keyword">in</span>&gt; [&lt;<span class="keyword">in</span>&gt;...] &lt;out&gt;</span><br><span class="line"><span class="comment"># 调用 wordcount 方法必须传递一个或多个源文件路径和一个统计结果保存的路径</span></span><br><span class="line"><span class="comment"># 源文件路径和统计结果保存的路径都是HDFS上的路径</span></span><br></pre></td></tr></table></figure></li>
<li><p>当前 HDFS 的 NameNode 的根目录下有一个 <code>testUpLoad</code> 文件，我们可以统计该文件中单词的个数</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount / /result</span><br><span class="line"><span class="comment"># 调用 wordcount 方法统计根目录 `/` 下的所有文件中的单词个数，并把结果输出到 `/result` 目录下 </span></span><br></pre></td></tr></table></figure></li>
<li><p>查看浏览器发现根目录下多了一个 <code>/result</code> 目录</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122173553541.png" alt="image-20211122173553541"></p>
</li>
<li><p><code>/result</code> 目录</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122173729381.png" alt="image-20211122173729381"></p>
</li>
<li><p><code>part-r-00000</code>，</p>
<p> 我们可以将这个文件下载到本地查看，发现该文件就是统计了 <code>/</code> 目录下每个单词出现的次数。</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122174007581.png" alt="image-20211122174007581"></p>
</li>
</ol>
<hr>
<h1 id="在-YARN-上运行-MapReduce"><a href="#在-YARN-上运行-MapReduce" class="headerlink" title="在 YARN 上运行 MapReduce"></a>在 YARN 上运行 MapReduce</h1><h2 id="1、修改-MapReduce-的运行模式"><a href="#1、修改-MapReduce-的运行模式" class="headerlink" title="1、修改 MapReduce 的运行模式"></a>1、修改 MapReduce 的运行模式</h2><ol>
<li><p>默认的 MapReduce 的运行模式是本地模式。我们可以通过 MapReduce 的默认配置文件 <code>mapred-default.xml</code> 来查看。</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>local<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The runtime framework for executing MapReduce jobs.</span><br><span class="line">  Can be one of local, classic or yarn. </span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">	运行模式可以是 local、classic、yarn 中的任意一个，默认选择的 local</span></span><br><span class="line"><span class="comment">		（1）local 是本地模式</span></span><br><span class="line"><span class="comment">		（2）classic 是Hadoop1.x 版本提供的一种运行模式</span></span><br><span class="line"><span class="comment">		（3）yarn 是将MR程序交给YARN去执行</span></span><br><span class="line"><span class="comment">	--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>如果我们想要通过 Yarn 来执行 MapReduce 程序，就需要在自定义的配置文件中重写这个配置信息。在 <code>$HADOOP_HOME/etc/hadoop/</code> 目录下创建并修改 <code>mapred-site.xml</code> 文件的信息，添加如下配置：</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">     将 MapReduce 从本地模式改为在 YARN 上运行</span></span><br><span class="line"><span class="comment"> --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2、启动-Yarn"><a href="#2、启动-Yarn" class="headerlink" title="2、启动 Yarn"></a>2、启动 Yarn</h2><p><strong>配置 ResourceManager 的通信地址</strong></p>
<p>Yarn 由 ResourceManager 和 NodeManager 线程组成。同理，Yarn 作为一个服务器集群，其中的 ResourceManager 既要负责和客户端通信，又要负责和 NodeManager 通信，所以 ResourceManager 必须对外暴露自己的服务地址。这一信息要在 <code>yarn-default.xml</code> 配置文件中进行修改。修改 <code>$HADOOP_HOME/etc/hadoop/yarn-site.xml</code> 文件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">     配置 Yarn 的 ResourceManager 的服务地址</span></span><br><span class="line"><span class="comment"> --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p><strong>配置 Reducer 获取数据的方式</strong></p>
<ul>
<li>  MapReduce 思想处理数据的方式是将任务拆分执行，最后将结果合并，这时候就可能出现多个 Map 间需要进行数据共享的问题，那么这多个 Map 之间数据传递的方式就叫做 shuffle，所以我们必须配置这一属性，否则在 Yarn 上运行 MR 程序时就会报错。</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">     配置 Reducer 获取数据的方式</span></span><br><span class="line"><span class="comment">     Map 阶段处理好的数据最后要传递到 Reducer 上使用。</span></span><br><span class="line"><span class="comment">     而传递数据的方式称为 shuffle ，所以 YARN 中必须配置shuffle 才能正常运行</span></span><br><span class="line"><span class="comment"> --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>





<p><strong>启动</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、启动 RM</span></span><br><span class="line">yarn-daemon.sh start resourcemanager</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、启动 NM</span></span><br><span class="line">yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure>



<p><strong>查看</strong></p>
<ol>
<li><p>通过 jps 命令查看</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ jps</span><br><span class="line">63025 NodeManager</span><br><span class="line">35193 NameNode</span><br><span class="line">36077 DataNode</span><br><span class="line">62367 ResourceManager</span><br><span class="line">64799 Jps</span><br></pre></td></tr></table></figure></li>
<li><p>通过浏览器查看</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">访问：http://&#123;ResourceManager线程所在机器的IP&#125;:8088</span><br><span class="line"><span class="comment"># ResourceManager 对外暴露的 http 端口号是 8088</span></span><br></pre></td></tr></table></figure>

<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122183127209.png" alt="image-20211122183127209"></p>
</li>
</ol>
<hr>
<h2 id="3、提交任务"><a href="#3、提交任务" class="headerlink" title="3、提交任务"></a>3、提交任务</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar  jar包  主类名 参数&#123;多个输入目录，一个输出目录&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li><p>在<code>/opt/module/hadoop-2.7.2/share/hadoop/mapreduce</code> 目录下创建一个文件<code>testMR</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 mapreduce]$ vim testMR</span><br></pre></td></tr></table></figure></li>
<li><p>编辑该文件的内容为：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">test</span></span><br><span class="line">MR</span><br><span class="line">demo</span><br></pre></td></tr></table></figure></li>
<li><p>将该文件上传到 HDFS 的<code>/mr</code> 目录下</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 mapreduce]$ hadoop fs -put ./testMR /mr</span><br></pre></td></tr></table></figure></li>
<li><p>调用 wordcount 方法统计目录 <code>/mr</code> 下的所有文件中的单词个数，并把结果输出到 <code>/count</code> 目录下</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount /mr /count</span><br></pre></td></tr></table></figure></li>
<li><p>通过浏览器查看执行结果。</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122194846938.png" alt="image-20211122194846938"></p>
</li>
</ol>
<p><strong>注意事项：</strong></p>
<ol>
<li> 更改配置信息后需要重启 ResourceManager 和 NodeManager！</li>
<li> 输入目录中必须全部是文件，不能有目录。如果有目录的话会报错！</li>
<li> 输出目录必须不存在！</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  





</body>
</html>
