<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/7/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/7/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/7/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Hexo</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Hexo</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">235</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">69</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/20/Hive%EF%BC%88%E4%B8%8B%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/20/Hive%EF%BC%88%E4%B8%8B%EF%BC%89/" class="post-title-link" itemprop="url">Hive（下）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-20 21:10:26" itemprop="dateCreated datePublished" datetime="2021-12-20T21:10:26+08:00">2021-12-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-12-22 09:38:28" itemprop="dateModified" datetime="2021-12-22T09:38:28+08:00">2021-12-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hive/" itemprop="url" rel="index"><span itemprop="name">Hive</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="第-7-章-函数"><a href="#第-7-章-函数" class="headerlink" title="第 7 章    函数"></a>第 7 章    函数</h1><h2 id="7-1-函数分类"><a href="#7-1-函数分类" class="headerlink" title="7.1    函数分类"></a>7.1    函数分类</h2><p>在 Hive 中，按照函数的来源可以将函数分为两类：</p>
<ol>
<li> 系统函数</li>
<li> 自定义函数</li>
</ol>
<h3 id="系统函数："><a href="#系统函数：" class="headerlink" title="系统函数："></a>系统函数：</h3><h3 id="用户自定义函数："><a href="#用户自定义函数：" class="headerlink" title="用户自定义函数："></a>用户自定义函数：</h3><ol>
<li><p>遵守 hive 函数接口的要求，自定义一个函数类；</p>
</li>
<li><p>将函数类打成jar包，保存在 Hive 的 lib 目录下，或者保存在 <code>HIVE_HOME/auxlib</code> 目录下</p>
<ul>
<li>auxlib 目录是用来存放 hive 可以加载的第三方 jar 包的目录</li>
<li>Hive 在启动时会加载该目录下的 jar 包，提供额外的第三方功能</li>
</ul>
</li>
<li><p>创建一个函数，让这个函数和 jar 中的函数类关联</p>
</li>
<li><p>使用函数</p>
</li>
</ol>
<p>因为在 Hive 中可以实现自定义的函数，所以函数有库的概念，当然系统提供的函数可以在任意库下使用。</p>
<h2 id="7-1-查看函数"><a href="#7-1-查看函数" class="headerlink" title="7.1    查看函数"></a>7.1    查看函数</h2><ol>
<li><p>查看当前库中的所有函数</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show functions;</span><br></pre></td></tr></table></figure></li>
<li><p>查看函数的用法</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc function upper;</span><br></pre></td></tr></table></figure></li>
<li><p>查看函数的用法（详情）</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc function extended upper;</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><h3 id="常用日期函数"><a href="#常用日期函数" class="headerlink" title="常用日期函数"></a>常用日期函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">	hive默认解析的时间格式必须是： 2019-11-24 08:09:10</span><br><span class="line">unix_timestamp:返回当前或指定时间的时间戳（日期=&gt;时间戳）</span><br><span class="line">from_unixtime：将时间戳转为日期格式（时间戳=&gt;日期）</span><br><span class="line">current_date：当前日期</span><br><span class="line">current_timestamp：当前的日期加时间</span><br><span class="line">* to_date：抽取日期部分</span><br><span class="line">year：获取年</span><br><span class="line">month：获取月</span><br><span class="line">day：获取日</span><br><span class="line">hour：获取时</span><br><span class="line">minute：获取分</span><br><span class="line">second：获取秒</span><br><span class="line">weekofyear：当前时间是一年中的第几周</span><br><span class="line">dayofmonth：当前时间是一个月中的第几天</span><br><span class="line">* months_between： 两个日期间的月份，前-后</span><br><span class="line">* add_months：日期加减月</span><br><span class="line">* datediff：两个日期相差的天数，前-后</span><br><span class="line">* date_add：日期加天数</span><br><span class="line">* date_sub：日期减天数</span><br><span class="line">* last_day：日期的当月的最后一天</span><br><span class="line"></span><br><span class="line">date_format格式化日期   date_format( 2019-11-24 08:09:10,&#x27;yyyy-MM&#x27;) mn</span><br></pre></td></tr></table></figure>



<h3 id="常用取整函数"><a href="#常用取整函数" class="headerlink" title="常用取整函数"></a>常用取整函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">round： 四舍五入</span><br><span class="line">ceil：  向上取整</span><br><span class="line">floor： 向下取整</span><br></pre></td></tr></table></figure>



<h3 id="常用字符串操作函数"><a href="#常用字符串操作函数" class="headerlink" title="常用字符串操作函数"></a>常用字符串操作函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">upper： 转大写</span><br><span class="line">lower： 转小写</span><br><span class="line">length： 长度</span><br><span class="line">* trim：  前后去空格</span><br><span class="line">lpad： 向左补齐，到指定长度</span><br><span class="line">rpad：  向右补齐，到指定长度</span><br><span class="line">* regexp_replace： SELECT regexp_replace(&#x27;100-200&#x27;, &#x27;(\d+)&#x27;, &#x27;num&#x27;)=&#x27;num-num</span><br><span class="line">	使用正则表达式匹配目标字符串，匹配成功后替换！</span><br></pre></td></tr></table></figure>



<h3 id="常用集合操作"><a href="#常用集合操作" class="headerlink" title="常用集合操作"></a>常用集合操作</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">size： 集合（map和list）中元素的个数</span><br><span class="line">map_keys： 返回map中的key</span><br><span class="line">map_values: 返回map中的value</span><br><span class="line">* array_contains: 判断array中是否包含某个元素</span><br><span class="line">sort_array： 将array中的元素排序</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="系统函数"><a href="#系统函数" class="headerlink" title="系统函数"></a>系统函数</h2><h3 id="NVL"><a href="#NVL" class="headerlink" title="NVL"></a>NVL</h3><ol>
<li> 语法： <code>NVL( str1, replace_with)</code></li>
<li> 说明：判断 <code>str1</code> 是否为 <code>NULL</code>，如果不为 <code>NULL</code> 则 返回原值，如果为 <code>NULL</code> 则返回 <code>replace_with</code> 的值。</li>
<li>使用场景：<ul>
<li>  ① 将 <code>NULL</code> 值替换为默认值</li>
<li>  ② 在使用一些统计函数时，如果统计函数会忽略 <code>NULL</code> 值，则使用该函数对 <code>NULL</code> 进行替换。（对于一些统计函数，例如 <code>avg()</code>，如果某一行的值为 <code>NULL</code>，在统计平均值时就会忽略该行，导致总行数减少，统计到的值自然也是不准确的。为了解决这个问题，一般会用 0 值代替 <code>NULL</code>）</li>
</ul>
</li>
</ol>
<p><strong>Demo</strong></p>
<p>数据准备：采用员工表</p>
<p><code>emp.txt</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">7369	SMITH	CLERK	7902	1980-12-17	800.00		20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-2-20	1600.00	300.00	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-2-22	1250.00	500.00	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-4-2	2975.00		20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-9-28	1250.00	1400.00	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-5-1	2850.00		30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.00		10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-4-19	3000.00		20</span><br><span class="line">7839	KING	PRESIDENT		1981-11-17	5000.00		10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-9-8	1500.00	0.00	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-5-23	1100.00		20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-3	950.00		30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-3	3000.00		20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.00		10</span><br></pre></td></tr></table></figure>





<ol>
<li><p>查询：如果员工的 <code>comm</code> 为 NULL，则用 <code>-1</code> 代替</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select nvl(comm,-1) from emp;</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">20.0</span><br><span class="line">300.0</span><br><span class="line">500.0</span><br><span class="line">-1.0</span><br><span class="line">1400.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">0.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br><span class="line">-1.0</span><br></pre></td></tr></table></figure>

</li>
<li><p>查询：如果员工的 <code>comm</code> 为 <code>NULL</code>，则用领导 <code>id</code> 代替</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select nvl(comm,mgr) from emp;</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">20.0</span><br><span class="line">300.0</span><br><span class="line">500.0</span><br><span class="line">7839.0</span><br><span class="line">1400.0</span><br><span class="line">7839.0</span><br><span class="line">7839.0</span><br><span class="line">7566.0</span><br><span class="line">NULL</span><br><span class="line">0.0</span><br><span class="line">7788.0</span><br><span class="line">7698.0</span><br><span class="line">7566.0</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="concat"><a href="#concat" class="headerlink" title="concat"></a>concat</h3><ol>
<li> 语法： <code>concat(str1, str2, ... strN)</code></li>
<li> 说明：字符串拼接。</li>
<li> 注意事项：一旦参数中存在一个字符串为 NULL，则返回 NULL</li>
</ol>
<hr>
<h3 id="concat-ws"><a href="#concat-ws" class="headerlink" title="concat_ws"></a>concat_ws</h3><ol>
<li><p> 语法：<code>concat_ws(separator, [string | array(string)]+)</code></p>
</li>
<li><p> 说明：拼接字符串或者字符串数组，并以指定的分隔符分隔。</p>
</li>
<li><p>Demo：</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT concat_ws(&#x27;.&#x27;, &#x27;www&#x27;, array(&#x27;facebook&#x27;, &#x27;com&#x27;)) FROM src LIMIT 1;</span><br><span class="line"></span><br><span class="line">&#x27;www.facebook.com&#x27;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h3 id="判断函数"><a href="#判断函数" class="headerlink" title="判断函数"></a>判断函数</h3><h4 id="CASE…WHEN"><a href="#CASE…WHEN" class="headerlink" title="CASE…WHEN"></a>CASE…WHEN</h4><ol>
<li><p>语法：</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">CASE 列名 </span><br><span class="line">	WHEN value1 THEN value2</span><br><span class="line">	WHEN value3 THEN value4</span><br><span class="line">	...</span><br><span class="line">	else valueN</span><br><span class="line">	end</span><br><span class="line">	;</span><br><span class="line"># 说明：</span><br><span class="line">#	1、对于该列的字段来说，如果 字段=value1，就返回 value2；</span><br><span class="line">#	如果 字段=value3，就返回 value4；可以有多个；</span><br><span class="line">#	2、如果找不到对应值，则返回 valueN</span><br><span class="line">#	3、最后以 end 结尾</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="if-函数"><a href="#if-函数" class="headerlink" title="if 函数"></a>if 函数</h4><ol>
<li><p>语法：</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if(判断表达式, value1, value2        )</span><br><span class="line"></span><br><span class="line"># 说明：</span><br><span class="line">#	表达式返回一个 Boolean 值，如果该值为 true，则返回 value1，否则返回 value2</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><p><strong>一、数据准备</strong></p>
<table>
<thead>
<tr>
<th align="center">name</th>
<th align="center">dept_id</th>
<th align="center">sex</th>
</tr>
</thead>
<tbody><tr>
<td align="center">悟空</td>
<td align="center">A</td>
<td align="center">男</td>
</tr>
<tr>
<td align="center">大海</td>
<td align="center">A</td>
<td align="center">男</td>
</tr>
<tr>
<td align="center">宋宋</td>
<td align="center">B</td>
<td align="center">男</td>
</tr>
<tr>
<td align="center">凤姐</td>
<td align="center">A</td>
<td align="center">女</td>
</tr>
<tr>
<td align="center">婷姐</td>
<td align="center">B</td>
<td align="center">女</td>
</tr>
<tr>
<td align="center">婷婷</td>
<td align="center">B</td>
<td align="center">女</td>
</tr>
</tbody></table>
<p><strong>二、需求：</strong></p>
<p>求出各个部门的男女各有多少人。结果应该如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 部门	男生人数	女生人数</span><br><span class="line">A	2	1</span><br><span class="line">B	1	2</span><br></pre></td></tr></table></figure>





<p>三、创建本地 <code>emp_sex.txt</code>，导入数据</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 datas]$ vim emp_sex.txt</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">悟空	A	男</span><br><span class="line">大海	A	男</span><br><span class="line">宋宋	B	男</span><br><span class="line">凤姐	A	女</span><br><span class="line">婷姐	B	女</span><br><span class="line">婷婷	B	女</span><br></pre></td></tr></table></figure>







<p><strong>四、创建 <code>hive</code> 表并导入数据</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 1、创建 Hive 表</span><br><span class="line">create table emp_sex</span><br><span class="line">	(</span><br><span class="line">        name string, </span><br><span class="line">        dept_id string, </span><br><span class="line">        sex string</span><br><span class="line">    ) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"># 2、导入数据</span><br><span class="line">load data local inpath &#x27;/opt/module/datas/emp_sex.txt&#x27; into table emp_sex;</span><br></pre></td></tr></table></figure>





<p><strong>五、按需求查询数据</strong></p>
<ul>
<li>  分析</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># 部门	男生人数	女生人数</span><br><span class="line">A	2	1</span><br><span class="line">B	1	2</span><br><span class="line"></span><br><span class="line">select * from emp_sex</span><br><span class="line">group by dept_id;</span><br><span class="line"></span><br><span class="line">要得到每个部门的男生和女生人数，需要按照部门将所有员工划分成两部分</span><br><span class="line">        悟空	A	男</span><br><span class="line">        大海	A	男</span><br><span class="line">        凤姐	A	女</span><br><span class="line"></span><br><span class="line">        宋宋	B	男</span><br><span class="line">        婷姐	B	女</span><br><span class="line">        婷婷	B	女</span><br><span class="line"></span><br><span class="line">在分组前，要分别得到男生人数和女生人数，可以通过两个子查询来实现</span><br><span class="line"># 得到男生的人数</span><br><span class="line">select count(*) as male_count, dept_id from emp_sex</span><br><span class="line">where sex=&quot;男&quot;</span><br><span class="line">group by dept_id;</span><br><span class="line"></span><br><span class="line"># 得到女生的人数</span><br><span class="line">select count(*) as female_count, dept_id from emp_sex</span><br><span class="line">where sex=&quot;女&quot;</span><br><span class="line">group by dept_id;</span><br><span class="line"></span><br><span class="line"># 将上面两个结果拼接起来</span><br><span class="line">select t1.dept_id, male_count, female_count</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">    select count(*) as male_count, dept_id from emp_sex</span><br><span class="line">	where sex=&quot;男&quot;</span><br><span class="line">	group by dept_id</span><br><span class="line">) t1,</span><br><span class="line">(</span><br><span class="line">    select count(*) as female_count, dept_id from emp_sex</span><br><span class="line">    where sex=&quot;女&quot;</span><br><span class="line">    group by dept_id</span><br><span class="line">) t2</span><br><span class="line">where t1.dept_id = t2.dept_id</span><br><span class="line">;</span><br></pre></td></tr></table></figure>



<p><strong>优化</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">  dept_id,</span><br><span class="line">  sum(case sex when &#x27;男&#x27; then 1 else 0 end) male_count,</span><br><span class="line">  sum(case sex when &#x27;女&#x27; then 1 else 0 end) female_count</span><br><span class="line">from </span><br><span class="line">  emp_sex</span><br><span class="line">group by</span><br><span class="line">  dept_id;</span><br><span class="line">  </span><br><span class="line"># sum 函数是在 group by 之后执行</span><br></pre></td></tr></table></figure>





<hr>
<h3 id="6-7-2-行转列"><a href="#6-7-2-行转列" class="headerlink" title="6.7.2    行转列"></a>6.7.2    行转列</h3><p><strong>行转列：</strong> <code>一列N行 ==&gt; 一列一行</code></p>
<p><strong>一、相关函数说明</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CONCAT(string A/col, string B/col...)</span><br></pre></td></tr></table></figure>

<ul>
<li>  返回输入字符串拼接后的结果，支持任意个输入字符串</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CONCAT_WS(separator, str1, str2,...)</span><br></pre></td></tr></table></figure>

<ul>
<li>  它是一个特殊形式的 <code>CONCAT()</code>。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">COLLECT_SET(col)</span><br><span class="line"># 返回一个去重的 Set 集合</span><br><span class="line"># col 列名</span><br><span class="line"># 作用：将此列的所有记录封装到一个 Set 集合中</span><br></pre></td></tr></table></figure>

<ul>
<li>  函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生 <code>array</code> 类型字段。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">COLLECT_LIST(col)</span><br><span class="line"># 返回一个 List 集合，允许重复</span><br><span class="line"># col 列名</span><br><span class="line"># 作用：将此列的所有记录封装到一个 List 集合中</span><br></pre></td></tr></table></figure>





<p><strong>数据准备</strong></p>
<table>
<thead>
<tr>
<th>name</th>
<th>constellation</th>
<th>blood_type</th>
</tr>
</thead>
<tbody><tr>
<td>孙悟空</td>
<td>白羊座</td>
<td>A</td>
</tr>
<tr>
<td>大海</td>
<td>射手座</td>
<td>A</td>
</tr>
<tr>
<td>宋宋</td>
<td>白羊座</td>
<td>B</td>
</tr>
<tr>
<td>猪八戒</td>
<td>白羊座</td>
<td>A</td>
</tr>
<tr>
<td>凤姐</td>
<td>射手座</td>
<td>A</td>
</tr>
</tbody></table>
<p><strong>需求</strong></p>
<p>把星座和血型一样的人归类到一起。结果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">射手座,A            大海|凤姐</span><br><span class="line">白羊座,A            孙悟空|猪八戒</span><br><span class="line">白羊座,B            宋宋</span><br></pre></td></tr></table></figure>

<p><strong>分析：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 1、按照 星座 和 血型 两个条件对数据进行分组</span><br><span class="line">select concat(constellation, &quot;,&quot;, blood_type)</span><br><span class="line">from person_info</span><br><span class="line">group by constellation, blood_type;</span><br><span class="line"></span><br><span class="line"># 临时结果：得到三个分组</span><br><span class="line">射手座,A</span><br><span class="line">白羊座,A</span><br><span class="line">白羊座,B</span><br><span class="line"></span><br><span class="line"># 2、把分组内的 name 数据（可能有多条）（一列N行）封装成一列一行</span><br><span class="line">select concat(constellation, &quot;,&quot;, blood_type) as cb, CONCAT_WS(&quot;|&quot;, COLLECT_LIST(name)) as nameSet</span><br><span class="line">from person_info</span><br><span class="line">group by constellation, blood_type;</span><br><span class="line"></span><br><span class="line"># select 后面只能使用分组后的字段，及聚集函数。COLLECT_LIST()函数也是一个聚集函数</span><br></pre></td></tr></table></figure>





<ol>
<li><p>创建本地 <code>person_info.txt</code>，导入数据</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 datas]$ vim person_info.txt</span><br><span class="line"></span><br><span class="line">孙悟空	白羊座	A</span><br><span class="line">大海	射手座	A</span><br><span class="line">宋宋	白羊座	B</span><br><span class="line">猪八戒	白羊座	A</span><br><span class="line">凤姐	射手座	A</span><br></pre></td></tr></table></figure>

</li>
<li><p>创建 <code>Hive</code> 表并导入数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 创建 Hive 表</span><br><span class="line">create table person_info</span><br><span class="line">(</span><br><span class="line">    name string, </span><br><span class="line">    constellation string, </span><br><span class="line">    blood_type string</span><br><span class="line">) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"># 导入数据</span><br><span class="line">load data local inpath &quot;/opt/module/datas/person_info.txt&quot; into table person_info;</span><br></pre></td></tr></table></figure>

</li>
<li><p>按需求查询数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select concat(constellation, &quot;,&quot;, blood_type) as cb, CONCAT_WS(&quot;|&quot;, COLLECT_LIST(name)) as nameSet</span><br><span class="line">from person_info</span><br><span class="line">group by constellation, blood_type;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h3><p><strong>列转行：</strong><code>一行一列 ==&gt; 一行N列</code></p>
<p><strong>常见函数：</strong></p>
<ol>
<li><p>函数说明</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">EXPLODE(col)：</span><br><span class="line"># 作用：将 Array 类型的参数拆分为 1 列 N 行，或将 Map 类型的参数拆分成 2 列 N 行。</span><br><span class="line"># col：列名，要求参数类型只能是 Array 类型或 Map 类型</span><br><span class="line"># EXPLODE 属于 UDTF，UDTF 在使用时，不能和其它表达式一起出现在 select 子句后，即 UDTF 只能单独出现在 select 子句后。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 侧写</span><br><span class="line">LATERAL VIEW	</span><br><span class="line"></span><br><span class="line"># 用法</span><br><span class="line">LATERAL VIEW udtf(expression) tableAlias AS columnAlias</span><br><span class="line"># tableAlias 临时表名</span><br><span class="line"># columnAlias	临时列名</span><br><span class="line"></span><br><span class="line"># 解释</span><br><span class="line">用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>数据准备</p>
<table>
<thead>
<tr>
<th>movie</th>
<th>category</th>
</tr>
</thead>
<tbody><tr>
<td>《疑犯追踪》</td>
<td>悬疑,动作,科幻,剧情</td>
</tr>
<tr>
<td>《Lie  to me》</td>
<td>悬疑,警匪,动作,心理,剧情</td>
</tr>
<tr>
<td>《战狼2》</td>
<td>战争,动作,灾难</td>
</tr>
</tbody></table>
<p>需求：将电影分类中的数组数据展开。结果如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 期望结果</span></span><br><span class="line"></span><br><span class="line">《疑犯追踪》	悬疑</span><br><span class="line">《疑犯追踪》	动作</span><br><span class="line">《疑犯追踪》	科幻</span><br><span class="line">《疑犯追踪》	剧情</span><br><span class="line">《Lie <span class="keyword">to</span> me》	悬疑</span><br><span class="line">《Lie <span class="keyword">to</span> me》	警匪</span><br><span class="line">《Lie <span class="keyword">to</span> me》	动作</span><br><span class="line">《Lie <span class="keyword">to</span> me》	心理</span><br><span class="line">《Lie <span class="keyword">to</span> me》	剧情</span><br><span class="line">《战狼<span class="number">2</span>》	战争</span><br><span class="line">《战狼<span class="number">2</span>》	动作</span><br><span class="line">《战狼<span class="number">2</span>》	灾难</span><br></pre></td></tr></table></figure>

<p><strong>分析：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">在原始数据中，category 是数组类型数据，可以将数组中的元素都拆出来，再与对应的电影名称做笛卡尔积。</span><br><span class="line"></span><br><span class="line"># 1、把集合转为多个元素</span><br><span class="line">select explode(category)</span><br><span class="line">from movie_info;</span><br><span class="line"></span><br><span class="line"># 2、将第一步的查询结果作为一张临时表，再与 movie_info 表进行关联</span><br><span class="line">select </span><br><span class="line">from movie_info</span><br><span class="line">join</span><br><span class="line">select explode(category)</span><br><span class="line">from movie_info</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"># 3、需要将 explode 拆分后的一列N行，在逻辑上依然视为一列一行。和 movie_info 做表关联。这个行为在 Hive 中叫做侧写（lateral view）</span><br></pre></td></tr></table></figure>





<p><strong>实现</strong></p>
<ol>
<li><p>创建本地 <code>movie.txt</code>，导入数据</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 datas]$ vim movie.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件内容</span></span><br><span class="line">《疑犯追踪》	悬疑,动作,科幻,剧情</span><br><span class="line">《Lie to me》	悬疑,警匪,动作,心理,剧情</span><br><span class="line">《战狼2》	战争,动作,灾难</span><br></pre></td></tr></table></figure>

</li>
<li><p>创建 <code>hive</code> 表，并导入数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 创建 hive 表</span><br><span class="line">create table movie_info</span><br><span class="line">(</span><br><span class="line">    movie string, </span><br><span class="line">    category array&lt;string&gt;</span><br><span class="line">) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;</span><br><span class="line">collection items terminated by &quot;,&quot;;</span><br><span class="line"></span><br><span class="line"># 导入数据</span><br><span class="line">load data local inpath &quot;/opt/module/datas/movie.txt&quot; into table movie_info;</span><br></pre></td></tr></table></figure>

</li>
<li><p>按需求查询数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">    movie,</span><br><span class="line">    category_name</span><br><span class="line">from </span><br><span class="line">    movie_info lateral view explode(category) table_tmp as category_name;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h3 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h3><p>窗口函数：窗口 + 函数</p>
<ul>
<li>  窗口：函数运行时计算的数据集的范围</li>
<li>  函数：运行的函数</li>
</ul>
<p>注意：不是所有的函数在运行时都可以通过改变窗口的大小，来控制计算的数据集的范围。</p>
<p>格式：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">函数名 <span class="keyword">over</span>( <span class="keyword">partition</span> <span class="keyword">by</span> 字段, <span class="keyword">order</span> <span class="keyword">by</span> 字段, window_clause) </span><br></pre></td></tr></table></figure>





<ol>
<li><p>相关函数说明</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化</span><br><span class="line"></span><br><span class="line">CURRENT ROW：当前行</span><br><span class="line">n PRECEDING：往前n行数据</span><br><span class="line">n FOLLOWING：往后n行数据</span><br><span class="line">UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点</span><br><span class="line"></span><br><span class="line">LAG(col, n)：往前第n行数据</span><br><span class="line"></span><br><span class="line">LEAD(col, n)：往后第n行数据</span><br><span class="line"></span><br><span class="line">NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。</span><br></pre></td></tr></table></figure>

</li>
<li><p>数据准备</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># name，orderdate，cost</span><br><span class="line">jack,2017-01-01,10</span><br><span class="line">tony,2017-01-02,15</span><br><span class="line">jack,2017-02-03,23</span><br><span class="line">tony,2017-01-04,29</span><br><span class="line">jack,2017-01-05,46</span><br><span class="line">jack,2017-04-06,42</span><br><span class="line">tony,2017-01-07,50</span><br><span class="line">jack,2017-01-08,55</span><br><span class="line">mart,2017-04-08,62</span><br><span class="line">mart,2017-04-09,68</span><br><span class="line">neil,2017-05-10,12</span><br><span class="line">mart,2017-04-11,75</span><br><span class="line">neil,2017-06-12,80</span><br><span class="line">mart,2017-04-13,94</span><br></pre></td></tr></table></figure>

</li>
<li><p>需求：</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">（1）查询在2017年4月份购买过商品的顾客及总人数</span><br><span class="line">（2）查询顾客的购买明细及月购买总额</span><br><span class="line">（3）上述的场景,要将cost按照日期进行累加</span><br><span class="line">（4）查询顾客上次的购买时间</span><br><span class="line">（5）查询前20%时间的订单信息</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><strong>实现：</strong></p>
<ol>
<li><p>创建本地 <code>business.txt</code>，导入数据</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 datas]$ vim business.txt</span><br></pre></td></tr></table></figure></li>
<li><p>创建 <code>hive</code> 表并导入数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 创建 business 表</span><br><span class="line">create table business</span><br><span class="line">(</span><br><span class="line">    name string, </span><br><span class="line">    orderdate string,</span><br><span class="line">    cost int</span><br><span class="line">) </span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;;</span><br><span class="line"></span><br><span class="line"># 导入数据</span><br><span class="line">load data local inpath &quot;/opt/module/datas/business.txt&quot; into table business;</span><br></pre></td></tr></table></figure></li>
<li><p>按需求查询数据</p>
<ul>
<li>  （1）查询在2017年4月份购买过的顾客及总人数</li>
</ul>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select name, count(*) over() </span><br><span class="line">from business </span><br><span class="line">where substring(orderdate,1,7) = &#x27;2017-04&#x27; </span><br><span class="line">group by name;</span><br><span class="line"></span><br><span class="line"># count() 函数用于分组后，统计每个组内所有的数据</span><br><span class="line"># count(*) over() 添加窗口函数，统计分组后临时表的个数</span><br><span class="line"># group by 用于去重，一个用户多次购物不会增加人数</span><br><span class="line"># substring()	作用：截取字符串，从第一位开始，截取7位</span><br></pre></td></tr></table></figure>

<ul>
<li>  （2）查询顾客的购买明细及月购买总额</li>
</ul>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select name, orderdate, cost, sum(cost) over(partition by month(orderdate)) </span><br><span class="line">from business;</span><br></pre></td></tr></table></figure>

<ul>
<li>  （3）上述的场景,要将 <code>cost</code> 按照日期进行累加</li>
</ul>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select name,orderdate,cost, </span><br><span class="line">sum(cost) over() as sample1,--所有行相加 </span><br><span class="line">sum(cost) over(partition by name) as sample2,--按name分组，组内数据相加 </span><br><span class="line">sum(cost) over(partition by name order by orderdate) as sample3,--按name分组，组内数据累加 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,--和sample3一样,由起点到当前行的聚合 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, --当前行和前面一行做聚合 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行 </span><br><span class="line">from business;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>  （4）查询顾客上次的购买时间</li>
</ul>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select name,orderdate,cost, </span><br><span class="line">lag(orderdate,1,&#x27;1900-01-01&#x27;) over(partition by name order by orderdate ) as time1, lag(orderdate,2) over (partition by name order by orderdate) as time2 </span><br><span class="line">from business;</span><br></pre></td></tr></table></figure>

<ul>
<li>  （5）查询前 <code>20%</code> 时间的订单信息</li>
</ul>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select * from </span><br><span class="line">(</span><br><span class="line">    select name,orderdate,cost, ntile(5) over(order by orderdate) sorted</span><br><span class="line">    from business</span><br><span class="line">) t</span><br><span class="line">where sorted = 1;</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="排名函数"><a href="#排名函数" class="headerlink" title="排名函数"></a>排名函数</h3><p><strong>常用的排名函数</strong></p>
<ol>
<li><p><code>RANK()</code>    排序相同时会重复（允许并列），总数不会变</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 1 3 3 5</span><br></pre></td></tr></table></figure></li>
<li><p><code>DENSE_RANK()</code>    排序相同时会重复，总数会减少</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 1 2 2 3</span><br></pre></td></tr></table></figure></li>
<li><p><code>ROW_NUMBER()</code>    会根据顺序计算。就是行号</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 2 3 4 5</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><strong>数据准备</strong></p>
<table>
<thead>
<tr>
<th align="center">name</th>
<th align="center">subject</th>
<th align="center">score</th>
</tr>
</thead>
<tbody><tr>
<td align="center">孙悟空</td>
<td align="center">语文</td>
<td align="center">87</td>
</tr>
<tr>
<td align="center">孙悟空</td>
<td align="center">数学</td>
<td align="center">95</td>
</tr>
<tr>
<td align="center">孙悟空</td>
<td align="center">英语</td>
<td align="center">68</td>
</tr>
<tr>
<td align="center">大海</td>
<td align="center">语文</td>
<td align="center">94</td>
</tr>
<tr>
<td align="center">大海</td>
<td align="center">数学</td>
<td align="center">56</td>
</tr>
<tr>
<td align="center">大海</td>
<td align="center">英语</td>
<td align="center">84</td>
</tr>
<tr>
<td align="center">宋宋</td>
<td align="center">语文</td>
<td align="center">64</td>
</tr>
<tr>
<td align="center">宋宋</td>
<td align="center">数学</td>
<td align="center">86</td>
</tr>
<tr>
<td align="center">宋宋</td>
<td align="center">英语</td>
<td align="center">84</td>
</tr>
<tr>
<td align="center">婷婷</td>
<td align="center">语文</td>
<td align="center">65</td>
</tr>
<tr>
<td align="center">婷婷</td>
<td align="center">数学</td>
<td align="center">85</td>
</tr>
<tr>
<td align="center">婷婷</td>
<td align="center">英语</td>
<td align="center">78</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">孙悟空	语文	87</span><br><span class="line">孙悟空	数学	95</span><br><span class="line">孙悟空	英语	68</span><br><span class="line">大海	语文	94</span><br><span class="line">大海	数学	56</span><br><span class="line">大海	英语	84</span><br><span class="line">宋宋	语文	64</span><br><span class="line">宋宋	数学	86</span><br><span class="line">宋宋	英语	84</span><br><span class="line">婷婷	语文	65</span><br><span class="line">婷婷	数学	85</span><br><span class="line">婷婷	英语	78</span><br></pre></td></tr></table></figure>





<p>需求：计算每门学科成绩排名。</p>
<p><strong>实现：</strong></p>
<ol>
<li><p>创建本地 <code>score.txt</code>，导入数据</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 datas]$ vim score.txt</span><br></pre></td></tr></table></figure>

</li>
<li><p>创建 <code>Hive</code> 表并导入数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create table score</span><br><span class="line">(</span><br><span class="line">    name string,</span><br><span class="line">    subject string, </span><br><span class="line">    score int</span><br><span class="line">) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &#x27;/opt/module/datas/score.txt&#x27; into table score;</span><br></pre></td></tr></table></figure>

</li>
<li><p>按需求查询数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">	name,</span><br><span class="line">	subject,</span><br><span class="line">	score,</span><br><span class="line">rank() over(partition by subject order by score desc) rp,</span><br><span class="line">dense_rank() over(partition by subject order by score desc) drp,</span><br><span class="line">row_number() over(partition by subject order by score desc) rmp</span><br><span class="line">from score;</span><br><span class="line"># 排名前一般需要先排序</span><br><span class="line"></span><br><span class="line"># 查询结果</span><br><span class="line">name    subject score   rp      drp     rmp</span><br><span class="line">孙悟空  数学    95      1       1       1</span><br><span class="line">宋宋    数学    86      2       2       2</span><br><span class="line">婷婷    数学    85      3       3       3</span><br><span class="line">大海    数学    56      4       4       4</span><br><span class="line">宋宋    英语    84      1       1       1</span><br><span class="line">大海    英语    84      1       1       2</span><br><span class="line">婷婷    英语    78      3       2       3</span><br><span class="line">孙悟空  英语    68      4       3       4</span><br><span class="line">大海    语文    94      1       1       1</span><br><span class="line">孙悟空  语文    87      2       2       2</span><br><span class="line">婷婷    语文    65      3       3       3</span><br><span class="line">宋宋    语文    64      4       4       4</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>练习</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 按照科目进行排名</span><br><span class="line"></span><br><span class="line">// 给每个学生的总分进行排名</span><br><span class="line"></span><br><span class="line">// 只查询每个科目的成绩的前2名</span><br><span class="line"></span><br><span class="line">// 查询学生成绩，并显示当前科目最高分</span><br><span class="line"></span><br><span class="line">// 查询学生成绩，并显示当前科目最低分</span><br></pre></td></tr></table></figure>







<hr>
<h2 id="7-2-自定义函数"><a href="#7-2-自定义函数" class="headerlink" title="7.2 自定义函数"></a>7.2 自定义函数</h2><ul>
<li>  <code>Hive</code> 提供了一些内置函数，比如 <code>max/min</code> 等，但是数量有限，所以 <code>Hive</code> 提供了用户自定义函数的功能，用户可以通过 <code>UDF</code> 实现自定义函数。</li>
<li>  UDF：User-defined Function</li>
</ul>
<h3 id="用户自定义函数分类（按照特征分类）："><a href="#用户自定义函数分类（按照特征分类）：" class="headerlink" title="用户自定义函数分类（按照特征分类）："></a>用户自定义函数分类（按照特征分类）：</h3><ol>
<li><p><strong>UDF（User-Defined-Function）</strong></p>
<ul>
<li>  一进一出。输入单个参数，返回单个结果</li>
</ul>
</li>
<li><p><strong>UDAF（User-Defined Aggregation Function）</strong>：用户定义的表聚集函数</p>
<ul>
<li>  聚集函数，多进一出</li>
<li>  类似于：<code>count/max/min</code></li>
</ul>
</li>
<li><p><strong>UDTF（User-Defined Table-Generating Functions）</strong>：用户定义的表生成函数</p>
<ul>
<li>   一进多出</li>
<li>  类似于：<code>lateral view explore()</code></li>
</ul>
<p> ​      </p>
</li>
</ol>
<h3 id="官方文档地址"><a href="#官方文档地址" class="headerlink" title="官方文档地址"></a>官方文档地址</h3><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></p>
<h3 id="自定义函数编程步骤："><a href="#自定义函数编程步骤：" class="headerlink" title="自定义函数编程步骤："></a>自定义函数编程步骤：</h3><ol>
<li><p> 继承 <code>org.apache.hadoop.hive.ql.UDF</code></p>
</li>
<li><p> 实现 <code>evaluate</code> 函数；<code>evaluate</code> 函数支持重载；</p>
</li>
<li><p>在 <code>Hive</code> 的命令行窗口创建函数</p>
<ul>
<li><p>添加 <code>jar</code></p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar linux_jar_path</span><br></pre></td></tr></table></figure></li>
<li><p>创建 <code>function</code></p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create [temporary] function [dbname.]function_name AS class_name;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>在 <code>hive</code> 的命令行窗口删除函数</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Drop [temporary] function [if exists] [dbname.]function_name;</span><br></pre></td></tr></table></figure>

</li>
<li><p>注意事项</p>
<ul>
<li>  <code>UDF</code> 必须要有返回类型，可以返回 <code>null</code>，但是返回类型不能为 <code>void</code>；</li>
</ul>
</li>
</ol>
<hr>
<h2 id="7-3-自定义-UDF-函数"><a href="#7-3-自定义-UDF-函数" class="headerlink" title="7.3 自定义 UDF 函数"></a>7.3 自定义 UDF 函数</h2><ol>
<li><p>创建一个 <code>Maven</code> 工程，并导入依赖</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
<li><p>创建一个类</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.hive;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Lower</span> <span class="keyword">extends</span> <span class="title class_">UDF</span> &#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">public</span> String <span class="title function_">evaluate</span> <span class="params">(<span class="keyword">final</span> String s)</span> &#123;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (s == <span class="literal">null</span>) &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> s.toLowerCase();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
<li><p> 打成 <code>jar</code> 包上传到服务器 <code>/opt/module/jars/udf.jar</code></p>
</li>
<li><p>将 <code>jar</code> 包添加到 <code>hive</code> 的 <code>classpath</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; add jar /opt/module/datas/udf.jar;</span><br></pre></td></tr></table></figure></li>
<li><p>创建临时函数与开发好的 <code>java class</code> 关联</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create temporary function mylower as &quot;com.atguigu.hive.Lower&quot;;</span><br></pre></td></tr></table></figure></li>
<li><p>即可在 <code>hql</code> 中使用自定义的函数 <code>strip</code> </p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename, mylower(ename) lowername from emp;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h1 id="第-8-章-压缩和存储"><a href="#第-8-章-压缩和存储" class="headerlink" title="第 8 章    压缩和存储"></a>第 8 章    压缩和存储</h1><h2 id="8-1-Hadoop源码编译支持Snappy压缩"><a href="#8-1-Hadoop源码编译支持Snappy压缩" class="headerlink" title="8.1 Hadoop源码编译支持Snappy压缩"></a>8.1 Hadoop源码编译支持Snappy压缩</h2><h3 id="8-1-1-资源准备"><a href="#8-1-1-资源准备" class="headerlink" title="8.1.1 资源准备"></a>8.1.1 资源准备</h3><p>1．CentOS联网 </p>
<p>配置CentOS能连接外网。Linux虚拟机ping <a target="_blank" rel="noopener" href="http://www.baidu.com/">www.baidu.com</a> 是畅通的</p>
<p>注意：采用root角色编译，减少文件夹权限出现问题</p>
<p>2．jar包准备(hadoop源码、JDK8 、maven、protobuf)</p>
<p>（1）hadoop-2.7.2-src.tar.gz</p>
<p>（2）jdk-8u144-linux-x64.tar.gz</p>
<p>（3）snappy-1.1.3.tar.gz</p>
<p>（4）apache-maven-3.0.5-bin.tar.gz</p>
<p>（5）protobuf-2.5.0.tar.gz</p>
<h3 id="8-1-2-jar包安装"><a href="#8-1-2-jar包安装" class="headerlink" title="8.1.2 jar包安装"></a>8.1.2 jar包安装</h3><p>注意：所有操作必须在root用户下完成</p>
<p>1．JDK解压、配置环境变量JAVA_HOME和PATH，验证<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/javase">java</a>-version(如下都需要验证是否配置成功)</p>
<p>[root@hadoop101 software] # tar -zxf jdk-8u144-linux-x64.tar.gz -C /opt/module/</p>
<p>[root@hadoop101 software]# vi /etc/profile</p>
<p>  #JAVA_HOME  export  JAVA_HOME=/opt/module/jdk1.8.0_144  export  PATH=$PATH:$JAVA_HOME/bin  </p>
<p>[root@hadoop101 software]#source /etc/profile</p>
<p>验证命令：java -version</p>
<p>2．Maven解压、配置  MAVEN_HOME和PATH</p>
<p>[root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/</p>
<p>[root@hadoop101 apache-maven-3.0.5]# vi /etc/profile</p>
<p>  #MAVEN_HOME  export  MAVEN_HOME=/opt/module/apache-maven-3.0.5  export  PATH=$PATH:$MAVEN_HOME/bin  </p>
<p>[root@hadoop101 software]#source /etc/profile</p>
<p>验证命令：mvn -version</p>
<h3 id="8-1-3-编译源码"><a href="#8-1-3-编译源码" class="headerlink" title="8.1.3 编译源码"></a>8.1.3 编译源码</h3><p>1．准备编译环境</p>
<p>[root@hadoop101 software]# yum install svn</p>
<p>[root@hadoop101 software]# yum install autoconf automake libtool cmake</p>
<p>[root@hadoop101 software]# yum install ncurses-devel</p>
<p>[root@hadoop101 software]# yum install openssl-devel</p>
<p>[root@hadoop101 software]# yum install gcc*</p>
<p>2．编译安装snappy</p>
<p>[root@hadoop101 software]# tar -zxvf snappy-1.1.3.tar.gz -C /opt/module/</p>
<p>[root@hadoop101 module]# cd snappy-1.1.3/</p>
<p>[root@hadoop101 snappy-1.1.3]# ./configure</p>
<p>[root@hadoop101 snappy-1.1.3]# make</p>
<p>[root@hadoop101 snappy-1.1.3]# make install</p>
<p># 查看snappy库文件</p>
<p>[root@hadoop101 snappy-1.1.3]# ls -lh /usr/local/lib |grep snappy</p>
<p>3．编译安装protobuf</p>
<p>[root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/</p>
<p>[root@hadoop101 module]# cd protobuf-2.5.0/</p>
<p>[root@hadoop101 protobuf-2.5.0]# ./configure </p>
<p>[root@hadoop101 protobuf-2.5.0]# make </p>
<p>[root@hadoop101 protobuf-2.5.0]# make install</p>
<p># 查看protobuf版本以测试是否安装成功<br> [root@hadoop101 protobuf-2.5.0]# protoc –version</p>
<p>4．编译hadoop native</p>
<p>[root@hadoop101 software]# tar -zxvf hadoop-2.7.2-src.tar.gz</p>
<p>[root@hadoop101 software]# cd hadoop-2.7.2-src/</p>
<p>[root@hadoop101 software]# mvn clean package -DskipTests -Pdist,native -Dtar -Dsnappy.lib=/usr/local/lib -Dbundle.snappy</p>
<p>执行成功后，/opt/software/hadoop-2.7.2-src/hadoop-dist/target/<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/hadoop">hadoop</a>-2.7.2.tar.gz即为新生成的支持snappy压缩的二进制安装包。</p>
<h2 id="8-2-Hadoop压缩配置"><a href="#8-2-Hadoop压缩配置" class="headerlink" title="8.2 Hadoop压缩配置"></a>8.2 Hadoop压缩配置</h2><h3 id="8-2-1-MR支持的压缩编码"><a href="#8-2-1-MR支持的压缩编码" class="headerlink" title="8.2.1 MR支持的压缩编码"></a>8.2.1 MR支持的压缩编码</h3><p>表6-8</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>工具</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
</tr>
</thead>
<tbody><tr>
<td>DEFAULT</td>
<td>无</td>
<td>DEFAULT</td>
<td>.deflate</td>
<td>否</td>
</tr>
<tr>
<td>Gzip</td>
<td>gzip</td>
<td>DEFAULT</td>
<td>.gz</td>
<td>否</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>lzop</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
</tr>
<tr>
<td>Snappy</td>
<td>无</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
</tr>
</tbody></table>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p>
<p>表6-9</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩性能的比较：</p>
<p>表6-10</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
<p><a target="_blank" rel="noopener" href="http://google.github.io/snappy/">http://google.github.io/snappy/</a></p>
<p>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p>
<h3 id="8-2-2-压缩参数配置"><a href="#8-2-2-压缩参数配置" class="headerlink" title="8.2.2 压缩参数配置"></a>8.2.2 压缩参数配置</h3><p>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：</p>
<p>表6-11</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs    （在core-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec,  org.apache.hadoop.io.compress.GzipCodec,  org.apache.hadoop.io.compress.BZip2Codec,  org.apache.hadoop.io.compress.Lz4Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec</td>
<td>org.apache.hadoop.io.compress.  DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
<h2 id="8-3-开启Map输出阶段压缩"><a href="#8-3-开启Map输出阶段压缩" class="headerlink" title="8.3 开启Map输出阶段压缩"></a>8.3 开启Map输出阶段压缩</h2><p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：</p>
<p><strong>案例实操：</strong></p>
<p>1．开启hive中间传输数据压缩功能</p>
<p>hive (default)&gt;set hive.exec.compress.intermediate=true;</p>
<p>2．开启mapreduce中map输出压缩功能</p>
<p>hive (default)&gt;set mapreduce.map.output.compress=true;</p>
<p>3．设置mapreduce中map输出数据的压缩方式</p>
<p>hive (default)&gt;set mapreduce.map.output.compress.codec=</p>
<p> org.apache.hadoop.io.compress.SnappyCodec;</p>
<p>4．执行查询语句</p>
<p>  hive (default)&gt; select count(ename) name from emp;</p>
<h2 id="8-4-开启Reduce输出阶段压缩"><a href="#8-4-开启Reduce输出阶段压缩" class="headerlink" title="8.4 开启Reduce输出阶段压缩"></a>8.4 开启Reduce输出阶段压缩</h2><p>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。</p>
<p><strong>案例实操：</strong></p>
<p>1．开启hive最终输出数据压缩功能</p>
<p>hive (default)&gt;set hive.exec.compress.output=true;</p>
<p>2．开启mapreduce最终输出数据压缩</p>
<p>hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true;</p>
<p>3．设置mapreduce最终数据输出压缩方式</p>
<p>hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec =</p>
<p> org.apache.hadoop.io.compress.SnappyCodec;</p>
<p>4．设置mapreduce最终数据输出压缩为块压缩</p>
<p>hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK;</p>
<p>5．测试一下输出结果是否是压缩文件</p>
<p>hive (default)&gt; insert overwrite local directory</p>
<p> ‘/opt/module/datas/distribute-result’ select * from emp distribute by deptno sort by empno desc;</p>
<h2 id="8-5-文件存储格式"><a href="#8-5-文件存储格式" class="headerlink" title="8.5 文件存储格式"></a>8.5 文件存储格式</h2><p>Hive支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p>
<h3 id="8-5-1-列式存储和行式存储"><a href="#8-5-1-列式存储和行式存储" class="headerlink" title="8.5.1 列式存储和行式存储"></a>8.5.1 列式存储和行式存储</h3><p><img src="/2021/12/20/Hive%EF%BC%88%E4%B8%8B%EF%BC%89/clip_image002.jpg" alt="img"></p>
<p>图6-10 列式存储和行式存储</p>
<p>如图6-10所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p>
<p>1．行存储的特点</p>
<p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p>
<p>2．列存储的特点</p>
<p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p>
<p>TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；</p>
<p>ORC和PARQUET是基于列式存储的。</p>
<h3 id="8-5-2-TextFile格式"><a href="#8-5-2-TextFile格式" class="headerlink" title="8.5.2 TextFile格式"></a>8.5.2 TextFile格式</h3><p>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p>
<h3 id="8-5-3-Orc格式"><a href="#8-5-3-Orc格式" class="headerlink" title="8.5.3 Orc格式"></a>8.5.3 Orc格式</h3><p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。</p>
<p>如图6-11所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer：</p>
<p><img src="/2021/12/20/Hive%EF%BC%88%E4%B8%8B%EF%BC%89/clip_image004.jpg" alt="img"></p>
<p>图6-11 Orc格式</p>
<p>​    1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。</p>
<p>   2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。</p>
<p>​    3）Stripe Footer：存的是各个Stream的类型，长度等信息。</p>
<p>每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p>
<h3 id="8-5-4-Parquet格式"><a href="#8-5-4-Parquet格式" class="headerlink" title="8.5.4 Parquet格式"></a>8.5.4 Parquet格式</h3><p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。</p>
<p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。</p>
<p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如图6-12所示。</p>
<p><img src="/2021/12/20/Hive%EF%BC%88%E4%B8%8B%EF%BC%89/clip_image006.jpg" alt="Parquet文件格式"></p>
<p>图6-12 Parquet格式</p>
<p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p>
<h3 id="8-5-5-主流文件存储格式对比实验"><a href="#8-5-5-主流文件存储格式对比实验" class="headerlink" title="8.5.5 主流文件存储格式对比实验"></a>8.5.5 主流文件存储格式对比实验</h3><p>从存储文件的压缩比和查询速度两个角度对比。</p>
<p><strong>存储文件的压缩比测试：</strong></p>
<p>\1.   测试数据</p>
<p>2．TextFile</p>
<p>（1）创建表，存储数据格式为TEXTFILE</p>
<p>  create  table log_text (  track_time  string,  url  string,  session_id  string,  referer  string,  ip  string,  end_user_id  string,  city_id  string  )  row  format delimited fields terminated by ‘\t’  stored  as textfile ;  </p>
<p>（2）向表中加载数据</p>
<p>  hive  (default)&gt; load data local inpath ‘/opt/module/datas/log.data’ into table  log_text ;  </p>
<p>（3）查看表中数据大小</p>
<p>  hive  (default)&gt; dfs -du -h /user/hive/warehouse/log_text;  </p>
<p>18.1 M /user/hive/warehouse/log_text/log.data</p>
<p>3．ORC</p>
<p>​    （1）创建表，存储数据格式为ORC</p>
<p>  create  table log_orc(  track_time  string,  url  string,  session_id  string,  referer  string,  ip  string,  end_user_id  string,  city_id  string  )  row  format delimited fields terminated by ‘\t’  stored  as orc ;  </p>
<p>（2）向表中加载数据</p>
<p>  hive  (default)&gt; insert into table log_orc select * from log_text ;  </p>
<p>（3）查看表中数据大小</p>
<p>  hive  (default)&gt; dfs -du -h /user/hive/warehouse/log_orc/ ;  </p>
<p>2.8 M /user/hive/warehouse/log_orc/000000_0</p>
<p>4．Parquet</p>
<p>（1）创建表，存储数据格式为parquet</p>
<p>  create  table log_parquet(  track_time  string,  url  string,  session_id  string,  referer  string,  ip  string,  end_user_id  string,  city_id  string  )  row  format delimited fields terminated by ‘\t’  stored  as parquet ;    </p>
<p>（2）向表中加载数据</p>
<p>  hive  (default)&gt; insert into table log_parquet select * from log_text ;  </p>
<p>（3）查看表中数据大小</p>
<p>  hive  (default)&gt; dfs -du -h /user/hive/warehouse/log_parquet/ ;  </p>
<p>13.1 M  /user/hive/warehouse/log_parquet/000000_0</p>
<p>存储文件的压缩比总结：</p>
<p>ORC &gt; Parquet &gt; textFile</p>
<p><strong>存储文件的查询速度测试：</strong></p>
<p>1．TextFile</p>
<p>hive (default)&gt; select count(*) from log_text;</p>
<p>_c0</p>
<p>100000</p>
<p>Time taken: 21.54 seconds, Fetched: 1 row(s)</p>
<p>Time taken: 21.08 seconds, Fetched: 1 row(s)</p>
<p>Time taken: 19.298 seconds, Fetched: 1 row(s)</p>
<p>2．ORC</p>
<p>hive (default)&gt; select count(*) from log_orc;</p>
<p>_c0</p>
<p>100000</p>
<p>Time taken: 20.867 seconds, Fetched: 1 row(s)</p>
<p>Time taken: 22.667 seconds, Fetched: 1 row(s)</p>
<p>Time taken: 18.36 seconds, Fetched: 1 row(s)</p>
<p>3．Parquet</p>
<p>hive (default)&gt; select count(*) from log_parquet;</p>
<p>_c0</p>
<p>100000</p>
<p>Time taken: 22.922 seconds, Fetched: 1 row(s)</p>
<p>Time taken: 21.074 seconds, Fetched: 1 row(s)</p>
<p>Time taken: 18.384 seconds, Fetched: 1 row(s)</p>
<p>存储文件的查询速度总结：查询速度相近。</p>
<h2 id="8-6-存储和压缩结合"><a href="#8-6-存储和压缩结合" class="headerlink" title="8.6 存储和压缩结合"></a>8.6 存储和压缩结合</h2><h3 id="8-6-1-修改Hadoop集群具有Snappy压缩方式"><a href="#8-6-1-修改Hadoop集群具有Snappy压缩方式" class="headerlink" title="8.6.1 修改Hadoop集群具有Snappy压缩方式"></a>8.6.1 修改Hadoop集群具有Snappy压缩方式</h3><p>1．查看hadoop checknative命令使用</p>
<p>[atguigu@hadoop104 hadoop-2.7.2]$ hadoop</p>
<p>​    checknative [-a|-h] check native hadoop and compression libraries availability</p>
<p>2．查看hadoop支持的压缩方式</p>
<p> [atguigu@hadoop104 hadoop-2.7.2]$ hadoop checknative</p>
<p>17/12/24 20:32:52 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version</p>
<p>17/12/24 20:32:52 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</p>
<p>Native library checking:</p>
<p>hadoop: true /opt/module/hadoop-2.7.2/lib/native/libhadoop.so</p>
<p>zlib:  true /lib64/libz.so.1</p>
<p>snappy: false </p>
<p>lz4:   true revision:99</p>
<p>bzip2:  false</p>
<p>3．将编译好的支持Snappy压缩的hadoop-2.7.2.tar.gz包导入到hadoop102的/opt/software中</p>
<p>4．解压hadoop-2.7.2.tar.gz到当前路径</p>
<p>[atguigu@hadoop102 software]$ tar -zxvf hadoop-2.7.2.tar.gz</p>
<p>5．进入到/opt/software/hadoop-2.7.2/lib/native路径可以看到支持Snappy压缩的动态链接库</p>
<p>[atguigu@hadoop102 native]$ pwd</p>
<p>/opt/software/hadoop-2.7.2/lib/native</p>
<p>[atguigu@hadoop102 native]$ ll</p>
<p>-rw-r–r–. 1 atguigu atguigu 472950 9月  1 10:19 libsnappy.a</p>
<p>-rwxr-xr-x. 1 atguigu atguigu   955 9月  1 10:19 libsnappy.la</p>
<p>lrwxrwxrwx. 1 atguigu atguigu   18 12月 24 20:39 libsnappy.so -&gt; libsnappy.so.1.3.0</p>
<p>lrwxrwxrwx. 1 atguigu atguigu   18 12月 24 20:39 libsnappy.so.1 -&gt; libsnappy.so.1.3.0</p>
<p>-rwxr-xr-x. 1 atguigu atguigu 228177 9月  1 10:19 libsnappy.so.1.3.0</p>
<p>6．拷贝/opt/software/hadoop-2.7.2/lib/native里面的所有内容到开发集群的/opt/module/hadoop-2.7.2/lib/native路径上</p>
<p>[atguigu@hadoop102 native]$ cp ../native/* /opt/module/hadoop-2.7.2/lib/native/</p>
<p>7．分发集群</p>
<p>[atguigu@hadoop102 lib]$ xsync native/</p>
<p>8．再次查看hadoop支持的压缩类型</p>
<p>[atguigu@hadoop102 hadoop-2.7.2]$ hadoop checknative</p>
<p>17/12/24 20:45:02 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version</p>
<p>17/12/24 20:45:02 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</p>
<p>Native library checking:</p>
<p>hadoop: true /opt/module/hadoop-2.7.2/lib/native/libhadoop.so</p>
<p>zlib:  true /lib64/libz.so.1</p>
<p>snappy: true /opt/module/hadoop-2.7.2/lib/native/libsnappy.so.1</p>
<p>lz4:   true revision:99</p>
<p>bzip2:  false</p>
<p>9．重新启动hadoop集群和hive</p>
<h3 id="8-6-2-测试存储和压缩"><a href="#8-6-2-测试存储和压缩" class="headerlink" title="8.6.2 测试存储和压缩"></a>8.6.2 测试存储和压缩</h3><p>官网：<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a></p>
<p>ORC存储方式的压缩：</p>
<p>表6-12</p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Default</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td>orc.compress</td>
<td>ZLIB</td>
<td>high level compression (one of NONE, ZLIB,  SNAPPY)</td>
</tr>
<tr>
<td>orc.compress.size</td>
<td>262,144</td>
<td>number of bytes in each compression chunk</td>
</tr>
<tr>
<td>orc.stripe.size</td>
<td>67,108,864</td>
<td>number of bytes in each stripe</td>
</tr>
<tr>
<td>orc.row.index.stride</td>
<td>10,000</td>
<td>number of rows between index entries (must be  &gt;= 1000)</td>
</tr>
<tr>
<td>orc.create.index</td>
<td>true</td>
<td>whether to create row indexes</td>
</tr>
<tr>
<td>orc.bloom.filter.columns</td>
<td>“”</td>
<td>comma separated list of column names for which  bloom filter should be created</td>
</tr>
<tr>
<td>orc.bloom.filter.fpp</td>
<td>0.05</td>
<td>false positive probability for bloom filter  (must &gt;0.0 and &lt;1.0)</td>
</tr>
</tbody></table>
<p>1．创建一个非压缩的的ORC存储方式</p>
<p>​    （1）建表语句</p>
<p>  create  table log_orc_none(  track_time  string,  url  string,  session_id  string,  referer  string,  ip  string,  end_user_id  string,  city_id  string  )  row  format delimited fields terminated by ‘\t’  stored  as orc tblproperties (“orc.compress”=”NONE”);  </p>
<p>​    （2）插入数据</p>
<p>  hive  (default)&gt; insert into table log_orc_none select * from log_text ;  </p>
<p>​    （3）查看插入后数据</p>
<p>  hive  (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_none/ ;  </p>
<p>7.7 M /user/hive/warehouse/log_orc_none/000000_0</p>
<p>2．创建一个SNAPPY压缩的ORC存储方式</p>
<p>​    （1）建表语句</p>
<p>  create table log_orc_snappy(  track_time string,  url string,  session_id string,  referer string,  ip string,  end_user_id string,  city_id string  )  row format delimited fields terminated by ‘\t’  stored as orc tblproperties  (“orc.compress”=”SNAPPY”);  </p>
<p>​    （2）插入数据</p>
<p>  hive  (default)&gt; insert into table log_orc_snappy select * from log_text ;  </p>
<p>​    （3）查看插入后数据</p>
<p>  hive  (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_snappy/ ;  </p>
<p>3.8 M  /user/hive/warehouse/log_orc_snappy/000000_0</p>
<p>3．上一节中默认创建的ORC存储方式，导入数据后的大小为</p>
<p>2.8 M /user/hive/warehouse/log_orc/000000_0</p>
<p>比Snappy压缩的还小。原因是orc存储文件默认采用ZLIB压缩。比snappy压缩的小。</p>
<p>4．存储方式和压缩总结</p>
<p>在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/17/Hive%E5%AE%89%E8%A3%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/17/Hive%E5%AE%89%E8%A3%85/" class="post-title-link" itemprop="url">Hive安装</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-17 23:37:03" itemprop="dateCreated datePublished" datetime="2021-12-17T23:37:03+08:00">2021-12-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-04-21 16:20:54" itemprop="dateModified" datetime="2022-04-21T16:20:54+08:00">2022-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hive/" itemprop="url" rel="index"><span itemprop="name">Hive</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="1-Hive安装地址"><a href="#1-Hive安装地址" class="headerlink" title="1 Hive安装地址"></a>1 Hive安装地址</h1><ol>
<li><p>Hive官网地址</p>
<p> <a target="_blank" rel="noopener" href="http://hive.apache.org/">http://hive.apache.org/</a></p>
</li>
<li><p>文档查看地址</p>
<p> <a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p>
</li>
<li><p>下载地址</p>
<p> <a target="_blank" rel="noopener" href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></p>
</li>
<li><p>github地址</p>
<p> <a target="_blank" rel="noopener" href="https://github.com/apache/hive">https://github.com/apache/hive</a></p>
</li>
</ol>
<hr>
<h1 id="2-Hive-安装部署"><a href="#2-Hive-安装部署" class="headerlink" title="2 Hive 安装部署"></a>2 Hive 安装部署</h1><ol>
<li> 保证系统中已经配置了 <code>JAVA_HOME</code> 和 <code>HADOOP_HOME</code> 环境变量。</li>
<li> 配置 <code>HIVE_HOME</code>。</li>
<li> Hive 在启动时，默认会读取 <code>$HIVE_HOME/conf</code> 目录下的配置文件。</li>
</ol>
<p><strong>配置：</strong></p>
<ul>
<li><p>  如果需要自定义 Hive 的属性，需要在 <code>conf/hive-site.xml</code> 配置文件中配置自定义的属性</p>
</li>
<li><p>Hive 启动时，读取配置文件的顺序：</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">① 由于 Hive 在使用时是依赖于 Hadoop 的，所以 Hive 启动时首先会读取 Hadoop 中的 8 个配置文件；</span><br><span class="line">==&gt; ② 读取 Hive 的默认配置文件：hive-default.xml</span><br><span class="line">==&gt; ③ 读取 Hive 的自定义配置文件：hive-site.xml</span><br><span class="line">==&gt; ④ 还可以在启动 Hive 时，通过命令行参数的方式，显式指定配置信息，但是这种方式只在当前次启动生效。[--hiveconf key=value]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="2-1-Hive-安装及配置"><a href="#2-1-Hive-安装及配置" class="headerlink" title="2.1    Hive 安装及配置"></a>2.1    Hive 安装及配置</h2><ol>
<li><p> 把 <code>apache-hive-1.2.1-bin.tar.gz</code> 上传到 Linux 的 <code>/opt/software</code> 目录下</p>
</li>
<li><p>解压 <code>apache-hive-1.2.1-bin.tar.gz</code> 到 <code>/opt/module/</code> 目录下面</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li>
<li><p>重命名 <code>apache-hive-1.2.1-bin.tar.gz</code> 为 <code>hive</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 module]$ <span class="built_in">mv</span> apache-hive-1.2.1-bin/ hive</span><br></pre></td></tr></table></figure></li>
<li><p>备份 <code>/opt/module/hive/conf</code> 目录下的 <code>hive-env.sh.template</code> 为 <code>hive-env.sh</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 conf]$ <span class="built_in">cp</span> hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure></li>
<li><p>配置 <code>hive-env.sh</code> 文件</p>
<ul>
<li><p>配置 <code>HADOOP_HOME</code> 路径</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure></li>
<li><p>配置 <code>HIVE_CONF_DIR</code> 路径</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/opt/module/hive/conf</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>配置 <code>HIVE_HOME</code></p>
<ul>
<li>  获取 Hive 安装路径</li>
</ul>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hive]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hive</span><br></pre></td></tr></table></figure>

<ul>
<li><p>编辑 <code>/etc/profile.d/my_env.sh</code> 文件</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure></li>
<li><p>在文件末尾添加 <code>HIVE_HOME</code></p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HIVE_HOME</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/opt/module/hive</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br></pre></td></tr></table></figure></li>
<li><p>测试配置是否生效</p>
<p>  <img src="/2021/12/17/Hive%E5%AE%89%E8%A3%85/image-20211218205539914.png" alt="image-20211218205539914"></p>
</li>
</ul>
</li>
</ol>
<hr>
<h2 id="2-2-Hadoop-集群配置"><a href="#2-2-Hadoop-集群配置" class="headerlink" title="2.2    Hadoop 集群配置"></a>2.2    Hadoop 集群配置</h2><ol>
<li><p>必须启动 <code>HDFS</code> 和 <code>Yarn</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 conf]$ sbin/start-dfs.sh</span><br><span class="line">[lvnengdong@hadoop103 conf]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 HDFS 上创建 <code>/tmp</code> 和 <code>/user/hive/warehouse</code> 两个目录并修改他们的同组权限可写</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -<span class="built_in">mkdir</span> /tmp</span><br><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -<span class="built_in">mkdir</span> -p /user/hive/warehouse</span><br><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -<span class="built_in">chmod</span> g+w /tmp</span><br><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -<span class="built_in">chmod</span> g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="2-3-Hive-基本操作"><a href="#2-3-Hive-基本操作" class="headerlink" title="2.3    Hive 基本操作"></a>2.3    Hive 基本操作</h2><ol>
<li><p>启动 Hive</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 ~]$ hive</span><br></pre></td></tr></table></figure></li>
<li><p>查看数据库</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure></li>
<li><p>打开默认数据库</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use default;</span><br></pre></td></tr></table></figure></li>
<li><p>显示当前数据库下管理的所有表</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure></li>
<li><p>创建一张表</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table student(id int, name string);</span><br></pre></td></tr></table></figure></li>
<li><p>查看表的结构</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc student;</span><br></pre></td></tr></table></figure></li>
<li><p>向表中插入数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert into student values(10,&quot;zhangsan&quot;);</span><br></pre></td></tr></table></figure></li>
<li><p>查询表中数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br></pre></td></tr></table></figure></li>
<li><p>退出 Hive</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; quit;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h1 id="3-将本地文件导入-Hive-中"><a href="#3-将本地文件导入-Hive-中" class="headerlink" title="3 将本地文件导入 Hive 中"></a>3 将本地文件导入 Hive 中</h1><h2 id="3-1-需求："><a href="#3-1-需求：" class="headerlink" title="3.1    需求："></a>3.1    需求：</h2><ul>
<li>  将本地 <code>/opt/module/datas/student.txt</code> 这个目录下的数据导入到 Hive 的 <code>student(id int, name string)</code> 表中。</li>
</ul>
<h2 id="3-2-数据准备"><a href="#3-2-数据准备" class="headerlink" title="3.2    数据准备"></a>3.2    数据准备</h2><p>在 <code>/opt/module/datas</code> 这个目录下准备数据。</p>
<ol>
<li><p>在 <code>/opt/module/</code> 目录下创建 datas 目录</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> datas</span><br></pre></td></tr></table></figure></li>
<li><p>在 <code>/opt/module/datas/</code> 目录下创建 <code>student.txt</code> 文件并添加数据</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 datas]$ <span class="built_in">touch</span> student.txt</span><br><span class="line">[lvnengdong@hadoop102 datas]$ vim student.txt</span><br></pre></td></tr></table></figure>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1001	zhangshan</span><br><span class="line">1002	lishi</span><br><span class="line">1003	zhaoliu</span><br></pre></td></tr></table></figure>
<ul>
<li>  注意：每行记录的字段之间以 Tab 键间隔。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="3-3-Hive-实际操作"><a href="#3-3-Hive-实际操作" class="headerlink" title="3.3    Hive 实际操作"></a>3.3    Hive 实际操作</h2><ol>
<li><p>启动 Hive</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 ~]$ hive</span><br></pre></td></tr></table></figure></li>
<li><p>显示数据库</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure></li>
<li><p>使用 <code>default</code> 数据库</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use default;</span><br></pre></td></tr></table></figure></li>
<li><p>显示 <code>default</code> 数据库中的表</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure></li>
<li><p>删除已创建的 <code>student</code> 表</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table student;</span><br></pre></td></tr></table></figure></li>
<li><p>创建 <code>student</code> 表，并声明文件分隔符 <code>&#39;\t&#39;</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line">	create table student(id int, name string) </span><br><span class="line">	ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;;	# 显式声明列与列之间的分隔符为 `\t` </span><br></pre></td></tr></table></figure></li>
<li><p>加载 <code>/opt/module/datas/student.txt</code> 文件到 <code>student</code> 数据库表中。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table student;</span><br></pre></td></tr></table></figure></li>
<li><p>Hive 查询结果</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br><span class="line">OK</span><br><span class="line">1001  zhangshan</span><br><span class="line">1002  lishi</span><br><span class="line">1003  zhaoliu</span><br><span class="line">Time taken: 0.266 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="3-4-遇到的问题"><a href="#3-4-遇到的问题" class="headerlink" title="3.4    遇到的问题"></a>3.4    遇到的问题</h2><p>再打开一个客户端窗口启动 Hive，会产生 <code>java.sql.SQLException</code> 异常。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.RuntimeException: java.lang.RuntimeException:</span><br><span class="line"> Unable to instantiate</span><br><span class="line"> org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br><span class="line">        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)</span><br><span class="line">        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)</span><br><span class="line">        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</span><br><span class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">        at java.lang.reflect.Method.invoke(Method.java:606)</span><br><span class="line">        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)</span><br><span class="line">        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</span><br><span class="line">Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br><span class="line">        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:86)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)</span><br><span class="line">        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)</span><br><span class="line">        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)</span><br><span class="line">        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)</span><br><span class="line">... 8 more</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>原因是：<code>Metastore</code> 默认存储在 Hive 自带的 <code>derby</code> 数据库中，推荐使用 <code>MySQL</code> 存储 <code>Metastore</code>。</p>
<hr>
<h1 id="4-MySQL-安装"><a href="#4-MySQL-安装" class="headerlink" title="4    MySQL 安装"></a>4    MySQL 安装</h1><ul>
<li>  <a href>超链接</a></li>
</ul>
<hr>
<h1 id="5-Hive-元数据配置到-MySQL"><a href="#5-Hive-元数据配置到-MySQL" class="headerlink" title="5    Hive 元数据配置到 MySQL"></a>5    Hive 元数据配置到 MySQL</h1><h2 id="5-1-驱动拷贝"><a href="#5-1-驱动拷贝" class="headerlink" title="5.1    驱动拷贝"></a>5.1    驱动拷贝</h2><p><code>Hive</code> 需要访问 MySQL 服务端，自身相当于是一个 MySQL 的客户端，所以需要安装一个 MySQL 客户端依赖。</p>
<ol>
<li><p>在 <code>/opt/software/mysql-libs</code> 目录下解压 <code>mysql-connector-java-5.1.27.tar.gz</code> 驱动包</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf mysql-connector-java-5.1.27.tar.gz</span><br></pre></td></tr></table></figure></li>
<li><p>拷贝 <code>/opt/software/mysql-libs/mysql-connector-java-5.1.27</code> 目录下的 <code>mysql-connector-java-5.1.27-bin.jar</code> 到 <code>/opt/module/hive/lib/</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> mysql-connector-java-5.1.27-bin.jar /opt/module/hive/lib/</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="5-2-配置-Metastore-到-MySQL"><a href="#5-2-配置-Metastore-到-MySQL" class="headerlink" title="5.2    配置 Metastore 到 MySQL"></a>5.2    配置 Metastore 到 MySQL</h2><ol>
<li><p>在<code>/opt/module/hive/conf</code>目录下创建一个<code>hive-site.xml</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">touch</span> hive-site.xml</span><br><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure>

</li>
<li><p>根据官方文档配置参数，拷贝数据到<code>hive-site.xml</code>文件中</p>
<ul>
<li>  官方文档：<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin">https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin</a></li>
</ul>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Hive连接哪个库来查找元数据--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p> 配置完毕后，如果启动 Hive 异常，可以重新启动虚拟机。（重启后，别忘了启动 Hadoop 集群）</p>
</li>
</ol>
<hr>
<h2 id="5-3-多窗口启动Hive测试"><a href="#5-3-多窗口启动Hive测试" class="headerlink" title="5.3    多窗口启动Hive测试"></a>5.3    多窗口启动Hive测试</h2><ol>
<li><p>先启动 MySQL</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p000000</span><br></pre></td></tr></table></figure></li>
<li><p>查看有几个数据库</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database      |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql       |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test        |</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>
</li>
<li><p>再次打开多个窗口，分别启动 <code>hive</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive</span><br></pre></td></tr></table></figure>

</li>
<li><p>启动 Hive 后，回到 MySQL 窗口查看数据库，显示增加了 <code>metastore</code> 数据库</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line">| Database      |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| metastore     |</span><br><span class="line">| mysql       |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test        |</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<h1 id="6-HiveJDBC-访问"><a href="#6-HiveJDBC-访问" class="headerlink" title="6    HiveJDBC 访问"></a>6    HiveJDBC 访问</h1><ol>
<li><p>启动 JDBC 的服务端进程：<code>hiveserver2</code> </p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 ~]$ hiveserver2</span><br></pre></td></tr></table></figure>

<p> <code>hiveserver2 </code>是一个前台进程，启动后控制台界面就不能动了，所以需要再新开一个 shell 窗口启动客户端进程 <code>beeline</code></p>
</li>
<li><p>启动 JDBC 的客户端：</p>
<ul>
<li><p>启动客户端进程 <code>beeline</code>。<code>beeline</code> 是一种 JDBC 客户端的实现方式</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">    [lvnengdong@hadoop102 ~]$ beeline</span><br><span class="line">    Beeline version 1.2.1 by Apache Hive</span><br><span class="line">beeline&gt; </span><br></pre></td></tr></table></figure></li>
<li><p>  使用其它的 JDBC 客户端</p>
</li>
</ul>
</li>
<li><p>连接 <code>hiveserver2</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">beeline&gt; !connect jdbc:hive2://hadoop102:10000（回车）</span><br><span class="line">Connecting to jdbc:hive2://hadoop102:10000</span><br><span class="line">Enter username <span class="keyword">for</span> jdbc:hive2://hadoop102:10000: lvnengdong（回车）</span><br><span class="line">Enter password <span class="keyword">for</span> jdbc:hive2://hadoop102:10000: （直接回车）</span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://hadoop102:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default        |</span><br><span class="line">+----------------+--+</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="IDEA-连接-Hive"><a href="#IDEA-连接-Hive" class="headerlink" title="IDEA 连接 Hive"></a>IDEA 连接 Hive</h2><hr>
<h1 id="7-Hive-常用交互命令"><a href="#7-Hive-常用交互命令" class="headerlink" title="7    Hive 常用交互命令"></a>7    Hive 常用交互命令</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 conf]$ hive --<span class="built_in">help</span></span><br><span class="line">Usage ./hive &lt;parameters&gt; --service serviceName &lt;service parameters&gt;</span><br><span class="line">Service List: beeline cli <span class="built_in">help</span> hiveburninclient hiveserver2 hiveserver hwi jar lineage metastore metatool orcfiledump rcfilecat schemaTool version </span><br><span class="line">Parameters parsed:</span><br><span class="line">  --auxpath : Auxillary jars </span><br><span class="line">  --config : Hive configuration directory</span><br><span class="line">  --service : Starts specific service/component. cli is default</span><br><span class="line">Parameters used:</span><br><span class="line">  HADOOP_HOME or HADOOP_PREFIX : Hadoop install directory</span><br><span class="line">  HIVE_OPT : Hive options</span><br><span class="line">For <span class="built_in">help</span> on a particular service:</span><br><span class="line">  ./hive --service serviceName --<span class="built_in">help</span></span><br><span class="line">Debug <span class="built_in">help</span>:  ./hive --debug --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[lvnengdong@hadoop102 hive]$ hive -<span class="built_in">help</span></span><br><span class="line">usage: hive</span><br><span class="line">-d,--define &lt;key=value&gt;          Variable subsitution to apply to hive</span><br><span class="line">commands. e.g. -d A=B or --define A=B</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定启动Hive之后要连接的数据库</span></span><br><span class="line">--database &lt;databasename&gt;     Specify the database to use</span><br><span class="line">-e &lt;quoted-query-string&gt;         SQL from <span class="built_in">command</span> line</span><br><span class="line">-f &lt;filename&gt;                    SQL from files</span><br><span class="line">-H,--<span class="built_in">help</span>                        Print <span class="built_in">help</span> information</span><br><span class="line">--hiveconf &lt;property=value&gt;   Use value <span class="keyword">for</span> given property</span><br><span class="line">--hivevar &lt;key=value&gt;         Variable subsitution to apply to hive</span><br><span class="line">commands. e.g. --hivevar A=B</span><br><span class="line">-i &lt;filename&gt;                    Initialization SQL file</span><br><span class="line"></span><br><span class="line"><span class="comment"># 静默模式，即在控制台上不打印无关的信息</span></span><br><span class="line">-S,--silent                      Silent mode <span class="keyword">in</span> interactive shell</span><br><span class="line">-v,--verbose                     Verbose mode (<span class="built_in">echo</span> executed SQL to the console)</span><br></pre></td></tr></table></figure>



<ol>
<li><p> <code>-d</code>：定义一个 <code>变量名=变量值</code>，在 <code>hive-cli</code> 中可以通过 <code>$&#123;变量名&#125;</code> 来引用变量值。</p>
</li>
<li><p><code>-e</code>：不进入 Hive 的交互窗口执行 SQL 语句</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive -e <span class="string">&quot;select id from student;&quot;</span></span><br></pre></td></tr></table></figure></li>
<li><p><code>-f</code>：执行脚本中 SQL 语句</p>
<ul>
<li>  在 <code>/opt/module/datas</code> 目录下创建 <code>hivef.sql</code> 文件</li>
</ul>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 datas]$ <span class="built_in">touch</span> hivef.sql</span><br></pre></td></tr></table></figure>

<ul>
<li>  文件中写入正确的 <code>sql</code> 语句</li>
</ul>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span><span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>

<ul>
<li>  执行文件中的 <code>sql</code> 语句</li>
</ul>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql</span><br></pre></td></tr></table></figure>

<ul>
<li>  执行文件中的sql语句并将结果写入文件中</li>
</ul>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt</span><br></pre></td></tr></table></figure></li>
<li><p> <code>--hiveconf key=value</code>：在 <code>hive-cli</code> 启动之前显式指定一组属性。</p>
</li>
</ol>
<hr>
<h1 id="8-Hive-其他命令操作"><a href="#8-Hive-其他命令操作" class="headerlink" title="8    Hive 其他命令操作"></a>8    Hive 其他命令操作</h1><ol>
<li><p> 在 Hive 中使用 HDFS </p>
</li>
<li><p>在 Hive 中使用 shell 命令</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!shell 命令</span><br></pre></td></tr></table></figure>

</li>
<li><p>退出 <code>Hive</code> 窗口</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;exit;</span><br><span class="line">hive(default)&gt;quit;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">	在新版的hive中没区别了，在以前的版本是有的：</span><br><span class="line">		exit：先隐性提交数据，再退出；</span><br><span class="line">		quit：不提交数据，退出；</span><br><span class="line">*/</span><br></pre></td></tr></table></figure>

</li>
<li><p>在 <code>hive cli</code> 命令窗口中如何查看 <code>HDFS</code> 文件系统</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;dfs -<span class="built_in">ls</span> /;</span><br></pre></td></tr></table></figure>

</li>
<li><p>在 <code>hive cli</code> 命令窗口中如何查看本地文件系统(Linux)</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;!ls /opt/module;</span><br></pre></td></tr></table></figure>

</li>
<li><p>查看在hive中输入的所有历史命令</p>
<ul>
<li>  （1）进入到当前用户的根目录 <code>/root</code> 或 <code>/home/atguigu</code></li>
<li>  （2）查看 <code>.hivehistory</code> 文件</li>
</ul>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> .hivehistory</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h1 id="9-Hive-常见属性配置"><a href="#9-Hive-常见属性配置" class="headerlink" title="9    Hive 常见属性配置"></a>9    Hive 常见属性配置</h1><h2 id="9-1-Hive-数据仓库位置配置"><a href="#9-1-Hive-数据仓库位置配置" class="headerlink" title="9.1    Hive 数据仓库位置配置"></a>9.1    Hive 数据仓库位置配置</h2><ol>
<li><p> <code>Hive</code> 安装完成后会默认生成一个 <code>default</code> 数据库，我们在创建表时如果没有显式指定数据库的话，创建的表就保存在该数据库下，该数据库默认保存数据的位置是在 <code>HDFS</code> 上的 <code>/user/hive/warehouse</code> 路径下。</p>
</li>
<li><p> 在仓库目录下，没有对默认的数据库 <code>default</code> 创建文件夹，如果某张表属于 <code>default</code> 数据库，直接在数据仓库目录下创建一个文件夹。</p>
</li>
<li><p>修改 <code>default</code> 数据库的默认位置（将 <code>hive-default.xml.template</code> 如下配置信息拷贝到 <code>hive-site.xml</code> 文件中）。</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">==&gt;修改为：</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>配置同组用户有执行权限</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -<span class="built_in">chmod</span> g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h2 id="9-2-查询后信息显示配置"><a href="#9-2-查询后信息显示配置" class="headerlink" title="9.2    查询后信息显示配置"></a>9.2    查询后信息显示配置</h2><ol>
<li><p>在 <code>hive-site.xml</code> 文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

</li>
<li><p>重新启动 <code>Hive</code>，对比配置前后差异。</p>
<ul>
<li>  <code>Before</code></li>
</ul>
<p> <img src="/2021/12/17/Hive%E5%AE%89%E8%A3%85/clip_image002.jpg" alt="img"></p>
<ul>
<li>  <code>After</code></li>
</ul>
</li>
</ol>
<p><img src="/2021/12/17/Hive%E5%AE%89%E8%A3%85/clip_image004.jpg" alt="img"></p>
<hr>
<h3 id="9-3-Hive-运行日志信息配置"><a href="#9-3-Hive-运行日志信息配置" class="headerlink" title="9.3    Hive 运行日志信息配置"></a>9.3    Hive 运行日志信息配置</h3><p><code>Hive</code> 的日志文件 <code>log</code> 默认存放在 <code>/tmp/$&#123;用户名&#125;/hive.log</code> 目录下。</p>
<p>修改 <code>Hive</code> 日志存放的目录为 <code>/opt/module/hive/logs</code>。</p>
<ol>
<li><p>备份 <code>/opt/module/hive/conf/hive-log4j.properties.template</code> 文件为 <code>hive-log4j.properties</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">atguigu@hadoop102 conf]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hive/conf</span><br><span class="line">[atguigu@hadoop102 conf]$ <span class="built_in">cp</span> hive-log4j.properties.template hive-log4j.properties</span><br></pre></td></tr></table></figure>

</li>
<li><p>在 <code>hive-log4j.properties</code> 文件中修改 <code>log</code> 存放位置</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure></li>
<li><p>重新启动 <code>hive</code> 时就会发现日志文件目录已经发生了改变</p>
<p> <img src="/2021/12/17/Hive%E5%AE%89%E8%A3%85/image-20211219112048028.png" alt="image-20211219112048028"></p>
</li>
</ol>
<hr>
<h2 id="9-4-参数配置方式"><a href="#9-4-参数配置方式" class="headerlink" title="9.4    参数配置方式"></a>9.4    参数配置方式</h2><p>查看当前所有的配置信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set;</span><br></pre></td></tr></table></figure>

<p>参数的配置三种方式</p>
<ol>
<li><p>配置文件方式</p>
<ul>
<li>  默认配置文件：<code>hive-default.xml</code></li>
<li>  用户自定义配置文件：<code>hive-site.xml</code></li>
<li>  注意：用户自定义配置会覆盖默认配置。另外，Hive 也会读入 Hadoop 的配置，因为 Hive 是作为 Hadoop 的客户端启动的，Hive 的配置会覆盖 Hadoop 的配置。配置文件的设定对本机启动的所有 Hive 进程都有效。</li>
</ul>
</li>
<li><p>命令行参数方式：</p>
<ul>
<li><p>  启动 Hive 时，可以在命令行添加 <code>-hiveconf param=value</code> 来设定参数。</p>
</li>
<li><p>例如：</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br></pre></td></tr></table></figure></li>
<li><p>  注意：仅对本次 Hive 启动有效</p>
</li>
<li><p>查看参数设置：</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>参数声明方式：</p>
<ul>
<li><p>  可以在 <code>HQL</code> 中使用 <code>SET</code> 关键字设定参数</p>
</li>
<li><p>例如：</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks=100;</span><br></pre></td></tr></table></figure>

</li>
<li><p>  注意：仅对本次 Hive 启动有效。</p>
</li>
<li><p>查看参数设置：</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; <span class="built_in">set</span> mapred.reduce.tasks;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<p>上述三种设定方式的优先级依次递增。即 <code>配置文件&lt;命令行参数&lt;参数声明</code>。注意某些系统级的参数，例如 <code>log4j</code> 相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/13/Hive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/13/Hive/" class="post-title-link" itemprop="url">Hive</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-13 16:21:50" itemprop="dateCreated datePublished" datetime="2021-12-13T16:21:50+08:00">2021-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-01-30 12:59:33" itemprop="dateModified" datetime="2022-01-30T12:59:33+08:00">2022-01-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hive/" itemprop="url" rel="index"><span itemprop="name">Hive</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>官网地址：<a target="_blank" rel="noopener" href="https://hive.apache.org/">https://hive.apache.org/</a></p>
<h1 id="第1章-Hive入门"><a href="#第1章-Hive入门" class="headerlink" title="第1章 Hive入门"></a>第1章 Hive入门</h1><h2 id="1-1-什么是Hive"><a href="#1-1-什么是Hive" class="headerlink" title="1.1 什么是Hive"></a>1.1 什么是Hive</h2><p>Hive 是一个由 Facebook 开源的用于解决海量<strong>结构化数据</strong>统计分析的框架。</p>
<p>Hive 只能分析结构化的数据。所以在使用 Hive 之前，需要先对非结构化数据进行 ETL ，转换为结构化的数据。</p>
<p>Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。</p>
<p><strong>Hive 的底层实现原理是：将 HQL 转化成 MapReduce 程序执行。</strong></p>
<p><img src="/2021/12/13/Hive/clip_image002.png" alt="img">   </p>
<ol>
<li> Hive 处理的数据存储在 HDFS 上。</li>
<li> Hive 分析数据底层的实现是 MapReduce</li>
<li> 执行程序运行在 Yarn 上</li>
</ol>
<h3 id="Hive的本质"><a href="#Hive的本质" class="headerlink" title="Hive的本质"></a>Hive的本质</h3><p>Hive 不提供存储功能，通过 Hive 创建的库/表中的业务数据都会以文件的形式存放在 HDFS 上。</p>
<p>与此同时，在建库/表后，会在 MySQL 中生成对应的 <code>schema</code> 数据库存储元数据信息。元数据信息以表的形式存储，常见的表有：</p>
<ul>
<li>  <code>tbls</code>：存放表相关的元数据信息</li>
<li>  <code>dbs</code>：存放库相关的元数据信息</li>
<li>  <code>column_v2</code>：存放列相关的元数据信息 </li>
</ul>
<p><img src="/2021/12/13/Hive/image-20220124205740685.png" alt="image-20220124205740685"></p>
<hr>
<h2 id="1-2-Hive的优缺点"><a href="#1-2-Hive的优缺点" class="headerlink" title="1.2 Hive的优缺点"></a>1.2 Hive的优缺点</h2><h3 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h3><ol>
<li> 操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。</li>
<li> 避免了去写 MapReduce 程序，减少开发人员的学习成本。</li>
<li> Hive 的执行延迟比较高，因此 Hive 常用于数据分析，对实时性要求不高的场合。</li>
<li> Hive 的优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。</li>
<li> Hive 支持用户自定义函数，用户可以根据需求实现自己的函数。</li>
</ol>
<hr>
<h3 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h3><ol>
<li>Hive 的 HQL 表达能力有限<ul>
<li>  迭代式算法无法表达</li>
<li>  数据挖掘方面不擅长</li>
</ul>
</li>
<li>Hive 的效率比较低<ul>
<li>  Hive 自动生成的 MapReduce Job，通常情况下不够智能化</li>
<li>  Hive 调优比较困难，粒度较粗</li>
</ul>
</li>
</ol>
<hr>
<h2 id="1-3-Hive架构原理"><a href="#1-3-Hive架构原理" class="headerlink" title="1.3 Hive架构原理"></a>1.3 Hive架构原理</h2><p><img src="/2021/12/13/Hive/clip_image002-1639754524896.png" alt="img"></p>
<ol>
<li><strong>用户接口：Client</strong><ul>
<li>  CLI（Hive shell）、JDBC/ODBC（Java 访问 Hive）、WEBUI（浏览器访问 Hive）</li>
</ul>
</li>
<li><strong>元数据：Metastore</strong><ul>
<li>  元数据包括：表名、表所属的数据库（默认属于default数据库）、表的拥有者、列/分区字段、表的类型（是否为外部表）、表的数据所在目录等；</li>
<li>  元数据默认存储在 Hive 自带的 <code>derby</code> 数据库中。推荐使用 MySQL 的 <code>metastore</code> 数据库存储数据。</li>
</ul>
</li>
<li><strong>Hadoop</strong><ul>
<li>  使用 HDFS 存储业务数据，使用 MapReduce 进行计算。</li>
</ul>
</li>
<li><strong>驱动器：Driver</strong><ul>
<li>  <strong>解析器（SQL Parser）</strong>：将 SQL 字符串转换成抽象语法树 AST，这一步一般都用第三方工具库完成，比如 <code>antlr</code>；对 AST 进行语法分析，比如表是否存在、字段是否存在、SQL 语义是否有误。</li>
<li>  <strong>编译器（Physical Plan）</strong>：将 AST 编译，生成逻辑执行计划。</li>
<li>  <strong>优化器（Query Optimizer）</strong>：对逻辑执行计划进行优化。</li>
<li>  <strong>执行器（Execution）</strong>：把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来说，就是 <code>MapReduce/Spark</code>。</li>
</ul>
</li>
</ol>
<p><img src="/2021/12/13/Hive/clip_image006.png" alt="img"></p>
<p>Hive 通过给用户提供的一系列交互接口，接收到用户的**指令(SQL)<strong>，使用自己的 <strong>Driver</strong>，结合</strong>元数据(MetaStore)**，将这些指令翻译成 MapReduce Job，提交到 Hadoop 中执行，最后，将执行返回的结果输出到用户交互接口。</p>
<hr>
<h2 id="1-4-Hive和数据库比较"><a href="#1-4-Hive和数据库比较" class="headerlink" title="1.4 Hive和数据库比较"></a>1.4 Hive和数据库比较</h2><p>由于 Hive 采用了类 SQL 查询语言 <code>HQL(Hive Query Language)</code>，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。</p>
<hr>
<h3 id="1-4-1-查询语言"><a href="#1-4-1-查询语言" class="headerlink" title="1.4.1 查询语言"></a>1.4.1 查询语言</h3><p>由于 SQL 被广泛的应用在数据仓库中，因此，专门针对 Hive 的特性设计了类 SQL 的查询语言 HQL。熟悉 SQL 开发的开发者可以很方便的使用 Hive 进行开发。</p>
<h3 id="1-4-2-数据存储位置"><a href="#1-4-2-数据存储位置" class="headerlink" title="1.4.2 数据存储位置"></a>1.4.2 数据存储位置</h3><p>Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。</p>
<h3 id="1-4-3-数据更新"><a href="#1-4-3-数据更新" class="headerlink" title="1.4.3 数据更新"></a>1.4.3 数据更新</h3><p>由于 Hive 是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive 中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此会经常使用 <code>INSERT INTO … VALUES</code> 添加数据，使用 <code>UPDATE … SET</code> 修改数据。</p>
<h3 id="1-4-4-索引"><a href="#1-4-4-索引" class="headerlink" title="1.4.4 索引"></a>1.4.4 索引</h3><p>Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些 key 建立索引。Hive 要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。</p>
<p>由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。</p>
<h3 id="1-4-5-执行"><a href="#1-4-5-执行" class="headerlink" title="1.4.5 执行"></a>1.4.5 执行</h3><p>Hive 中的查询操作默认是通过 Hadoop 提供的 MapReduce 计算框架来实现的。而数据库通常有自己的执行引擎。</p>
<h3 id="1-4-6-执行延迟"><a href="#1-4-6-执行延迟" class="headerlink" title="1.4.6 执行延迟"></a>1.4.6 执行延迟</h3><p>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。</p>
<p>另外一个导致 Hive 执行延迟高的因素是 MapReduce 框架。由于 MapReduce 本身具有较高的延迟，因此在利用 MapReduce 执行 Hive 查询时，也会有较高的延迟。</p>
<p>相对的，数据库的执行延迟较低。当然这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive 的并行计算显然能体现出优势。</p>
<h3 id="1-4-7-可扩展性"><a href="#1-4-7-可扩展性" class="headerlink" title="1.4.7 可扩展性"></a>1.4.7 可扩展性</h3><p>由于 Hive 是建立在 Hadoop 之上的，因此 Hive 的可扩展性是和 Hadoop 的可扩展性是一致的（世界上最大的 Hadoop 集群在 Yahoo，2009 年的规模在 4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle 在理论上的扩展能力也只有 100 台左右。</p>
<h3 id="1-4-8-数据规模"><a href="#1-4-8-数据规模" class="headerlink" title="1.4.8 数据规模"></a>1.4.8 数据规模</h3><p>由于 Hive 建立在集群上并可以利用 MapReduce 进行并行计算，因此可以支持很大规模的数据；</p>
<p>对应的，数据库可以支持的数据规模较小。</p>
<hr>
<h1 id="第2章-Hive安装"><a href="#第2章-Hive安装" class="headerlink" title="第2章 Hive安装"></a>第2章 Hive安装</h1><ul>
<li>  <a target="_blank" rel="noopener" href="https://lvnengdong.github.io/2021/12/17/Hive%E5%AE%89%E8%A3%85/">Hive安装</a></li>
</ul>
<hr>
<h1 id="第3章-Hive数据类型"><a href="#第3章-Hive数据类型" class="headerlink" title="第3章 Hive数据类型"></a>第3章 Hive数据类型</h1><p>Hive 是用 Java 语言实现的，所以 Hive 中的所有数据类型底层还是使用的 Java 数据类型。</p>
<h2 id="3-1-基本数据类型"><a href="#3-1-基本数据类型" class="headerlink" title="3.1 基本数据类型"></a>3.1 基本数据类型</h2><table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
</tr>
</thead>
<tbody><tr>
<td><code>TINYINT</code></td>
<td><code>byte</code></td>
<td><code>1byte</code></td>
</tr>
<tr>
<td><code>SMALLINT</code></td>
<td><code>short</code></td>
<td><code>2byte</code></td>
</tr>
<tr>
<td><code>INT</code></td>
<td><code>int</code></td>
<td><code>4byte</code></td>
</tr>
<tr>
<td><code>BIGINT</code></td>
<td><code>long</code></td>
<td><code>8byte</code></td>
</tr>
<tr>
<td><code>BOOLEAN</code></td>
<td><code>boolean</code></td>
<td></td>
</tr>
<tr>
<td><code>FLOAT</code></td>
<td><code>float</code></td>
<td>单精度浮点数</td>
</tr>
<tr>
<td><code>DOUBLE</code></td>
<td><code>double</code></td>
<td>双精度浮点数</td>
</tr>
<tr>
<td><code>STRING</code></td>
<td><code>string</code></td>
<td></td>
</tr>
<tr>
<td><code>TIMESTAMP</code></td>
<td></td>
<td>时间类型</td>
</tr>
<tr>
<td><code>BINARY</code></td>
<td></td>
<td>字节数组</td>
</tr>
</tbody></table>
<p>Hive 的 String 类型相当于数据库的 varchar 类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储 2GB 的字符数。</p>
<hr>
<h2 id="3-2-集合数据类型"><a href="#3-2-集合数据类型" class="headerlink" title="3.2 集合数据类型"></a>3.2 集合数据类型</h2><table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td><strong>STRUCT</strong></td>
<td>和 C 语言中的 <code>struct</code> 类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是 <code>STRUCT&#123;first STRING, last STRING&#125;</code>，那么第 1 个元素可以通过 <code>字段.first</code> 来引用。</td>
<td><code>struct()</code></td>
</tr>
<tr>
<td><code>MAP</code></td>
<td>MAP 是一组键值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是 MAP，其中键值对是 <code>’first’-&gt;’John’</code> 和 <code>’last’-&gt;’Doe’</code>，那么可以通过 <code>字段名[‘last’]</code> 获取最后一个元素</td>
<td><code>map()</code></td>
</tr>
<tr>
<td><code>ARRAY</code></td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为 <code>[‘John’,  ‘Doe’]</code>，那么第 2 个元素可以通过 <code>数组名[1]</code> 进行引用。</td>
<td><code>Array()</code></td>
</tr>
</tbody></table>
<p>Hive 有三种复杂数据类型 <code>ARRAY</code>、<code>MAP</code> 和 <code>STRUCT</code>。<code>ARRAY</code> 和 <code>MAP</code> 与 Java 中的 <code>Array</code> 和 <code>Map</code> 类似，而 <code>STRUCT</code> 与 C 语言中的<code>Struct</code> 类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p>
<h3 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h3><ol>
<li><p>假设某表有如下一行，我们用 JSON 格式来表示其数据结构。在 <code>Hive</code> 下访问的格式为：</p>
 <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;songsong&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;friends&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;bingbing&quot;</span> <span class="punctuation">,</span> <span class="string">&quot;lili&quot;</span><span class="punctuation">]</span> <span class="punctuation">,</span>       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">&quot;children&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>                      <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span><span class="punctuation">:</span> <span class="number">18</span> <span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span><span class="punctuation">:</span> <span class="number">19</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>                      <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">&quot;street&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hui long guan&quot;</span> <span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;city&quot;</span><span class="punctuation">:</span> <span class="string">&quot;beijing&quot;</span> </span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

</li>
<li><p>基于上述数据结构，我们在 Hive 里创建对应的表，并导入数据。</p>
<ul>
<li>  创建本地测试文件 <code>test.txt</code></li>
</ul>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure>

<ul>
<li>  注意：<code>MAP</code>，<code>STRUCT</code> 和 <code>ARRAY</code> 里的元素间的分隔符都可以用同一个字符表示，这里用 <code>_</code>。</li>
</ul>
</li>
<li><p>  <code>Hive</code> 上创建测试表 <code>test</code></p>
</li>
</ol>
<pre><code><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test(</span><br><span class="line">    name string,</span><br><span class="line">    friends <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>,</span><br><span class="line">    children map<span class="operator">&lt;</span>string, <span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">    address struct<span class="operator">&lt;</span>street:string, city:string<span class="operator">&gt;</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">collection items terminated <span class="keyword">by</span> <span class="string">&#x27;_&#x27;</span></span><br><span class="line">map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--------------</span></span><br><span class="line">字段解释：</span><br><span class="line"></span><br><span class="line">    <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span> <span class="comment">-- 列分隔符</span></span><br><span class="line">    collection items terminated <span class="keyword">by</span> <span class="string">&#x27;_&#x27;</span>     <span class="comment">--MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</span></span><br><span class="line">    map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span>               <span class="comment">-- MAP中的key与value的分隔符</span></span><br><span class="line">    lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;                  <span class="comment">-- 行分隔符</span></span><br><span class="line"><span class="comment">--------------</span></span><br></pre></td></tr></table></figure>
</code></pre>
<ol start="4">
<li><p>导入文本数据到测试表</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/home/lvnengdong/mytmp/test.txt&#x27; into table test;</span><br></pre></td></tr></table></figure></li>
<li><p>访问三种集合列里的数据，以下分别是 <code>ARRAY</code>，<code>MAP</code>，<code>STRUCT</code> 的访问方式</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	select friends[1],children[&#x27;xiao song&#x27;],address.city </span><br><span class="line">	from test</span><br><span class="line">	where name=&quot;songsong&quot;;</span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line">_c0     _c1     city</span><br><span class="line">lili    18      beijing</span><br><span class="line">Time taken: 0.076 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h2 id="3-3-类型转化"><a href="#3-3-类型转化" class="headerlink" title="3.3 类型转化"></a>3.3 类型转化</h2><p><code>Hive</code> 的原子数据类型是可以进行隐式转换的，类似于 Java 的类型转换，例如某表达式使用 <code>INT</code> 类型，<code>TINYINT</code> 会自动转换为 <code>INT</code> 类型，但是 <code>Hive</code> 不会进行反向转化，例如，某表达式使用 <code>TINYINT</code> 类型，<code>INT</code> 不会自动转换为 <code>TINYINT</code> 类型，它会返回错误，除非使用 <code>CAST</code> 操作。</p>
<p><strong>一、隐式类型转换规则如下：</strong></p>
<ol>
<li> 任何整数类型都可以隐式地转换为一个范围更广的类型，如 <code>TINYINT</code> 可以转换成 <code>INT</code>，<code>INT</code> 可以转换成 <code>BIGINT</code>。</li>
<li> <code>所有整数类型</code>、<code>FLOAT</code> 和 <code>STRING</code> 类型都可以隐式地转换成 <code>DOUBLE</code>。</li>
<li> <code>TINYINT</code>、<code>SMALLINT</code>、<code>INT</code> 都可以转换为 <code>FLOAT</code>。</li>
<li> <code>BOOLEAN</code> 类型<strong>不可以</strong>转换为任何其它的类型。</li>
</ol>
<p><strong>二、可以使用 <code>CAST</code> 操作显式进行数据类型转换</strong></p>
<p>例如 <code>CAST(&#39;1&#39; AS INT)</code> 将把 <code>字符串&#39;1&#39;</code> 转换成 <code>整数1</code>；如果强制类型转换失败，如执行 <code>CAST(&#39;X&#39; AS INT)</code>，表达式返回空值 <code>NULL</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select cast(&quot;1&quot; AS int);</span><br></pre></td></tr></table></figure>



<hr>
<h1 id="第4章-DDL"><a href="#第4章-DDL" class="headerlink" title="第4章 DDL"></a>第4章 DDL</h1><blockquote>
<p>  <strong>DDL；Data Definition Language；数据定义语言</strong></p>
</blockquote>
<h2 id="4-1-库操作"><a href="#4-1-库操作" class="headerlink" title="4.1 库操作"></a>4.1 库操作</h2><h3 id="4-1-1-增：创建数据库"><a href="#4-1-1-增：创建数据库" class="headerlink" title="4.1.1 增：创建数据库"></a>4.1.1 增：创建数据库</h3><p><strong>语法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE [REMOTE] (DATABASE|SCHEMA) [IF NOT EXISTS] database_name		# DATABASE 和 SCHEMA 都表示创建数据库，DATABASE 在新版本中使用，SCHEMA 在老版本中使用</span><br><span class="line">  [COMMENT database_comment]	# 关于数据库的注释</span><br><span class="line">  [LOCATION hdfs_path]	# 数据库在 HDFS 上的路径</span><br><span class="line">  [MANAGEDLOCATION hdfs_path]</span><br><span class="line">  [WITH DBPROPERTIES (property_name=property_value, ...)];	# 库的 dbproperties 属性</span><br></pre></td></tr></table></figure>



<p><strong>Demo：</strong></p>
<ol>
<li><p>使用默认配置创建数据库。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure>

<ul>
<li>  数据库在 HDFS 上的默认存储路径是 <code>/user/hive/warehouse/*.db</code></li>
</ul>
</li>
<li><p>创建数据库时手动指定数据库在 HDFS 上的存储路径。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">create database db_hive2 \</span><br><span class="line">location &#x27;/custom_hive.db&#x27;;</span><br></pre></td></tr></table></figure>

<p> <img src="/2021/12/13/Hive/image-20211219121900037.png" alt="image-20211219121900037"></p>
</li>
</ol>
<hr>
<h3 id="4-1-2-删：删除数据库"><a href="#4-1-2-删：删除数据库" class="headerlink" title="4.1.2 删：删除数据库"></a>4.1.2 删：删除数据库</h3><p><strong>语法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 情况1： </span><br><span class="line">drop database 库名;   # drop 只能删除空数据库</span><br><span class="line"></span><br><span class="line"># 情况2：</span><br><span class="line">drop database db_hive cascade;  # 强制删除，可以删除非空的库</span><br></pre></td></tr></table></figure>



<p><strong>Demo：</strong></p>
<ol>
<li><p>删除空数据库</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database if exists db_hive;</span><br></pre></td></tr></table></figure></li>
<li><p>删除非空数据库。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</span><br><span class="line"></span><br><span class="line"># 如果数据库不为空，可以采用 cascade 命令，强制删除数据库</span><br><span class="line">hive&gt; drop database db_hive cascade;</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="4-1-3-改：修改数据库"><a href="#4-1-3-改：修改数据库" class="headerlink" title="4.1.3 改：修改数据库"></a>4.1.3 改：修改数据库</h3><p>Hive 允许修改的数据库的部分元数据信息。可以修改的数据在不同的 Hive 版本中是不一样的。以下是一些常见的允许修改的数据库信息：</p>
<ol>
<li> <code>DBPROPERTIES</code>：数据库描述信息</li>
<li> <code>OWNER</code>：数据库属主</li>
<li> <code>LOCATION</code>：数据库保存的位置</li>
<li> <code>MANAGEDLOCATION</code>：数据库管理位置</li>
</ol>
<p>数据库的其它元数据信息都是不可更改的，包括数据库名等等。</p>
<p><strong>语法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...);   -- (Note: SCHEMA added in Hive 0.14.0)</span><br><span class="line"> </span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role;   -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)</span><br><span class="line">  </span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later)</span><br><span class="line"> </span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET MANAGEDLOCATION hdfs_path; -- (Note: Hive 4.0.0 and later)</span><br></pre></td></tr></table></figure>





<p><strong>Demo</strong></p>
<ol>
<li><p>使用 <code>ALTER DATABASE</code> 命令来修改数据库的 <code>DBPROPERTIES</code>信息。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	alter database db_hive set dbproperties(&#x27;createtime&#x27;=&#x27;20170830&#x27;);</span><br><span class="line">	# 新增 &#x27;createtime&#x27;=&#x27;20170830&#x27; 信息。</span><br></pre></td></tr></table></figure></li>
<li><p>在 <code>Hive</code> 中查看修改结果：<code>createtime=20170830</code> 是新增的属性。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">db_name comment location        owner_name      owner_type      parameters</span><br><span class="line">db_hive		hdfs://hadoop102:9000/hive/warehouse/db_hive.db	lvnengdong	USER    &#123;createtime=20170830&#125;</span><br></pre></td></tr></table></figure>

<p> <strong>注意：</strong>在修改 <code>DBPROPERTIES</code> 中的数据时，同名的属性值会覆盖，非同名的属性会新增。</p>
</li>
</ol>
<hr>
<h3 id="4-1-4-查：查询数据库"><a href="#4-1-4-查：查询数据库" class="headerlink" title="4.1.4 查：查询数据库"></a>4.1.4 查：查询数据库</h3><h4 id="1、显示数据库"><a href="#1、显示数据库" class="headerlink" title="1、显示数据库"></a>1、显示数据库</h4><ol>
<li><p>显示当前连接管理的所有数据库</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure></li>
<li><p>过滤显示查询的数据库</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases like &#x27;db_hive*&#x27;;</span><br><span class="line">OK</span><br><span class="line">db_hive</span><br><span class="line">db_hive_1</span><br></pre></td></tr></table></figure></li>
<li><p>查看某个数据库中有哪些表</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 语法：show tables in database</span><br><span class="line"></span><br><span class="line">hive (default)&gt; show tables in default;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">person</span><br><span class="line">student</span><br><span class="line">test</span><br><span class="line">Time taken: 0.038 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h4 id="2、查看数据库的描述信息"><a href="#2、查看数据库的描述信息" class="headerlink" title="2、查看数据库的描述信息"></a>2、查看数据库的描述信息</h4><ol>
<li><p>查看数据库的描述信息</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;  desc database db_hive;</span><br><span class="line">OK</span><br><span class="line">db_name	comment	location	owner_name	owner_type	parameters</span><br><span class="line">db_hive		hdfs://hadoop102:9000/hive/warehouse/db_hive.db	lvnengdong	USER	</span><br><span class="line">Time taken: 0.026 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>

</li>
<li><p>显示数据库详细信息，<code>extended</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    hive (default)&gt; desc database extended db_hive;</span><br><span class="line">    OK</span><br><span class="line">    db_name	comment	location	owner_name	owner_type	parameters</span><br><span class="line">    db_hive		hdfs://hadoop102:9000/hive/warehouse/db_hive.db	lvnengdong	USER	</span><br><span class="line">Time taken: 0.019 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<h4 id="3、切换数据库上下文"><a href="#3、切换数据库上下文" class="headerlink" title="3、切换数据库上下文"></a>3、切换数据库上下文</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="4-2-表操作"><a href="#4-2-表操作" class="headerlink" title="4.2 表操作"></a>4.2 表操作</h2><h3 id="4-2-1-增：创建表"><a href="#4-2-1-增：创建表" class="headerlink" title="4.2.1 增：创建表"></a>4.2.1 增：创建表</h3><p><strong>理论知识：</strong></p>
<p>Hive 中元数据和业务数据是分开存储的，元数据存储在 MySQL 中，业务数据存储在 HDFS 上。</p>
<p>所以在执行<strong>建表</strong>语句时，</p>
<ol>
<li> Hive 会在 HDFS 上生成存储业务数据的目录，业务数据会以文件的形式存储在该目录下。</li>
<li> 同时 Hive 还会向 MySQL 的 metastore 数据库中插入表的元数据信息。</li>
</ol>
<p>在执行<strong>删除表</strong>的语句时：</p>
<ul>
<li>对于<strong>内部表</strong>，将会删除两个地方的数据。<ol>
<li> MySQL 的 metastore 数据库中的元数据信息；</li>
<li> HDFS 中对应目录下的业务数据文件。</li>
</ol>
</li>
<li>  对于<strong>外部表</strong>，只删除 MySQL 的 metastore 数据库中的元数据信息，而不会删除 HDFS 中的业务数据。</li>
</ul>
<p><strong>建表语法</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name </span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)] 	# 表中的字段信息（包括字段的注释）</span><br><span class="line">[COMMENT table_comment] # 表的注释信息</span><br><span class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[CLUSTERED BY (col_name, col_name, ...) 	# 分桶表</span><br><span class="line">[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] # 分桶之后对桶中的数据排序</span><br><span class="line">[ROW FORMAT row_format] # 表中每行数据的格式</span><br><span class="line">[STORED AS file_format]	# 表中的数据要以哪种格式来存储（如：textfile/sequencefile 等，默认为textfile）</span><br><span class="line">[LOCATION hdfs_path]	# 表在 HDFS 上的位置</span><br></pre></td></tr></table></figure>



<p><strong>字段解释说明（部分）：</strong></p>
<ol>
<li><p><strong>EXTERNAL</strong>：声明要创建一个外部表。需要配合 <strong>LOCATION</strong> 共同使用，LOCATION 关键字的作用是指定保存业务数据的文件的存储路径。</p>
<blockquote>
<p>  <strong>外部表 vs. 内部表</strong></p>
<ol>
<li>建表时，如果不使用关键字 EXTERNAL，默认创建的表就是一个 MANAGED_TABLE（内部表/管理表）；如果建表时显式地使用了 EXTERNAL 关键字，那么创建的表就是一个外部表。</li>
<li>外部表和内部表的区别是：<ul>
<li>  内部表在执行删除操作时，会将表的元数据（schema）信息和表中的记录数据一起删除。</li>
<li>  外部表在执行删除操作时，只删除表的元数据（schema）信息。</li>
</ul>
</li>
<li>Hive 在创建内部表时，会将数据<strong>拷贝</strong>到 Location 关键字指向的路径。若创建外部表，则仅记录业务数据文件所在的路径，而不对数据文件做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</li>
</ol>
</blockquote>
</li>
<li><p> <strong>COMMENT</strong>：注释。</p>
</li>
<li><p> <strong>PARTITIONED BY</strong>：创建分区表</p>
</li>
<li><p> <strong>CLUSTERED BY</strong>：创建分桶表</p>
</li>
<li><p> <strong>SORTED BY</strong>：排序。</p>
</li>
<li><p><strong>ROW FORMAT</strong>：表中每行数据的格式</p>
<ul>
<li>  SerDe 是 Serialize/Deserilize 的简称，目的是用于序列化和反序列化。</li>
<li>  用户在建表的时候可以自定义 SerDe 或者使用默认的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用默认的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe 确定表的具体的列的数据。</li>
</ul>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DELIMITED </span><br><span class="line">[FIELDS TERMINATED BY char] </span><br><span class="line">[COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">[MAP KEYS TERMINATED BY char] </span><br><span class="line">[LINES TERMINATED BY char] </span><br><span class="line"> | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure>

</li>
<li><p><strong>STORED AS</strong>：指定表中的数据以什么样的文件类型类型存储。常用的文件类型有：</p>
<ul>
<li>  SEQUENCEFILE（二进制序列文件）</li>
<li>  TEXTFILE（文本）</li>
<li>  RCFILE（列式存储格式文件）</li>
</ul>
<p> 如果文件数据是纯文本，可以使用 <code>STORED AS TEXTFILE</code>。如果数据需要压缩，使用 <code>STORED AS SEQUENCEFILE</code>。</p>
</li>
<li><p> <strong>LOCATION</strong>：用于指定当前表的业务数据文件在 HDFS 上的存储位置。</p>
</li>
<li><p> <strong>LIKE</strong>：允许用户复制现有的表结构，但不复制表数据。</p>
</li>
</ol>
<hr>
<h4 id="1-内部表（管理表）"><a href="#1-内部表（管理表）" class="headerlink" title="1 内部表（管理表）"></a>1 内部表（管理表）</h4><p><strong>理论</strong></p>
<p>Hive 默认创建的表都是<strong>管理表（内部表）</strong>，创建外部表需要显示使用 <strong>EXTERNAL</strong> 关键字。</p>
<p>对于内部表，Hive 控制着业务数据的生命周期。Hive 默认情况下会将表的业务数据文件存储在由配置属性 <code>hive.metastore.warehouse.dir</code> 所定义的目录的下，例如 <code>/user/hive/warehouse</code>。当我们删除一个内部表时，Hive 不仅会删除 MySQL 中 metastore 数据库中表的元数据信息，同时也会删除这个目录下的业务数据文件。</p>
<p>内部表不适合和其他工具共享数据。</p>
<p><strong>Demo</strong></p>
<ol>
<li><p>普通创建表（默认创建的就是一个内部表）</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists student2</span><br><span class="line">	(  </span><br><span class="line">        id int, </span><br><span class="line">        name string  </span><br><span class="line">    )</span><br><span class="line">    row format delimited fields terminated by &#x27;\t&#x27;  </span><br><span class="line">    stored as textfile  </span><br><span class="line">    location &#x27;/user/hive/warehouse/student2&#x27;;    </span><br></pre></td></tr></table></figure>

</li>
<li><p>根据查询结果创建表，同时查询的结果集会添加到新创建的表中</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists student3 </span><br><span class="line">as </span><br><span class="line">select id, name from student;  </span><br></pre></td></tr></table></figure>

</li>
<li><p>根据已经存在的表结构创建表</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists student4 </span><br><span class="line">like student;</span><br></pre></td></tr></table></figure>

</li>
<li><p>查询表的描述信息</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:       MANAGED_TABLE </span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h4 id="2-外部表"><a href="#2-外部表" class="headerlink" title="2 外部表"></a>2 外部表</h4><p><strong>理论</strong></p>
<p>对于外部表，Hive 并非完全拥有表的业务数据，所以在删除外部表时并不会删除业务数据，而只是删除描述表的元数据信息。</p>
<p>一般情况下，MySQL 数据库中的业务数据会以文件的格式上传到 HDFS 中的某个目录下。对于内部表而言，会将业务数据拷贝一份到 Hive 指定的目录下，所以可以说 Hive 拥有这份业务数据文件的所有权限，对该文件执行写/删除操作不会影响 HDFS 中最初的源文件。而对于外部表而言，Hive 直接引用 HDFS 上源文件的地址作为 Hive 读取业务数据的来源，所以 Hive 不能轻易去修改/删除这个源文件，因为该源文件很有可能还被其它程序使用。</p>
<p><strong>内部表和外部表的使用场景：</strong></p>
<p>每天将收集到的网站日志定期流入 HDFS 文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的<strong>中间表、结果表</strong>使用内部表存储，数据通过 <code>SELECT+INSERT</code> 进入内部表。</p>
<p><strong>Demo：</strong></p>
<p>分别创建部门和员工外部表，并向表中导入数据。</p>
<ol>
<li><p>原始数据：</p>
<p> <code>/opt/module/datas/dept.txt</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">10	ACCOUNTING	1700</span><br><span class="line">20	RESEARCH	1800</span><br><span class="line">30	SALES	1900</span><br><span class="line">40	OPERATIONS	1700</span><br></pre></td></tr></table></figure>

<p> <code>/opt/module/datas/emp.txt</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">7369	SMITH	CLERK	7902	1980-12-17	800.00		20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-2-20	1600.00	300.00	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-2-22	1250.00	500.00	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-4-2	2975.00		20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-9-28	1250.00	1400.00	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-5-1	2850.00		30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.00		10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-4-19	3000.00		20</span><br><span class="line">7839	KING	PRESIDENT		1981-11-17	5000.00		10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-9-8	1500.00	0.00	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-5-23	1100.00		20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-3	950.00		30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-3	3000.00		20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.00		10</span><br></pre></td></tr></table></figure>

</li>
<li><p>建表语句</p>
<ul>
<li><p>创建部门表</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists default.dept</span><br><span class="line">	(  </span><br><span class="line">        deptno  int,  </span><br><span class="line">        dname  string,  </span><br><span class="line">        loc  int  </span><br><span class="line">    )  </span><br><span class="line">    row  format delimited fields terminated by &#x27;\t&#x27;;  </span><br></pre></td></tr></table></figure>

</li>
<li><p>创建员工表</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists default.emp</span><br><span class="line">	(  </span><br><span class="line">        empno  int,  </span><br><span class="line">        ename  string,  </span><br><span class="line">        job  string,  </span><br><span class="line">        mgr  int,  </span><br><span class="line">        hiredate  string,   </span><br><span class="line">        sal  double,   </span><br><span class="line">        comm  double,  </span><br><span class="line">        deptno  int</span><br><span class="line">    )  </span><br><span class="line">    row  format delimited fields terminated by &#x27;\t&#x27;;  </span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
<li><p>查看创建的表</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">dept</span><br><span class="line">emp</span><br></pre></td></tr></table></figure>

</li>
<li><p>向外部表中导入数据</p>
<ul>
<li>  导入数据</li>
</ul>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept;</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/emp.txt&#x27; into table default.emp;</span><br></pre></td></tr></table></figure>

<ul>
<li>  查询结果</li>
</ul>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp;</span><br><span class="line">hive (default)&gt; select * from dept;</span><br></pre></td></tr></table></figure>

</li>
<li><p>查看表格式化数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted dept;</span><br><span class="line">Table Type:       EXTERNAL_TABLE</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h4 id="3-内部表-lt-gt-外部表"><a href="#3-内部表-lt-gt-外部表" class="headerlink" title="3 内部表 &lt;=&gt; 外部表"></a>3 内部表 &lt;=&gt; 外部表</h4><p>想要实现内部表与外部表之间的相互转换，只需要切换表的 <code>Table Type: MANAGED_TABLE | EXTERNAL_TABLE</code> 属性就可以了。具体的实现方式如下：</p>
<ol>
<li><p>查询表的类型</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE	# 管理表，也就是内部表</span><br></pre></td></tr></table></figure></li>
<li><p>修改内部表 <code>student2</code> 为外部表</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table student2 set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;TRUE&#x27;);	# 打开外部表的开关</span><br></pre></td></tr></table></figure></li>
<li><p>查询表的类型</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             EXTERNAL_TABLE	# 表的类型变成了外部表</span><br></pre></td></tr></table></figure></li>
<li><p>修改外部表 <code>student2</code> 为内部表</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table student2 set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;FALSE&#x27;);	# 关闭外部表的开关，就会自动变为内部表了</span><br></pre></td></tr></table></figure></li>
<li><p>查询表的类型</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>注意：</strong>在 Hive 中查询语句不区分大小写，但是在参数中严格区分大小写。所以 <code>(&#39;EXTERNAL&#39;=&#39;TRUE&#39;)</code> 和 <code>(&#39;EXTERNAL&#39;=&#39;FALSE&#39;)</code> 为固定写法，不能小写。</p>
<hr>
<h3 id="4-2-2-删：删除表"><a href="#4-2-2-删：删除表" class="headerlink" title="4.2.2 删：删除表"></a>4.2.2 删：删除表</h3><h4 id="彻底删除表"><a href="#彻底删除表" class="headerlink" title="彻底删除表"></a>彻底删除表</h4><p><strong>语法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop table 表名;</span><br></pre></td></tr></table></figure>

<p><strong>Demo：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop table dept_partition;</span><br></pre></td></tr></table></figure>





<h4 id="清空表中数据，但保留表结构"><a href="#清空表中数据，但保留表结构" class="headerlink" title="清空表中数据，但保留表结构"></a>清空表中数据，但保留表结构</h4><p><strong>语法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truncate table 表名;</span><br></pre></td></tr></table></figure>

<p><strong>Demo：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; truncate table dept_partition;</span><br></pre></td></tr></table></figure>

<p><strong>Tip：</strong>只能清空内部表的业务的数据，无法清空外部表的业务数据，因为外部表的业务数据不属于 Hive 管理。</p>
<hr>
<h3 id="4-2-3-改"><a href="#4-2-3-改" class="headerlink" title="4.2.3 改"></a>4.2.3 改</h3><h4 id="4-2-3-1-重命名表"><a href="#4-2-3-1-重命名表" class="headerlink" title="4.2.3.1 重命名表"></a>4.2.3.1 重命名表</h4><p><strong>语法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name RENAME TO new_table_name</span><br></pre></td></tr></table></figure>

<p><strong>Demo：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="4-2-3-2-增加、修改和删除表分区"><a href="#4-2-3-2-增加、修改和删除表分区" class="headerlink" title="4.2.3.2 增加、修改和删除表分区"></a>4.2.3.2 增加、修改和删除表分区</h4><p>详见 4.6.1 分区表基本操作。</p>
<hr>
<h4 id="4-2-3-3-增加-修改-替换列信息"><a href="#4-2-3-3-增加-修改-替换列信息" class="headerlink" title="4.2.3.3 增加/修改/替换列信息"></a>4.2.3.3 增加/修改/替换列信息</h4><p><strong>语法</strong></p>
<ol>
<li><p>更新列</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name </span><br><span class="line">CHANGE [COLUMN] col_old_name col_new_name column_type </span><br><span class="line">[COMMENT col_comment] </span><br><span class="line">[FIRST|AFTER column_name]	# 调整列的顺序（FIRST表示调整到第一列）</span><br></pre></td></tr></table></figure></li>
<li><p>增加和重设列</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name</span><br><span class="line">&#123;ADD | REPLACE&#125; COLUMNS (col_name data_type [COMMENT col_comment], ...) </span><br><span class="line"></span><br><span class="line"># 注：ADD 代表新增一字段，字段位置在所有列后面(partition列前)，</span><br><span class="line"># REPLACE 则是表示重设表中所有字段。</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>Demo：</strong></p>
<ol>
<li><p>查询表结构</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure></li>
<li><p>添加列</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add columns(deptdesc string);</span><br></pre></td></tr></table></figure></li>
<li><p>更新列</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition change column deptdesc desc int;</span><br></pre></td></tr></table></figure></li>
<li><p>替换列</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	alter table dept_partition </span><br><span class="line">	replace columns(</span><br><span class="line">        deptno string, </span><br><span class="line">        dname string, </span><br><span class="line">        loc string</span><br><span class="line">    );</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="4-2-4-查"><a href="#4-2-4-查" class="headerlink" title="4.2.4 查"></a>4.2.4 查</h3><p><strong>一、查看表的描述（概要信息）</strong></p>
<ul>
<li><p><strong>语法：</strong> </p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc 表名;</span><br></pre></td></tr></table></figure></li>
<li><p><strong>Demo</strong></p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 查看 student 表的描述信息</span><br><span class="line">hive (default)&gt; desc student;</span><br><span class="line">OK</span><br><span class="line">col_name	data_type	comment</span><br><span class="line">id                  	int                 	                    </span><br><span class="line">name                	string              	                    </span><br><span class="line">Time taken: 0.133 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>二、查看表的描述（详细信息）</strong></p>
<ul>
<li><p>语法：</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc extended 表名;</span><br></pre></td></tr></table></figure></li>
<li><p>Demo：</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc extended student;</span><br><span class="line">OK</span><br><span class="line">col_name	data_type	comment</span><br><span class="line">id                  	int                 	                    </span><br><span class="line">name                	string              	                    </span><br><span class="line">	 	 </span><br><span class="line">Detailed Table Information	Table(tableName:student, dbName:default, owner:lvnengdong, createTime:1639841380, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:null), FieldSchema(name:name, type:string, comment:null)], location:hdfs://hadoop102:9000/user/hive/warehouse/student, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:&#123;serialization.format=1&#125;), bucketCols:[], sortCols:[], parameters:&#123;&#125;, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:&#123;&#125;), storedAsSubDirectories:false), partitionKeys:[], parameters:&#123;totalSize=12, numRows=-1, rawDataSize=-1, COLUMN_STATS_ACCURATE=false, numFiles=1, transient_lastDdlTime=1639841380&#125;, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	</span><br><span class="line">Time taken: 0.096 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure></li>
<li><p>  可以看到详情信息非常的乱，这时候就可以使用另一条命令以格式化的形式输出表的详情信息</p>
</li>
</ul>
<p><strong>三、查看表详情（以格式化后的形式展示在控制台上）</strong></p>
<ul>
<li><p>语法：</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc formatted 表名;</span><br></pre></td></tr></table></figure></li>
<li><p>Demo：</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student;</span><br><span class="line">OK</span><br><span class="line">col_name	data_type	comment</span><br><span class="line"># col_name            	data_type           	comment             </span><br><span class="line">	 	 </span><br><span class="line">id                  	int                 	                    </span><br><span class="line">name                	string              	                    </span><br><span class="line">	 	 </span><br><span class="line"># Detailed Table Information	 	 </span><br><span class="line">Database:           	default             	 </span><br><span class="line">Owner:              	lvnengdong          	 </span><br><span class="line">CreateTime:         	Sat Dec 18 23:29:40 CST 2021	 </span><br><span class="line">LastAccessTime:     	UNKNOWN             	 </span><br><span class="line">Protect Mode:       	None                	 </span><br><span class="line">Retention:          	0                   	 </span><br><span class="line">Location:           	hdfs://hadoop102:9000/user/hive/warehouse/student	 </span><br><span class="line">Table Type:         	MANAGED_TABLE       	</span><br><span class="line">Table Parameters:	 	 </span><br><span class="line">	COLUMN_STATS_ACCURATE	false               </span><br><span class="line">	numFiles            	1                   </span><br><span class="line">	numRows             	-1                  </span><br><span class="line">	rawDataSize         	-1                  </span><br><span class="line">	totalSize           	12                  </span><br><span class="line">	transient_lastDdlTime	1639841380          </span><br><span class="line">	 	 </span><br><span class="line"># Storage Information	 	 </span><br><span class="line">SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 </span><br><span class="line">InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 </span><br><span class="line">OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 </span><br><span class="line">Compressed:         	No                  	 </span><br><span class="line">Num Buckets:        	-1                  	 </span><br><span class="line">Bucket Columns:     	[]                  	 </span><br><span class="line">Sort Columns:       	[]                  	 </span><br><span class="line">Storage Desc Params:	 	 </span><br><span class="line">	serialization.format	1                   </span><br><span class="line">Time taken: 0.907 seconds, Fetched: 32 row(s)</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h2 id="4-3-分区表"><a href="#4-3-分区表" class="headerlink" title="4.3 分区表"></a>4.3 分区表</h2><h3 id="4-3-1-是什么？"><a href="#4-3-1-是什么？" class="headerlink" title="4.3.1 是什么？"></a>4.3.1 是什么？</h3><p>Hive 表中的业务数据文件保存在 HDFS 中。<strong>默认情况</strong>下（也就是没有显式指定分区的情况下），每张表映射到 HDFS 中有一个存储目录，业务数据就保存在该目录下。</p>
<p>因为 Hive 表是没有索引的，所以检索表中数据的方式只能是遍历。在某些情况下，我们执行的查询操作并不想要检索整张表，而只想要检索表的一部分数据，为了细化数据的粒度，Hive 引入了<code>分区</code>的概念。<strong>分区就是把整张表的数据根据业务需要分割成多个小的数据集，对应到 HDFS 上就是在表目录下再创建多个子目录（分区目录），将对应的子数据集存储在分区目录中。</strong>在执行查询语句时通过 WHERE 子句中的条件查询指定的分区，提高查询效率。</p>
<p><strong>Tip</strong></p>
<ul>
<li>  只有分区表才有分区目录（子目录）</li>
<li>  分区目录的名称由两部分决定：<code>分区列列名=分区列列值</code></li>
</ul>
<h3 id="4-3-2-分区表语法"><a href="#4-3-2-分区表语法" class="headerlink" title="4.3.2 分区表语法"></a>4.3.2 分区表语法</h3><p><strong>分区表语法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] </span><br></pre></td></tr></table></figure>



<p>引入分区表（需要根据日期对日志进行管理）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse/log_partition/20170702/20170702.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170703/20170703.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170704/20170704.log</span><br></pre></td></tr></table></figure>





<h3 id="4-3-3-分区表基本操作"><a href="#4-3-3-分区表基本操作" class="headerlink" title="4.3.3 分区表基本操作"></a>4.3.3 分区表基本操作</h3><h4 id="1、准备数据"><a href="#1、准备数据" class="headerlink" title="1、准备数据"></a>1、准备数据</h4><p>准备数据：<code>/opt/module/datas/dept.txt</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">10	ACCOUNTING	1700</span><br><span class="line">20	RESEARCH	1800</span><br><span class="line">30	SALES	1900</span><br><span class="line">40	OPERATIONS	1700</span><br></pre></td></tr></table></figure>

<h4 id="2、创建分区表"><a href="#2、创建分区表" class="headerlink" title="2、创建分区表"></a>2、创建分区表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	create table dept_partition(</span><br><span class="line">        deptno int, </span><br><span class="line">        dname string, </span><br><span class="line">        loc string</span><br><span class="line">    )</span><br><span class="line">    partitioned by (month string)	# 创建分区表时要额外指定分区信息</span><br><span class="line">    row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>



<h4 id="3、导入数据（失败）"><a href="#3、导入数据（失败）" class="headerlink" title="3、导入数据（失败）"></a>3、导入数据（失败）</h4><p>加载数据到分区表 <code>dept_partition</code> 中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table dept_partition;</span><br><span class="line">FAILED: SemanticException [Error 10062]: Need to specify partition columns because the destination table is partitioned</span><br><span class="line"># 导入数据失败，因为目标表是一个分区表，所以在导入数据时必须数据要导入到哪一个分区目录中</span><br></pre></td></tr></table></figure>

<p>如果当前表是一个分区表，在导入数据时，必须指定向哪个分区导入数据。因为对于分区表来说，在 HDFS 的表目录下还有多个分区目录，对应的数据应该导入到这些分区目录中。</p>
<h4 id="4、增加分区"><a href="#4、增加分区" class="headerlink" title="4、增加分区"></a>4、增加分区</h4><p><strong>增加分区</strong>：目前我们还没有创建分区，所以我们首先要在当前表下创建分区。</p>
<ul>
<li><p>创建单个分区</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># alter table 表名 add partition(分区字段名=分区字段值);</span><br><span class="line">hive (default)&gt; </span><br><span class="line">	hive (default)&gt; alter table dept_partition add partition(month=&#x27;202112&#x27;);</span><br></pre></td></tr></table></figure></li>
<li><p>同时创建多个分区</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	alter table dept_partition </span><br><span class="line">	add partition(month=&#x27;202111&#x27;) partition(month=&#x27;202110&#x27;);</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="新增分区（方式一）："><a href="#新增分区（方式一）：" class="headerlink" title="新增分区（方式一）："></a>新增分区（方式一）：</h5><p>在给分区表新增分区时：</p>
<ol>
<li><p>在 HDFS 上会生成分区路径。查看 HDFS 的分区目录结构：</p>
<p> <img src="/2021/12/13/Hive/image-20211219223705371.png" alt="image-20211219223705371"></p>
</li>
<li><p>在 MySQL 中的 <code>metastore.partitions</code> 表中会生成分区相关的元数据信息</p>
<p> <img src="/2021/12/13/Hive/image-20211220102736063.png" alt="image-20211220102736063"></p>
<ul>
<li>  <strong>PART_NAME</strong>：保存了分区列的信息；</li>
<li>  <strong>TBL_ID</strong>：指定了分区所在的表的 ID，对应 <code>TBLS</code>表的主键</li>
</ul>
<p> <img src="/2021/12/13/Hive/image-20211220102924703.png" alt="image-20211220102924703"></p>
</li>
</ol>
<h5 id="增加分区（方式二）"><a href="#增加分区（方式二）" class="headerlink" title="增加分区（方式二）"></a>增加分区（方式二）</h5><p>我们还可以直接使用 <code>load</code> 命令向分区加载数据，如果分区不存在，load 时会自动帮我们生成分区</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table dept_partition partition(month=202109);</span><br></pre></td></tr></table></figure>





<h4 id="5、导入数据（成功）"><a href="#5、导入数据（成功）" class="headerlink" title="5、导入数据（成功）"></a>5、导入数据（成功）</h4><p>加载数据到分区表的指定分区中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table dept_partition partition(month=&#x27;202110&#x27;);</span><br><span class="line">Loading data to table default.dept_partition partition (month=202110)</span><br><span class="line">Partition default.dept_partition&#123;month=202110&#125; stats: [numFiles=1, totalSize=65]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.975 seconds</span><br></pre></td></tr></table></figure>

<ul>
<li>  在 HDFS 上查看上传的数据文件</li>
</ul>
<p><img src="/2021/12/13/Hive/image-20211220103704244.png" alt="image-20211220103704244"></p>
<h4 id="6、查询分区表"><a href="#6、查询分区表" class="headerlink" title="6、查询分区表"></a>6、查询分区表</h4><ol>
<li><p>将数据导入到指定的分区之后，数据会额外附加上分区列的信息。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition;</span><br><span class="line">OK</span><br><span class="line">dept_partition.deptno	dept_partition.dname	dept_partition.loc	dept_partition.month</span><br><span class="line">NULL	1700	NULL	202110</span><br><span class="line">20	RESEARCH	1800	202110</span><br><span class="line">30	SALES	1900	202110</span><br><span class="line">40	OPERATIONS	1700	202110</span><br><span class="line">Time taken: 0.47 seconds, Fetched: 4 row(s)</span><br><span class="line"></span><br><span class="line"># 分区列 dept_partition.month 的信息在源数据文件中是没有的，是在将数据分区表之后由 Hive 自动生成的</span><br></pre></td></tr></table></figure>

</li>
<li><p>可以通过分区列来对数据进行过滤，指定查找的目录</p>
<ul>
<li><p>单分区查询</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month = &quot;202110&quot;;</span><br><span class="line">OK</span><br><span class="line">dept_partition.deptno	dept_partition.dname	dept_partition.loc	dept_partition.month</span><br><span class="line">NULL	1700	NULL	202110</span><br><span class="line">20	RESEARCH	1800	202110</span><br><span class="line">30	SALES	1900	202110</span><br><span class="line">40	OPERATIONS	1700	202110</span><br><span class="line">Time taken: 0.388 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure></li>
<li><p>多分区联合查询：通过关键字 union 联合查询多个分区</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	select * from dept_partition where month=&#x27;202110&#x27;</span><br><span class="line">	union</span><br><span class="line">	select * from dept_partition where month=&#x27;202111&#x27;</span><br><span class="line">	union</span><br><span class="line">	select * from dept_partition where month=&#x27;202112&#x27;;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ol>
<h4 id="7、删除分区"><a href="#7、删除分区" class="headerlink" title="7、删除分区"></a>7、删除分区</h4><ul>
<li><p>删除单个分区</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	alter table dept_partition drop partition (month=&#x27;202110&#x27;);</span><br></pre></td></tr></table></figure></li>
<li><p>同时删除多个分区</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	alter table dept_partition </span><br><span class="line">	drop partition (month=&#x27;202110&#x27;), partition (month=&#x27;202111&#x27;);</span><br></pre></td></tr></table></figure></li>
</ul>
<p>在执行删除表操作时，首先会删除 MySQL 数据库中对应的元数据信息；并且如果当前表是内部表，还会删除 HDFS 上对应的数据信息。</p>
<h4 id="8、查看分区表有哪些分区"><a href="#8、查看分区表有哪些分区" class="headerlink" title="8、查看分区表有哪些分区"></a>8、查看分区表有哪些分区</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure>



<h4 id="9、查看分区表结构"><a href="#9、查看分区表结构" class="headerlink" title="9、查看分区表结构"></a>9、查看分区表结构</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc formatted dept_partition;</span><br></pre></td></tr></table></figure>







<hr>
<h3 id="4-3-4-多级分区表"><a href="#4-3-4-多级分区表" class="headerlink" title="4.3.4 多级分区表"></a>4.3.4 多级分区表</h3><p><strong>一、创建多级分区表</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	create table dept_partition2</span><br><span class="line">		(           </span><br><span class="line">            deptno int, </span><br><span class="line">            dname string,</span><br><span class="line">            loc string</span><br><span class="line">        )           </span><br><span class="line">        partitioned by (month string, day string)	# 二级分区，先按 month 分区，month 相等时再按 day 分区           </span><br><span class="line">        row format delimited fields terminated by &#x27;\t&#x27;;  </span><br></pre></td></tr></table></figure>



<p><strong>二、正常的加载数据</strong></p>
<ol>
<li><p>加载数据到二级分区表中</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition2</span><br><span class="line">	partition(month=&#x27;202112&#x27;, day=&#x27;20&#x27;);</span><br></pre></td></tr></table></figure>

<ul>
<li>  对应的目录结构就是三级目录：<strong>表目录/一级分区目录/二级分区目录</strong>，在 HDFS 上的结构如下：</li>
</ul>
<p> <img src="/2021/12/13/Hive/image-20211220114355694.png" alt="image-20211220114355694"></p>
<ul>
<li>Tip：如果是多级分区表，数据必须位于最内层目录下。</li>
</ul>
</li>
<li><p>查询分区数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">              &gt; select * from dept_partition2</span><br><span class="line">              &gt; where month=&#x27;202112&#x27; and day=&#x27;20&#x27;;</span><br><span class="line">OK</span><br><span class="line">dept_partition2.deptno	dept_partition2.dname	dept_partition2.loc	dept_partition2.month	dept_partition2.day</span><br><span class="line">NULL	1700	NULL	202112	20</span><br><span class="line">20	RESEARCH	1800	202112	20</span><br><span class="line">30	SALES	1900	202112	20</span><br><span class="line">40	OPERATIONS	1700	202112	20</span><br><span class="line">Time taken: 0.106 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="4-3-5-自动修复分区"><a href="#4-3-5-自动修复分区" class="headerlink" title="4.3.5 自动修复分区"></a>4.3.5 自动修复分区</h3><p>通常情况下，HDFS 上保存的数据是预先采集好的，再由 Hive 对 HDFS 上保存的数据进行分析。所以一般是先有数据后有表，所以 Hive 在建表时首先要读取 HDFS 上的数据，再根据数据建立对应的表。</p>
<p><strong>把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式：</strong></p>
<p><strong>方式一：上传数据后自动修复</strong></p>
<ul>
<li>  上传数据</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br><span class="line">hive (default)&gt; </span><br><span class="line">	dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br></pre></td></tr></table></figure>

<ul>
<li>  查询数据（查询不到刚上传的数据）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure>

<ul>
<li>  <strong>执行修复命令</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line">	msck repair table dept_partition2;</span><br></pre></td></tr></table></figure>

<ul>
<li>  再次查询数据</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure>



<p><strong>方式二：上传数据后添加分区</strong></p>
<ul>
<li>  上传数据</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br><span class="line">hive (default)&gt; </span><br><span class="line">	dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br></pre></td></tr></table></figure>

<ul>
<li>  手动添加分区</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	alter table dept_partition2 add partition(month=&#x27;201709&#x27;,day=&#x27;11&#x27;);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>  查询数据</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;11&#x27;;</span><br></pre></td></tr></table></figure>



<p><strong>方式三：创建分区目录后load数据到指定分区</strong></p>
<ul>
<li>  创建目录</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10;</span><br></pre></td></tr></table></figure>

<ul>
<li>  上传数据</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table dept_partition2 partition(month=&#x27;201709&#x27;,day=&#x27;10&#x27;);</span><br></pre></td></tr></table></figure>

<ul>
<li>  查询数据</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;10&#x27;;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="4-4-分桶表"><a href="#4-4-分桶表" class="headerlink" title="4.4 分桶表"></a>4.4 分桶表</h2><h3 id="4-4-1-理论知识"><a href="#4-4-1-理论知识" class="headerlink" title="4.4.1 理论知识"></a>4.4.1 理论知识</h3><p><strong>语法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    CLUSTERED BY (col_name, col_name, ...) 	# 分桶表	</span><br><span class="line">    [SORTED BY (col_name [ASC|DESC], ...)]  # 分桶之后对桶中的数据排序</span><br><span class="line">    INTO num_buckets BUCKETS</span><br><span class="line">] 	</span><br></pre></td></tr></table></figure>

<ul>
<li>  在创建分区表的时候，需要手动指定<strong>分区列名称</strong>和<strong>分区列类型</strong>；而在创建分桶表的时候，只需要手动指定<strong>分桶列名称</strong>就可以了。</li>
<li>  在创建分区表时，<strong>分区列名称</strong>和<strong>分区列类型</strong>会作为新的属性组追加到表中，而创建分桶表时，则是复用表中已有的字段作为分桶的依据。</li>
</ul>
<p><strong>是什么？</strong></p>
<ul>
<li>  如果在建表时，显式指定了 <code>CLUSTERED BY</code> 属性，那么这张表就称为分桶表。</li>
<li>  分桶就是把表数据分散到多个文件中。比如一张表中有 100W 条数据，存储在一个文件中。假如分成 4 个桶，这 100W 条数据就会分散到 4 个桶中存储，降低了单个文件的负担。</li>
<li>  分桶的意义是为了分散数据。在分桶后，可以结合 Hive 提供的抽样查询，只查询指定桶的数据。</li>
<li>  <strong>分区</strong>是把一张表中的数据分散存储到各个目录下，<strong>分桶</strong>是把一张表中的数据分散存储到各个文件中。</li>
</ul>
<p><strong>单个桶中数据的排序：</strong></p>
<ul>
<li>  在分桶后，可以将每个桶中的数据根据一定的规则进行排序。</li>
<li>  如果需要排序，可以通过 <code>SORTED BY</code> 关键字来显式指定排序的字段和规则。</li>
</ul>
<hr>
<h3 id="4-4-2-案例实操"><a href="#4-4-2-案例实操" class="headerlink" title="4.4.2 案例实操"></a>4.4.2 案例实操</h3><h4 id="1、创建分桶表"><a href="#1、创建分桶表" class="headerlink" title="1、创建分桶表"></a>1、创建分桶表</h4><ol>
<li><p>数据准备：<code>/opt/module/datas/student.txt</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 学号	姓名</span><br><span class="line">1001	ss1</span><br><span class="line">1002	ss2</span><br><span class="line">1003	ss3</span><br><span class="line">1004	ss4</span><br><span class="line">1005	ss5</span><br><span class="line">1006	ss6</span><br><span class="line">1007	ss7</span><br><span class="line">1008	ss8</span><br><span class="line">1009	ss9</span><br><span class="line">1010	ss10</span><br><span class="line">1011	ss11</span><br><span class="line">1012	ss12</span><br><span class="line">1013	ss13</span><br><span class="line">1014	ss14</span><br><span class="line">1015	ss15</span><br><span class="line">1016	ss16</span><br></pre></td></tr></table></figure>

</li>
<li><p>创建分桶表</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">              &gt; create table stu_buck(id int, name string)</span><br><span class="line">              &gt; clustered by(id) into 4 buckets</span><br><span class="line">              &gt; row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看表结构</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted stu_buck;</span><br><span class="line">OK</span><br><span class="line">col_name	data_type	comment</span><br><span class="line"># col_name            	data_type           	comment             </span><br><span class="line">	 	 </span><br><span class="line">id                  	int                 	                    </span><br><span class="line">name                	string              	                    </span><br><span class="line">	 	 </span><br><span class="line"># Detailed Table Information	 	 </span><br><span class="line">Database:           	default             	 </span><br><span class="line">Owner:              	lvnengdong          	 </span><br><span class="line">CreateTime:         	Mon Dec 20 12:17:47 CST 2021	 </span><br><span class="line">LastAccessTime:     	UNKNOWN             	 </span><br><span class="line">Protect Mode:       	None                	 </span><br><span class="line">Retention:          	0                   	 </span><br><span class="line">Location:           	hdfs://hadoop102:9000/hive/warehouse/stu_buck	 </span><br><span class="line">Table Type:         	MANAGED_TABLE       	 </span><br><span class="line">Table Parameters:	 	 </span><br><span class="line">	transient_lastDdlTime	1639973867          </span><br><span class="line">	 	 </span><br><span class="line"># Storage Information	 	 </span><br><span class="line">SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 </span><br><span class="line">InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 </span><br><span class="line">OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 </span><br><span class="line">Compressed:         	No                  	 </span><br><span class="line">Num Buckets:        	4    # 桶数量              	 </span><br><span class="line">Bucket Columns:     	[id]     # 分桶的列名           	 </span><br><span class="line">Sort Columns:       	[]       # 排序字段      	 </span><br><span class="line">Storage Desc Params:	 	 </span><br><span class="line">	field.delim         	\t                  </span><br><span class="line">	serialization.format	\t                  </span><br><span class="line">Time taken: 0.089 seconds, Fetched: 28 row(s)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>导入数据到分桶表中</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table  stu_buck;</span><br></pre></td></tr></table></figure></li>
<li><p>查看创建的分桶表中是否分成4个桶。</p>
<p> <img src="/2021/12/13/Hive/image-20211220122413669.png" alt="image-20211220122413669"></p>
<blockquote>
<p>  发现并没有分成 4 个桶。是什么原因呢？</p>
<ul>
<li>  向分桶表中导入数据时，必须运行 MapReduce 程序，才能实现分桶操作。</li>
<li>  load 方式只是简单的上传数据，类似于 put 操作，无法满足分桶表的导入数据。必须采用 insert into 方式导入数据才能实现分桶</li>
</ul>
</blockquote>
</li>
</ol>
<h4 id="2、创建分桶表时，数据通过子查询的方式导入"><a href="#2、创建分桶表时，数据通过子查询的方式导入" class="headerlink" title="2、创建分桶表时，数据通过子查询的方式导入"></a>2、创建分桶表时，数据通过子查询的方式导入</h4><ol>
<li><p>先建一个普通的 <code>stu</code> 表</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table stu(id int, name string)  row  format delimited fields terminated by &#x27;\t&#x27;;  </span><br></pre></td></tr></table></figure></li>
<li><p>向普通的 <code>stu</code> 表中导入数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table stu;  </span><br></pre></td></tr></table></figure></li>
<li><p>清空 <code>stu_buck</code> 表中数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truncate  table stu_buck;  select  * from stu_buck;  </span><br></pre></td></tr></table></figure></li>
<li><p>通过子查询的方式导入数据到分桶表。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into table stu_buck select id, name from stu;  </span><br></pre></td></tr></table></figure></li>
<li><p>发现还是只有一个分桶</p>
<p> <img src="/2021/12/13/Hive/image-20211220125128806.png" alt="image-20211220125128806"></p>
</li>
<li><p>需要设置一个属性</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 1、打开强制分桶开关：</span><br><span class="line">hive  (default)&gt; set hive.enforce.bucketing=true;</span><br><span class="line"># 2、mapreduce.job.reduces=-1 表示默认值，表示 Hive 会读取分桶表的设置的 numBuckets 来设置分桶值，</span><br><span class="line"># 但是该参数生效的前提是必须打开分桶开关</span><br><span class="line">hive  (default)&gt; set mapreduce.job.reduces=-1;  </span><br><span class="line">hive  (default)&gt; insert into table stu_buck  select  id, name from stu;  </span><br></pre></td></tr></table></figure></li>
<li><p>查询分桶的数据</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from stu_buck;</span><br><span class="line">OK</span><br><span class="line">stu_buck.id	stu_buck.name</span><br><span class="line">1016	ss16</span><br><span class="line">1012	ss12</span><br><span class="line">1008	ss8</span><br><span class="line">1004	ss4</span><br><span class="line">1009	ss9</span><br><span class="line">1005	ss5</span><br><span class="line">1001	ss1</span><br><span class="line">1013	ss13</span><br><span class="line">1010	ss10</span><br><span class="line">1002	ss2</span><br><span class="line">1006	ss6</span><br><span class="line">1014	ss14</span><br><span class="line">1003	ss3</span><br><span class="line">1011	ss11</span><br><span class="line">1007	ss7</span><br><span class="line">1015	ss15</span><br><span class="line">Time taken: 0.043 seconds, Fetched: 16 row(s)</span><br></pre></td></tr></table></figure></li>
<li><p>查看 HDFS 上的分桶结果</p>
<p> 表中的所有数据现在都根据 <code>id</code> 的 Hash 值分散在四个桶中了。Hash 是采用 MapReduce 默认的 Hash 分区器。</p>
<p> <img src="/2021/12/13/Hive/image-20211220125548362.png" alt="image-20211220125548362"></p>
</li>
</ol>
<hr>
<h3 id="4-4-2-分桶抽样查询"><a href="#4-4-2-分桶抽样查询" class="headerlink" title="4.4.2 分桶抽样查询"></a>4.4.2 分桶抽样查询</h3><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。<code>Hive</code> 可以通过对表进行抽样来满足这个需求。</p>
<p><strong>语法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from 分桶表名 tablesample(bucket x out of y on 分桶字段);  </span><br></pre></td></tr></table></figure>

<p><strong>要求：</strong></p>
<ul>
<li><p>  抽样查询的表必须是分桶表</p>
</li>
<li><p>  假设当前表一共分了 <code>z</code> 个桶，<code>x</code> 表示从第 <code>x</code> 桶开始抽样，每间隔 <code>y</code> 桶抽一次，直到抽满 <code>z/y</code> 桶。</p>
</li>
<li><p>  <code>y</code> 必须是 <code>z</code> 的倍数或者因子。<code>Hive</code> 根据 <code>y</code> 的大小，决定抽样的比例。例如，<code>table</code> 总共分了 4 份，当 <code>y=2</code> 时，抽取 <code>(4/2)=2</code> 个 <code>bucket</code> 的数据，当 <code>y=8</code> 时，抽取 <code>(4/8)=1/2</code> 个 <code>bucket</code> 的数据。</p>
</li>
<li><p>  <code>x</code> 表示从第几个 <code>bucket</code> 开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上 <code>y</code>。例如，<code>table</code> 总 <code>bucket</code> 数为 4， <code>tablesample(bucket 1 out of 2)</code>，表示总共抽取 <code>（4/2）= 2</code> 个 <code>bucket</code> 的数据，抽取第 1 个和第 3 个 <code>bucket</code> 的数据。</p>
</li>
<li><p>注意： <code>x</code> 的值必须小于等于 <code>y</code> 的值，否则会抛出异常。</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table stu_buck</span><br></pre></td></tr></table></figure>

</li>
</ul>
<ol>
<li><p>查询表 <code>stu_buck</code> 中的数据。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 当前表分了四桶（0~3号桶），当前次查询只查询第0号桶和2号桶中的数据</span><br><span class="line">hive (default)&gt; select * from stu_buck tablesample(bucket 1 out of 2 on id);</span><br><span class="line">OK</span><br><span class="line">stu_buck.id	stu_buck.name</span><br><span class="line">1016	ss16</span><br><span class="line">1012	ss12</span><br><span class="line">1008	ss8</span><br><span class="line">1004	ss4</span><br><span class="line">1010	ss10</span><br><span class="line">1002	ss2</span><br><span class="line">1006	ss6</span><br><span class="line">1014	ss14</span><br><span class="line">Time taken: 0.164 seconds, Fetched: 8 row(s)</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h1 id="第5章-DML数据操作"><a href="#第5章-DML数据操作" class="headerlink" title="第5章 DML数据操作"></a>第5章 DML数据操作</h1><h2 id="5-1-数据导入"><a href="#5-1-数据导入" class="headerlink" title="5.1 数据导入"></a>5.1 数据导入</h2><h3 id="5-1-1-向表中装载数据（Load）"><a href="#5-1-1-向表中装载数据（Load）" class="headerlink" title="5.1.1 向表中装载数据（Load）"></a>5.1.1 向表中装载数据（Load）</h3><p><strong>作用：</strong>将数据加载到 Hive 表中</p>
<p><strong>语法：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> load data [<span class="keyword">local</span>] inpath <span class="string">&#x27;/opt/module/datas/student.txt&#x27;</span> [overwrite] <span class="keyword">into</span> <span class="keyword">table</span> student [<span class="keyword">partition</span> (partcol1<span class="operator">=</span>val1,…)];</span><br><span class="line"></span><br><span class="line"><span class="comment">---------------------------</span></span><br><span class="line">（<span class="number">1</span>）load data：表示加载数据</span><br><span class="line">（<span class="number">2</span>）<span class="keyword">local</span>：如果导入的文件位于本地文件系统，则使用 <span class="keyword">local</span> 关键字，表示使用 put 的方式将本地文件上传到 Hive 上的表目录中；如果不加 <span class="keyword">local</span> 默认导入的文件位于 HDFS 上，通过 mv 的方式将源文件移动到 Hive 的表目录中。</span><br><span class="line">（<span class="number">3</span>）inpath：表示加载数据的路径</span><br><span class="line">（<span class="number">4</span>）overwrite：表示覆盖表中已有数据，否则表示追加</span><br><span class="line">（<span class="number">5</span>）<span class="keyword">into</span> <span class="keyword">table</span>：表示加载到哪张表</span><br><span class="line">（<span class="number">6</span>）student：表示具体的表名</span><br><span class="line">（<span class="number">7</span>）<span class="keyword">partition</span>：表示上传到指定分区</span><br><span class="line"><span class="comment">---------------------------</span></span><br></pre></td></tr></table></figure>





<p><strong>实操案例</strong></p>
<ol>
<li><p>创建一张表</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> student(id string, name string) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>

</li>
<li><p>加载本地文件到 Hive</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> default.student;</span><br></pre></td></tr></table></figure>

</li>
<li><p>加载 HDFS 中的文件到 Hive 中</p>
<ul>
<li><p>上传文件到 HDFS</p>
  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>datas<span class="operator">/</span>student.txt <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>atguigu<span class="operator">/</span>hive;</span><br></pre></td></tr></table></figure></li>
<li><p>加载 HDFS 中的文件到 Hive 中</p>
  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data inpath <span class="string">&#x27;/user/atguigu/hive/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> default.student;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>加载数据覆盖表中已有的数据</p>
<ul>
<li><p>上传文件到 HDFS</p>
  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>datas<span class="operator">/</span>student.txt <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>atguigu<span class="operator">/</span>hive;</span><br></pre></td></tr></table></figure></li>
<li><p>加载 HDFS 上的数据覆盖表中已有的数据</p>
  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data inpath <span class="string">&#x27;/user/atguigu/hive/student.txt&#x27;</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> default.student;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<hr>
<h3 id="5-1-2-通过查询语句向表中插入数据（Insert）"><a href="#5-1-2-通过查询语句向表中插入数据（Insert）" class="headerlink" title="5.1.2    通过查询语句向表中插入数据（Insert）"></a>5.1.2    通过查询语句向表中插入数据（Insert）</h3><p><code>insert</code> 方式运行 MapReduce 程序，通过程序将数据输出到表目录。</p>
<p>在某些场景，必须使用 <code>insert</code> 方式来导入数据。</p>
<ol>
<li> 向桶中插入数据</li>
<li> 如果想要使表中的数据以非文本的格式存储，需要使用 <code>insert</code> 的方式导入数据</li>
</ol>
<p><strong>语法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert &#123;into | overwrite&#125; table 表名 &#123;select xxx | values(...)&#125;</span><br><span class="line"></span><br><span class="line"># insert into：追加写</span><br><span class="line"># insert overwrite：覆盖写</span><br><span class="line"></span><br><span class="line"># select xxx：将从MySQL中查到的数据写入到Hive中</span><br><span class="line"># values(...)：直接显式指定要插入的值，如 values(1,&#x27;张三&#x27;)</span><br></pre></td></tr></table></figure>





<p><strong>Demo</strong></p>
<ol>
<li><p>创建一张分区表</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> student(id <span class="type">int</span>, name string) partitioned <span class="keyword">by</span> (<span class="keyword">month</span> string) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>基本插入数据</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;201709&#x27;</span>) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">&#x27;wangwu&#x27;</span>);</span><br></pre></td></tr></table></figure></li>
<li><p>基本模式插入（根据单张表查询结果）</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;201708&#x27;</span>)</span><br><span class="line">             <span class="keyword">select</span> id, name <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;201709&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>多插入模式（根据多张表查询结果）</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line">              insert overwrite table student partition(month=&#x27;201707&#x27;)</span><br><span class="line">              select id, name where month=&#x27;201709&#x27;</span><br><span class="line">              insert overwrite table student partition(month=&#x27;201706&#x27;)</span><br><span class="line">              select id, name where month=&#x27;201710&#x27;;</span><br><span class="line">              </span><br></pre></td></tr></table></figure>

<blockquote>
<p>  <strong>多插入模式（从一张源表查询，向多张目标表插入）</strong>。在上面的语句中就是：</p>
<ol>
<li> 查询 month=’201709’ 的数据插入到 student 表的 month=’201707’ 分区中；</li>
<li> 查询 month=’201710’ 的数据插入到 student 表的 month=’201706’ 分区中；</li>
</ol>
<p>  <strong>语法：</strong></p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from 源表</span><br><span class="line">insert into 目标表 select xxx</span><br><span class="line">insert overwrite 目标表 select xxx</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

</blockquote>
</li>
</ol>
<p>​    </p>
<hr>
<h3 id="5-1-3-查询语句中创建表并加载数据（As-Select）"><a href="#5-1-3-查询语句中创建表并加载数据（As-Select）" class="headerlink" title="5.1.3 查询语句中创建表并加载数据（As Select）"></a>5.1.3 查询语句中创建表并加载数据（As Select）</h3><p>详见 4.5.1 章创建表。</p>
<p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student3</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> id, name <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="5-1-4-创建表时通过-Location-指定加载数据路径"><a href="#5-1-4-创建表时通过-Location-指定加载数据路径" class="headerlink" title="5.1.4 创建表时通过 Location 指定加载数据路径"></a>5.1.4 创建表时通过 Location 指定加载数据路径</h3><ol>
<li><p>创建表，并指定表数据在 Hive 上的位置</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student5(</span><br><span class="line">    id <span class="type">int</span>, name string</span><br><span class="line">)</span><br><span class="line">	<span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">	location <span class="string">&#x27;/user/hive/warehouse/student5&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
<li><p>上传数据到 HDFS 上</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>datas<span class="operator">/</span>student.txt</span><br><span class="line"><span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>student5;</span><br></pre></td></tr></table></figure>

</li>
<li><p>查询数据</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student5;</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="5-1-5-Import数据到指定Hive表中"><a href="#5-1-5-Import数据到指定Hive表中" class="headerlink" title="5.1.5 Import数据到指定Hive表中"></a>5.1.5 Import数据到指定Hive表中</h3><ul>
<li>  <strong>import</strong> 不仅可以导入业务数据，同时还可以导入元数据。 </li>
<li>  注意：<strong>import</strong> 只能导入 <strong>export</strong> 导出的数据。</li>
</ul>
<p><strong>语法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">IMPORT [[EXTERNAL] TABLE new_or_original_tablename [PARTITION (part_column=&quot;value&quot;[, ...])]]</span><br><span class="line">  FROM &#x27;source_path&#x27;</span><br><span class="line">  [LOCATION &#x27;import_target_path&#x27;]</span><br></pre></td></tr></table></figure>

<ul>
<li><p>  向一张表中导入数据时，如果表不存在，Hive 会根据导入数据中的元数据信息自动创建表。</p>
</li>
<li><p>  如果表已经存在，在导入之前会先检查目标表的元数据与导入数据中的元数据信息是否一致。检查通过后才会执行导入。</p>
</li>
<li><p>不管表是否为空，目标表中的分区必须是不存在的。</p>
</li>
</ul>
<p><strong>Demo</strong></p>
<ol>
<li><p>通过 import 将数据导入指定的 Hive 表中</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; import table import_table1 from &#x27;/export&#x27;;</span><br></pre></td></tr></table></figure>

</li>
<li><p>查看导入后的表信息</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">import_table1</span><br><span class="line">student</span><br><span class="line">test</span><br><span class="line">Time taken: 0.018 seconds, Fetched: 8 row(s)</span><br></pre></td></tr></table></figure>

 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc import_table1;</span><br><span class="line">OK</span><br><span class="line">col_name	data_type	comment</span><br><span class="line">deptno              	int                 	                    </span><br><span class="line">dname               	string              	                    </span><br><span class="line">loc                 	string              	                    </span><br><span class="line">month               	string              	                    </span><br><span class="line">	 	 </span><br><span class="line"># Partition Information	 	 分区信息</span><br><span class="line"># col_name            	data_type           	comment             </span><br><span class="line">	 	 </span><br><span class="line">month               	string              	                    </span><br><span class="line">Time taken: 0.076 seconds, Fetched: 9 row(s)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h2 id="5-2-数据导出"><a href="#5-2-数据导出" class="headerlink" title="5.2 数据导出"></a>5.2 数据导出</h2><h3 id="5-2-1-Insert-导出"><a href="#5-2-1-Insert-导出" class="headerlink" title="5.2.1 Insert 导出"></a>5.2.1 Insert 导出</h3><ol>
<li><p>将查询的结果导出到本地</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/export/student&#x27;</span><br><span class="line">            select * from student;</span><br></pre></td></tr></table></figure></li>
<li><p>将查询的结果<strong>格式化</strong>后导出到本地</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;insert overwrite local directory &#x27;/opt/module/datas/export/student1&#x27;</span><br><span class="line">           ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;             </span><br><span class="line">           select * from student;</span><br></pre></td></tr></table></figure>

</li>
<li><p>将查询的结果导出到 HDFS 上（省略 local 关键字）</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite directory &#x27;/user/atguigu/student2&#x27;</span><br><span class="line">             ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; </span><br><span class="line">             select * from student;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h3 id="5-2-2-Hadoop命令导出到本地"><a href="#5-2-2-Hadoop命令导出到本地" class="headerlink" title="5.2.2 Hadoop命令导出到本地"></a>5.2.2 Hadoop命令导出到本地</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure>





<hr>
<h3 id="5-2-3-Hive-Shell-命令导出"><a href="#5-2-3-Hive-Shell-命令导出" class="headerlink" title="5.2.3    Hive Shell 命令导出"></a>5.2.3    Hive Shell 命令导出</h3><p>基本语法：<code>hive -f/-e 执行语句或者脚本 &gt; file</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -e <span class="string">&#x27;select * from default.student;&#x27;</span> &gt; </span><br><span class="line">/opt/module/datas/export/student4.txt;</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="5-2-4-Export-导出到-HDFS-上"><a href="#5-2-4-Export-导出到-HDFS-上" class="headerlink" title="5.2.4    Export 导出到 HDFS 上"></a>5.2.4    Export 导出到 HDFS 上</h3><p><strong>Export：</strong></p>
<ul>
<li>  既能导出数据，还可以导入元数据</li>
<li>  <code>export</code> 只能导出到 HDFS 中，会在 HDFS 的导出目录中，生成数据和元数据文件。</li>
<li>  如果当前表是一个分区表，可以选择只导出表的部分分区数据及其元数据信息。</li>
</ul>
<p><strong>语法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">EXPORT TABLE tablename [PARTITION (part_column=&quot;value&quot;[, ...])]</span><br><span class="line">  TO &#x27;export_target_path&#x27; [ FOR replication(&#x27;eventid&#x27;) ]</span><br></pre></td></tr></table></figure>





<p><strong>Demo：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> EXPORT <span class="keyword">TABLE</span> dept_partition <span class="keyword">TO</span> <span class="string">&#x27;/export&#x27;</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>查看 HDFS 上的文件结构</p>
<p>  <img src="/2021/12/13/Hive/image-20211220175855162.png" alt="image-20211220175855162"></p>
</li>
</ul>
<hr>
<h3 id="5-2-5-Sqoop-导出"><a href="#5-2-5-Sqoop-导出" class="headerlink" title="5.2.5    Sqoop 导出"></a>5.2.5    Sqoop 导出</h3><p>后续课程专门讲。</p>
<hr>
<h2 id="5-3-清除表中数据（Truncate）"><a href="#5-3-清除表中数据（Truncate）" class="headerlink" title="5.3 清除表中数据（Truncate）"></a>5.3 清除表中数据（Truncate）</h2><p>注意：<code>Truncate</code> 只能删除管理表，不能删除外部表中数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">truncate</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure>





<hr>
<h1 id="第-6-章-查询"><a href="#第-6-章-查询" class="headerlink" title="第 6 章    查询"></a>第 6 章    查询</h1><p>官网：<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a></p>
<p><strong>查询语句语法：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">WITH</span> CommonTableExpression (, CommonTableExpression)<span class="operator">*</span>]    (Note: <span class="keyword">Only</span> available starting <span class="keyword">with</span> Hive <span class="number">0.13</span><span class="number">.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> <span class="operator">|</span> <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line"><span class="keyword">FROM</span> table_reference</span><br><span class="line">[<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">[<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[CLUSTER <span class="keyword">BY</span> col_list <span class="operator">|</span> [DISTRIBUTE <span class="keyword">BY</span> col_list] [SORT <span class="keyword">BY</span> col_list]]</span><br><span class="line">[LIMIT number]</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="6-1-基础查询（Select…From）"><a href="#6-1-基础查询（Select…From）" class="headerlink" title="6.1 基础查询（Select…From）"></a>6.1 基础查询（Select…From）</h2><h3 id="6-1-1-全表和特定列查询"><a href="#6-1-1-全表和特定列查询" class="headerlink" title="6.1.1 全表和特定列查询"></a>6.1.1 全表和特定列查询</h3><ol>
<li><p>全表查询</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure></li>
<li><p>特定列查询</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> empno, ename <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>注意：</strong></p>
<ol>
<li> SQL 语言大小写不敏感。</li>
<li>  SQL 可以写在一行或者多行。</li>
</ol>
<h3 id="6-1-2-列别名"><a href="#6-1-2-列别名" class="headerlink" title="6.1.2 列别名"></a>6.1.2 列别名</h3><p>列别名的规则与 MySQL 相同，可以紧跟列名，也可以在列名和别名之间加入关键字 AS。 </p>
<p>案例实操：查询名称和部门</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename <span class="keyword">AS</span> name, deptno dn <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="6-1-3-算术运算符"><a href="#6-1-3-算术运算符" class="headerlink" title="6.1.3 算术运算符"></a>6.1.3 算术运算符</h3><table>
<thead>
<tr>
<th align="center">运算符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>A + B</code></td>
<td>A 加 B</td>
</tr>
<tr>
<td align="center"><code>A - B</code></td>
<td>A 减 B</td>
</tr>
<tr>
<td align="center"><code>A * B</code></td>
<td>A 乘 B</td>
</tr>
<tr>
<td align="center"><code>A / B</code></td>
<td>A 除 B</td>
</tr>
<tr>
<td align="center"><code>A % B</code></td>
<td>A 对 B 取余</td>
</tr>
<tr>
<td align="center"><code>A &amp; B</code></td>
<td>A 和 B 按位与</td>
</tr>
<tr>
<td align="center">`A</td>
<td>B`</td>
</tr>
<tr>
<td align="center"><code>A ^ B</code></td>
<td>A 和 B 按位异或</td>
</tr>
<tr>
<td align="center"><code>~A</code></td>
<td>A 按位取反</td>
</tr>
</tbody></table>
<p>案例实操：查询出所有员工的薪水后 <code>+1</code> 显示。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> sal <span class="operator">+</span><span class="number">1</span> <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>





<hr>
<h3 id="6-1-4-常用函数"><a href="#6-1-4-常用函数" class="headerlink" title="6.1.4 常用函数"></a>6.1.4 常用函数</h3><ol>
<li><p>求总行数（count）</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) cnt <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure></li>
<li><p>求工资的最大值（max）</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">max</span>(sal) max_sal <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure></li>
<li><p>求工资的最小值（min）</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">min</span>(sal) min_sal <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure></li>
<li><p>求工资的总和（sum）</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">sum</span>(sal) sum_sal <span class="keyword">from</span> emp; </span><br></pre></td></tr></table></figure></li>
<li><p>求工资的平均值（avg）</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h3 id="6-1-5-Limit-语句"><a href="#6-1-5-Limit-语句" class="headerlink" title="6.1.5 Limit 语句"></a>6.1.5 Limit 语句</h3><p>典型的查询会返回多行数据。LIMIT 子句用于限制返回的行数。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp limit <span class="number">5</span>;</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="6-2-Where语句"><a href="#6-2-Where语句" class="headerlink" title="6.2 Where语句"></a>6.2 Where语句</h2><p>案例实操：查询出薪水大于 1000 的所有员工</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="operator">&gt;</span><span class="number">1000</span>;</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="6-2-1-比较运算符（Between-In-Is-Null）"><a href="#6-2-1-比较运算符（Between-In-Is-Null）" class="headerlink" title="6.2.1 比较运算符（Between / In / Is Null）"></a>6.2.1 比较运算符（Between / In / Is Null）</h3><p>下面表中描述了谓词操作符，这些操作符同样可以用于 <code>JOIN…ON</code> 和 <code>HAVING</code> 语句中。</p>
<table>
<thead>
<tr>
<th>操作符</th>
<th>支持的数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>A=B</code></td>
<td>基本数据类型</td>
<td>如果 A 等于 B 则返回 TRUE，反之返回 FALSE</td>
</tr>
<tr>
<td><code>A&lt;=&gt;B</code></td>
<td>基本数据类型</td>
<td>如果 A 和 B 都为 NULL，则返回TRUE；<br>如果任一为 NULL 则结果为 NULL；<br>其它的和等号（=）操作符的结果一致。</td>
</tr>
<tr>
<td><code>A&lt;&gt;B</code>，<code>A!=B</code></td>
<td>基本数据类型</td>
<td>A 或 B 为 NULL 则返回NULL；<br>如果 A 不等于 B，则返回 TRUE，反之返回 FALSE</td>
</tr>
<tr>
<td><code>A&lt;B</code></td>
<td>基本数据类型</td>
<td>A 或 B 为 NULL，则返回 NULL；<br>如果 A 小于 B，则返回 TRUE，反之返回 FALSE</td>
</tr>
<tr>
<td><code>A&lt;=B</code></td>
<td>基本数据类型</td>
<td>A 或 B 为 NULL，则返回 NULL；<br>如果 A 小于等于 B，则返回 TRUE，反之返回 FALSE</td>
</tr>
<tr>
<td><code>A&gt;B</code></td>
<td>基本数据类型</td>
<td>A 或 B 为 NULL，则返回 NULL；<br>如果 A 大于 B，则返回 TRUE，反之返回 FALSE</td>
</tr>
<tr>
<td><code>A&gt;=B</code></td>
<td>基本数据类型</td>
<td>A 或 B 为 NULL，则返回 NULL；<br>如果 A 大于等于 B，则返回 TRUE，反之返回 FALSE</td>
</tr>
<tr>
<td><code>A [NOT] BETWEEN B AND C</code></td>
<td>基本数据类型</td>
<td>如果 A、B 或 C 任一为 NULL，则结果为NULL。<br>如果 A 的值大于等于 B 而且小于或等于 C，则结果为 TRUE，<br>反之为 FALSE。如果使用 NOT 关键字则可达到相反的效果</td>
</tr>
<tr>
<td><code>A IS NULL</code></td>
<td>所有数据类型</td>
<td>如果 A 等于 NULL，则返回 TRUE，反之返回 FALSE</td>
</tr>
<tr>
<td><code>A IS NOT NULL</code></td>
<td>所有数据类型</td>
<td>如果 A 不等于 NULL，则返回 TRUE，反之返回 FALSE</td>
</tr>
<tr>
<td><code>IN(数值1, 数值2) </code></td>
<td>所有数据类型</td>
<td>使用 IN 运算显示列表中的值</td>
</tr>
<tr>
<td><code>A [NOT] LIKE B</code></td>
<td>STRING 类型</td>
<td>B 是一个 SQL 下的简单正则表达式，如果 A 与其匹配的话，则返回TRUE；<br>反之返回 FALSE。B 的表达式说明如下：<code>x%</code> 表示 A 必须以字母 <code>x</code> 开头，<br><code>%x</code> 表示 A 必须以字母 <code>x</code> 结尾，而 <code>%x%</code> 表示 A 包含有字母 <code>x</code>，可以位于开头，<br>结尾或者字符串中间。如果使用 NOT 关键字则可达到相反的效果。</td>
</tr>
<tr>
<td><code>A RLIKE B, A REGEXP B</code></td>
<td>STRING 类型</td>
<td>B 是一个正则表达式，如果 A 与其匹配，则返回 TRUE；反之返回 FALSE。<br>匹配使用的是 JDK 中的正则表达式接口实现的，因为正则也依据其中的规则。<br>例如，正则表达式必须和整个字符串 A 相匹配，而不是只需与其字符串匹配</td>
</tr>
</tbody></table>
<p><strong>案例实操</strong></p>
<ol>
<li><p>查询出薪水等于 5000 的所有员工</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="operator">=</span><span class="number">5000</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>查询工资在 500 到 1000 的员工信息</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">between</span> <span class="number">500</span> <span class="keyword">and</span> <span class="number">1000</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>查询 <code>comm</code> 为空的所有员工信息</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> comm <span class="keyword">is</span> <span class="keyword">null</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>查询工资是 1500 或 5000 的员工信息</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">IN</span> (<span class="number">1500</span>, <span class="number">5000</span>);</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h3 id="6-2-2-Like-和-RLike"><a href="#6-2-2-Like-和-RLike" class="headerlink" title="6.2.2 Like 和 RLike"></a>6.2.2 Like 和 RLike</h3><ol>
<li> 使用 LIKE 运算选择类似的值</li>
<li>选择条件可以包含字符或数字：<ul>
<li>  <code>%</code> 代表零个或多个字符(任意个字符)。</li>
<li>  <code>_</code> 代表一个字符。</li>
</ul>
</li>
<li> <code>RLIKE</code> 子句是 Hive 中这个功能的一个扩展，其可以通过 Java 的正则表达式这个更强大的语言来指定匹配条件。</li>
</ol>
<p><strong>案例实操</strong></p>
<ol>
<li><p>查找以 2 开头薪水的员工信息</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">LIKE</span> <span class="string">&#x27;2%&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>查找第二个数值为 2 的薪水的员工信息</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">LIKE</span> <span class="string">&#x27;_2%&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>查找薪水中含有 2 的员工信息</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal RLIKE <span class="string">&#x27;[2]&#x27;</span>;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h3 id="6-2-3-逻辑运算符（And-Or-Not）"><a href="#6-2-3-逻辑运算符（And-Or-Not）" class="headerlink" title="6.2.3    逻辑运算符（And/Or/Not）"></a>6.2.3    逻辑运算符（And/Or/Not）</h3><table>
<thead>
<tr>
<th align="center">操作符</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">AND</td>
<td align="center">与</td>
</tr>
<tr>
<td align="center">OR</td>
<td align="center">或</td>
</tr>
<tr>
<td align="center">NOT</td>
<td align="center">非</td>
</tr>
</tbody></table>
<p><strong>案例实操</strong></p>
<ol>
<li><p>查询薪水大于 1000，部门是 30</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal<span class="operator">&gt;</span><span class="number">1000</span> <span class="keyword">and</span> deptno<span class="operator">=</span><span class="number">30</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>查询薪水大于 1000，或者部门是30</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal<span class="operator">&gt;</span><span class="number">1000</span> <span class="keyword">or</span> deptno<span class="operator">=</span><span class="number">30</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>查询除了 20 部门和 30 部门以外的员工信息</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> deptno <span class="keyword">not</span> <span class="keyword">IN</span>(<span class="number">30</span>, <span class="number">20</span>);</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h2 id="6-3-分组"><a href="#6-3-分组" class="headerlink" title="6.3    分组"></a>6.3    分组</h2><h3 id="6-3-1-Group-By语句"><a href="#6-3-1-Group-By语句" class="headerlink" title="6.3.1    Group By语句"></a>6.3.1    Group By语句</h3><p><code>GROUP BY</code> 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p>
<p><strong>案例实操：</strong></p>
<ol>
<li><p>计算 <code>emp</code> 表每个部门的平均工资</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> t.deptno, <span class="built_in">avg</span>(t.sal) avg_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno;</span><br></pre></td></tr></table></figure></li>
<li><p>计算 <code>emp</code> 每个部门中每个岗位的最高薪水</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> t.deptno, t.job, <span class="built_in">max</span>(t.sal) max_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno, t.job;</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="6-3-2-Having-语句"><a href="#6-3-2-Having-语句" class="headerlink" title="6.3.2    Having 语句"></a>6.3.2    Having 语句</h3><p><strong><code>having</code> 与 <code>where</code> 不同点：</strong></p>
<ol>
<li> <code>where</code> 针对表中的列发挥作用，查询数据；<code>having</code> 针对查询结果中的列发挥作用，筛选数据。</li>
<li> <code>where</code> 后面不能写分组函数，而 <code>having</code> 后面可以使用分组函数。</li>
<li> <code>having</code> 只用于 <code>group by</code> 分组统计语句。</li>
</ol>
<p><strong>案例实操</strong>：求所有部门中平均薪水大于 2000 的部门</p>
<ol>
<li><p>求每个部门的平均工资</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno, <span class="built_in">avg</span>(sal) <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure></li>
<li><p>求所有部门中平均薪水大于 2000 的部门</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno, <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno <span class="keyword">having</span> avg_sal <span class="operator">&gt;</span> <span class="number">2000</span>;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h2 id="6-4-Join语句"><a href="#6-4-Join语句" class="headerlink" title="6.4 Join语句"></a>6.4 Join语句</h2><h3 id="6-4-1-等值Join"><a href="#6-4-1-等值Join" class="headerlink" title="6.4.1 等值Join"></a>6.4.1 等值Join</h3><p>Hive 支持常用的 <code>SQL JOIN</code> 语句，但是只支持等值连接，不支持非等值连接。</p>
<p><strong>案例实操</strong></p>
<ol>
<li><p>根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称；</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> </span><br><span class="line">	<span class="keyword">select</span> e.empno, e.ename, d.deptno, d.dname </span><br><span class="line">	<span class="keyword">from</span> emp e <span class="keyword">join</span> dept d</span><br><span class="line">	<span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h3 id="6-4-2-表的别名"><a href="#6-4-2-表的别名" class="headerlink" title="6.4.2    表的别名"></a>6.4.2    表的别名</h3><p><strong>好处：</strong></p>
<ol>
<li> 使用别名可以简化查询。</li>
<li> 使用表名前缀可以提高执行效率。</li>
</ol>
<p><strong>案例实操</strong></p>
<ol>
<li><p>合并员工表和部门表</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> </span><br><span class="line">	<span class="keyword">select</span> e.empno, e.ename, d.deptno </span><br><span class="line">	<span class="keyword">from</span> emp e <span class="keyword">join</span> dept d </span><br><span class="line">	<span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h3 id="6-4-3-内连接"><a href="#6-4-3-内连接" class="headerlink" title="6.4.3    内连接"></a>6.4.3    内连接</h3><p><strong>内连接：</strong>只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> </span><br><span class="line">	<span class="keyword">select</span> e.empno, e.ename, d.deptno </span><br><span class="line">	<span class="keyword">from</span> emp e <span class="keyword">join</span> dept d </span><br><span class="line">	<span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="6-4-4-左外连接"><a href="#6-4-4-左外连接" class="headerlink" title="6.4.4    左外连接"></a>6.4.4    左外连接</h3><p><strong>左外连接：</strong>JOIN 操作符左边表中符合 WHERE 子句的所有记录将会被返回。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> </span><br><span class="line">	<span class="keyword">select</span> e.empno, e.ename, d.deptno </span><br><span class="line">	<span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d </span><br><span class="line">	<span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>





<hr>
<h3 id="6-4-5-右外连接"><a href="#6-4-5-右外连接" class="headerlink" title="6.4.5    右外连接"></a>6.4.5    右外连接</h3><p><strong>右外连接：</strong>JOIN 操作符右边表中符合 WHERE 子句的所有记录将会被返回。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> </span><br><span class="line">	<span class="keyword">select</span> e.empno, e.ename, d.deptno </span><br><span class="line">	<span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d </span><br><span class="line">	<span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="6-4-6-全外连接"><a href="#6-4-6-全外连接" class="headerlink" title="6.4.6    全外连接"></a>6.4.6    全外连接</h3><p><strong>全外连接：</strong>将会返回所有表中符合 WHERE 语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值替代。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> </span><br><span class="line">	<span class="keyword">select</span> e.empno, e.ename, d.deptno </span><br><span class="line">	<span class="keyword">from</span> emp e <span class="keyword">full</span> <span class="keyword">join</span> dept d </span><br><span class="line">	<span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="6-4-7-多表连接"><a href="#6-4-7-多表连接" class="headerlink" title="6.4.7    多表连接"></a>6.4.7    多表连接</h3><p><strong>注意：</strong>连接 <code>n</code> 个表，至少需要 <code>n-1</code> 个连接条件。例如：连接三个表，至少需要两个连接条件。</p>
<p><strong>数据准备</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1700	Beijing</span><br><span class="line">1800	London</span><br><span class="line">1900	Tokyo</span><br></pre></td></tr></table></figure>



<ol>
<li><p>创建位置表</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> default.location(</span><br><span class="line">    loc <span class="type">int</span>,</span><br><span class="line">    loc_name string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>导入数据</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/location.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> default.location;</span><br></pre></td></tr></table></figure></li>
<li><p>多表连接查询</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span></span><br><span class="line">    <span class="keyword">SELECT</span> e.ename, d.deptno, l. loc_name</span><br><span class="line">    <span class="keyword">FROM</span>   emp e </span><br><span class="line">    <span class="keyword">JOIN</span>   dept d</span><br><span class="line">    <span class="keyword">ON</span>     d.deptno <span class="operator">=</span> e.deptno </span><br><span class="line">    <span class="keyword">JOIN</span>   location l</span><br><span class="line">    <span class="keyword">ON</span>     d.loc <span class="operator">=</span> l.loc;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>大多数情况下，Hive 会对每对 <code>JOIN</code> 连接对象启动一个 <code>MapReduce</code> 任务。本例中会首先启动一个 <code>MapReduce job</code> 对表 <code>e</code> 和表 <code>d</code> 进行连接操作，然后会再启动一个 <code>MapReduce job</code> 将第一个 <code>MapReduce job</code> 的输出和表 <code>l</code> 进行连接操作。</p>
<p>注意：为什么不是表 <code>d</code> 和表 <code>l</code> 先进行连接操作呢？这是因为 Hive 总是按照从左到右的顺序执行的。</p>
<hr>
<h3 id="6-4-8-笛卡尔积"><a href="#6-4-8-笛卡尔积" class="headerlink" title="6.4.8    笛卡尔积"></a>6.4.8    笛卡尔积</h3><p>笛卡尔集会在下面条件下产生：</p>
<ol>
<li> 省略连接条件</li>
<li> 连接条件无效</li>
<li> 所有表中的所有行互相连接</li>
</ol>
<p><strong>案例实操</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> empno, dname <span class="keyword">from</span> emp, dept;</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="6-4-9-连接谓词中不支持or"><a href="#6-4-9-连接谓词中不支持or" class="headerlink" title="6.4.9    连接谓词中不支持or"></a>6.4.9    连接谓词中不支持or</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	select e.empno, e.ename, d.deptno </span><br><span class="line">	from emp e join dept d </span><br><span class="line">	on e.deptno = d.deptno or e.ename=d.ename;   # 错误的</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="6-5-排序"><a href="#6-5-排序" class="headerlink" title="6.5 排序"></a>6.5 排序</h2><p>Hive 的本质是 MapReduce，所以 Hive 的排序和 MapReduce 中的排序相关。在 MapReduce 中，排序可分为：</p>
<ul>
<li>  全排序：结果只有一个（只有一个分区），所有的数据整体有序</li>
<li>  部分排序：结果有多个（有多个分区），每个分区内部有序</li>
</ul>
<p>排序是在 Shuffle 阶段完成的，在 Reduce 阶段开始前就已经完成了。</p>
<h3 id="6-5-1-全局排序（Order-By）"><a href="#6-5-1-全局排序（Order-By）" class="headerlink" title="6.5.1    全局排序（Order By）"></a>6.5.1    全局排序（Order By）</h3><ol>
<li> <code>Order By</code>：全排序，只有一个分区，使用一个 <code>ReducTask</code> 进程</li>
<li>使用 <code>ORDER BY</code> 子句排序<ul>
<li>  <code>ASC（ascend）</code>: 升序（默认）</li>
<li>  <code>DESC（descend）</code>: 降序</li>
</ul>
</li>
<li> <code>ORDER BY</code> 子句在 <code>SELECT</code> 语句的结尾</li>
</ol>
<p><strong>案例实操</strong></p>
<ol>
<li><p>查询员工信息按工资升序排列</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp order by sal;</span><br></pre></td></tr></table></figure></li>
<li><p>查询员工信息按工资降序排列</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp order by sal desc;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h3 id="6-5-2-按照别名排序"><a href="#6-5-2-按照别名排序" class="headerlink" title="6.5.2    按照别名排序"></a>6.5.2    按照别名排序</h3><ol>
<li><p>按照员工薪水的 2 倍排序</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename, sal*2 twosal from emp order by twosal;</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="6-5-3-复合排序"><a href="#6-5-3-复合排序" class="headerlink" title="6.5.3    复合排序"></a>6.5.3    复合排序</h3><p>按照多个列的值进行排序</p>
<ol>
<li><p>按照部门号和工资升序排序</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename, deptno, sal from emp order by deptno, sal ;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h3 id="6-5-4-部分排序（Sort-By）"><a href="#6-5-4-部分排序（Sort-By）" class="headerlink" title="6.5.4    部分排序（Sort By）"></a>6.5.4    部分排序（Sort By）</h3><p><code>Sort By</code>：每个分区内部排序，对全局结果集来说不是排序。对于多个分区需要设置 <code>reduceTaskNum &gt; 1</code>。</p>
<ol>
<li><p>设置 <code>reduceTaskNum</code> 的数量，决定了分区的个数</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces=3;</span><br></pre></td></tr></table></figure>

</li>
<li><p>查看 <code>reduceTaskNum</code> 的值</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces;</span><br></pre></td></tr></table></figure>

</li>
<li><p>根据部门编号降序查看员工信息</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp sort by empno desc;</span><br></pre></td></tr></table></figure>

</li>
<li><p>将查询结果导入到文件中（按照部门编号降序排序）</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">	insert overwrite local directory &#x27;/opt/module/datas/sortby-result&#x27;</span><br><span class="line">	select * from emp sort by deptno desc;</span><br></pre></td></tr></table></figure></li>
</ol>
<p><code>Hive</code> 底层调用了 <code>MapReduce</code>，对于 <code>MapReduce</code> 来说，默认的分区器是 <code>Hash</code> 分区器，并且采用键值对数据中 key 的 <code>hash</code> 值来进行分区。对于使用 <code>sort by</code> 进行分区内排序的记录，我们无法确定哪一个字段将会作为 <code>hash</code> 分区器的 key，所以说 <code>sort by</code> 是随机分区，能够作为 <code>key</code> 的字段是随机的。如果希望能够显式指定一个表中的字段作为 <code>key</code>，则需要使用 <code>Distribute By</code> 关键字来进行分区间的排序。</p>
<hr>
<h3 id="6-5-5-分区排序（Distribute-By）"><a href="#6-5-5-分区排序（Distribute-By）" class="headerlink" title="6.5.5    分区排序（Distribute By）"></a>6.5.5    分区排序（Distribute By）</h3><p><code>Distribute By</code>：指定按照哪个字段分区。类似 <code>MR</code> 中 <code>partition</code>，进行分区，结合 <code>sort by</code> 使用。</p>
<p><strong>注意</strong>：<code>Hive</code> 要求 <code>DISTRIBUTE BY</code> 语句要写在 <code>SORT BY</code> 语句之前。</p>
<p>对于 <code>distribute by</code> 进行测试，一定要分配多 <code>reduce</code> 进行处理，否则无法看到 <code>distribute by</code> 的效果。</p>
<p><strong>案例实操：</strong></p>
<ol>
<li><p>先按照部门编号分区，再按照员工编号降序排序。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   hive (default)&gt; set mapreduce.job.reduces=3;</span><br><span class="line">   hive (default)&gt; </span><br><span class="line">   	insert overwrite local directory &#x27;/opt/module/datas/distribute-result&#x27; </span><br><span class="line">   	select * from emp </span><br><span class="line">   	distribute by deptno 	# 指定作为分区器key的字段</span><br><span class="line">sort by empno desc;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<h3 id="6-5-6-Cluster-By"><a href="#6-5-6-Cluster-By" class="headerlink" title="6.5.6    Cluster By"></a>6.5.6    Cluster By</h3><p>如果用于分区的字段和用于排序的字段相同时，可以简写为 <code>cluster by</code> 。</p>
<p>当 <code>distribute by</code> 和 <code>sorts by</code> 排序的字段相同时，可以使用 <code>cluster by</code> 的方式。</p>
<p><code>cluster by</code> 除了具有 <code>distribute by</code> 的功能外还兼具 <code>sort by</code> 的功能。但是排序只能是升序排序，不能指定排序规则为 <code>ASC</code> 或者 <code>DESC</code>。</p>
<ul>
<li><p>以下两种写法等价</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br></pre></td></tr></table></figure>

  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp distribute by deptno sort by deptno;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/11/Spark-Streaming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/11/Spark-Streaming/" class="post-title-link" itemprop="url">Spark_Streaming</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-11 20:50:37" itemprop="dateCreated datePublished" datetime="2021-12-11T20:50:37+08:00">2021-12-11</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/11/Spark-SQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/11/Spark-SQL/" class="post-title-link" itemprop="url">Spark_SQL</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-11 20:50:18" itemprop="dateCreated datePublished" datetime="2021-12-11T20:50:18+08:00">2021-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-06-07 20:58:21" itemprop="dateModified" datetime="2022-06-07T20:58:21+08:00">2022-06-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="第-1-章-Spark-SQL-概述"><a href="#第-1-章-Spark-SQL-概述" class="headerlink" title="第 1 章    Spark SQL 概述"></a>第 1 章    Spark SQL 概述</h1><h2 id="1-1-什么是-Spark-SQL"><a href="#1-1-什么是-Spark-SQL" class="headerlink" title="1.1    什么是 Spark SQL"></a>1.1    什么是 Spark SQL</h2><p><code>Spark SQL</code> 是 <code>Spark</code> 用于处理结构化数据的模块。</p>
<p>与基础的 <code>Spark RDD API</code> 不同， <code>Spark SQL</code> 的抽象数据类型为 <code>Spark</code> 提供了关于数据结构和正在执行的计算的更多信息。</p>
<p>在内部， <code>Spark SQL</code> 使用这些额外的信息去做一些额外的优化。</p>
<p><code>Spark</code> 提供了多种方式与 <code>Spark SQL</code> 进行交互，比如： <code>SQL</code> 和 <code>Dataset API</code>。当计算结果的时候，使用的是相同的执行引擎，不依赖你正在使用哪种 <code>API</code> 或者语言。这种统一也就意味着开发者可以很容易在不同的 <code>API</code> 之间进行切换，这些 <code>API</code> 提供了最自然的方式来表达给定的转换。</p>
<p>我们已经学习了 <code>Hive</code>，它是将 <code>Hive SQL</code> 转换成 <code>MapReduce</code> 然后提交到集群上执行，大大简化了编写 <code>MapReduce</code> 程序的复杂性，由于 <code>MapReduce</code> 这种计算模型执行效率比较慢，所以 <code>Spark SQL</code> 的应运而生，它是将 <code>Spark SQL</code> 转换成 <code>RDD</code>，然后提交到集群执行，执行效率非常快。</p>
<p><code>Spark SQL</code> 提供了 2 个编程抽象，类似 <code>Spark Core</code> 中的 <code>RDD</code>。它们分别是：</p>
<ul>
<li>  <strong>DataFrame</strong></li>
<li>  <strong>DataSet</strong></li>
</ul>
<hr>
<h2 id="1-2-Spark-SQL-的特点"><a href="#1-2-Spark-SQL-的特点" class="headerlink" title="1.2    Spark SQL 的特点"></a>1.2    Spark SQL 的特点</h2><h3 id="1-Integrated-易整合"><a href="#1-Integrated-易整合" class="headerlink" title="1. Integrated(易整合)"></a>1. Integrated(易整合)</h3><p>无缝的整合了 <code>SQL</code> 查询和 <code>Spark</code> 编程。</p>
<h3 id="2-Uniform-Data-Access-统一的数据访问方式"><a href="#2-Uniform-Data-Access-统一的数据访问方式" class="headerlink" title="2. Uniform Data Access(统一的数据访问方式)"></a>2. Uniform Data Access(统一的数据访问方式)</h3><p>使用相同的方式连接不同的数据源。</p>
<h3 id="3-Hive-Integration（集成-Hive）"><a href="#3-Hive-Integration（集成-Hive）" class="headerlink" title="3. Hive Integration（集成 Hive）"></a>3. Hive Integration（集成 Hive）</h3><p>在已有的仓库上直接运行 <code>SQL</code> 或者 <code>HQL</code></p>
<p><img src="/2021/12/11/Spark-SQL/image-20211221125130508.png" alt="image-20211221125130508"></p>
<h3 id="4-Standard-Connectivity（标准的连接方式）"><a href="#4-Standard-Connectivity（标准的连接方式）" class="headerlink" title="4. Standard Connectivity（标准的连接方式）"></a>4. Standard Connectivity（标准的连接方式）</h3><p>通过 <code>JDBC</code> 或者 <code>ODBC</code> 来连接</p>
<hr>
<h2 id="1-3-什么是-DataFrame"><a href="#1-3-什么是-DataFrame" class="headerlink" title="1.3    什么是 DataFrame"></a>1.3    什么是 DataFrame</h2><p>与 <code>RDD</code> 类似，<strong>DataFrame 也是一个分布式的数据容器</strong>。</p>
<p>然而 <code>DataFrame</code> 更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息（元数据信息），即 <code>schema</code>。</p>
<p>同时，与 <code>Hive</code> 类似，<code>DataFrame</code> 也支持嵌套数据类型（<code>struct</code>、<code>array</code> 和 <code>map</code>）。</p>
<p>从 <code>API</code> 易用性的角度上看， <code>DataFrame API</code> 提供了一套更高层的关系操作，比函数式的 <code>RDD API</code> 要更加友好，门槛更低。</p>
<p><img src="/2021/12/11/Spark-SQL/image-20211216221858916.png" alt="image-20211216221858916"></p>
<p>上图直观地体现了 <code>DataFrame</code> 和 <code>RDD</code> 的区别：</p>
<ul>
<li>  左侧的 <code>RDD[Person]</code> 虽然以 <code>Person</code> 为类型参数，但 <code>Spark</code> 框架本身不了解 <code>Person</code> 类的内部结构。</li>
<li>  而右侧的 <code>DataFrame</code> 却提供了详细的结构信息，通过 <code>Spark SQL</code> 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。</li>
</ul>
<p><code>DataFrame</code> 为数据提供了 <code>Schema（元数据）</code> 的视图，可以把它当做数据库中的一张表来对待。</p>
<p><code>DataFrame</code> 也是懒执行的。</p>
<p><code>DataFrame</code> 性能上比 <code>RDD</code> 要高，原因是 <code>Spark</code> 底层会通过 <code>Spark catalyst optimiser</code> 对这种类 <code>SQL</code> 的语句进行优化。比如下面一个例子：</p>
<p> <img src="/2021/12/11/Spark-SQL/image-20211216222125022.png" alt="image-20211216222125022"></p>
<p>为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个 <code>DataFrame</code>，将它们 <code>join</code> 之后又做了一次 <code>filter</code> 操作。</p>
<p>如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为 <code>join</code> 是一个代价较大的操作，也可能会产生一个较大的数据集。</p>
<p>如果我们能将 <code>filter</code> 下推到 <code>join</code> 下方，先对 <code>DataFrame</code> 进行过滤，再 <code>join</code> 过滤后的较小的结果集，便可以有效缩短执行时间。</p>
<p>而 <code>Spark SQL</code> 的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。</p>
<p><img src="/2021/12/11/Spark-SQL/image-20211216222328574.png" alt="image-20211216222328574"></p>
<hr>
<h2 id="1-4-什么是-DataSet"><a href="#1-4-什么是-DataSet" class="headerlink" title="1.4    什么是 DataSet"></a>1.4    什么是 DataSet</h2><ol>
<li>   是 <code>DataFrame API</code> 的一个扩展，是 <code>SparkSQL</code> 最新的数据抽象（1.6 新增）。</li>
<li>   用户友好的 <code>API</code> 风格，既具有类型安全检查也具有 <code>DataFrame</code> 的查询优化特性。</li>
<li>   <code>Dataset</code> 支持编/解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。</li>
<li>   样例类被用来在 <code>DataSet</code> 中定义数据的结构信息，样例类中每个属性的名称直接映射到 <code>DataSet</code> 中的字段名称。</li>
<li>   <code>DataFrame</code> 是 <code>DataSet</code> 的特例，<code>DataFrame=DataSet[Row]</code>，所以可以通过 <code>as</code> 方法将 <code>DataFrame</code> 转换为 <code>DataSet</code>。 <code>Row</code> 是一个类型，跟 <code>Car</code>、 <code>Person</code> 这些类型一样，所有的表结构信息都可以用 <code>Row</code> 来表示。</li>
<li>   <code>DataSet</code> 是强类型的。比如可以有 <code>DataSet[Car]</code>，<code>DataSet[Person]</code> 等。</li>
<li>   <code>DataFrame</code> 只是知道字段，但是不知道字段的类型，所以是没办法在编译的时候检查是否类型失败的，比如你可以对一个 <code>String</code> 进行减法操作，在执行的时候才报错，而 <code>DataSet</code> 不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟 <code>JSON</code> 对象和类对象之间的类比。</li>
</ol>
<hr>
<h1 id="第-2-章-Spark-SQL-编程"><a href="#第-2-章-Spark-SQL-编程" class="headerlink" title="第 2 章 Spark SQL 编程"></a>第 2 章 Spark SQL 编程</h1><p>本章重点学习如何使用 <code>DataFrame</code> 和 <code>DataSet</code> 进行编程，已以及它们之间的关系和转换。</p>
<hr>
<h2 id="2-1-SparkSession"><a href="#2-1-SparkSession" class="headerlink" title="2.1    SparkSession"></a>2.1    SparkSession</h2><p>在老的版本中， <code>SparkSQL</code> 提供两种 <code>SQL</code> 查询起始点：一个叫 <code>SQLContext</code>，用于 <code>Spark</code> 自己提供的 <code>SQL</code> 查询；一个叫 <code>HiveContext</code>，用于连接 <code>Hive</code> 的查询。</p>
<p>从 2.0 开始，<code>SparkSession</code> 是 <code>Spark</code> 最新的 <code>SQL</code> 查询起始点，实质上是 <code>SQLContext</code> 和 <code>HiveContext</code> 的组合，所以在 <code>SQLContext</code> 和<code>HiveContext</code> 上可用的 <code>API</code> 在 <code>SparkSession</code> 上同样是可以使用的。</p>
<p><code>SparkSession</code> 内部封装了 <code>SparkContext</code>，所以计算实际上是由 <code>SparkContext</code> 完成的。</p>
<p>当我们使用 <code>spark-shell</code> 客户端的时候， <code>Spark</code> 会自动的创建一个叫做 <code>spark</code> 的 <code>SparkSession</code> 对象，就像我们以前可以自动获取到一个 <code>sc</code> 来表示 <code>SparkContext</code> 对象。</p>
<p><img src="/2021/12/11/Spark-SQL/image-20211217222307261.png" alt="image-20211217222307261"></p>
<hr>
<h2 id="2-2-使用-DataFrame-进行编程"><a href="#2-2-使用-DataFrame-进行编程" class="headerlink" title="2.2    使用 DataFrame 进行编程"></a>2.2    使用 DataFrame 进行编程</h2><p>首先学习 <code>DataFrame</code> 相关的知识。</p>
<p><code>Spark SQL</code> 的 <code>DataFrame API</code> 允许我们使用 <code>DataFrame</code> 而不用必须去注册临时表或者生成 <code>SQL</code> 表达式。<br><code>DataFrame API</code> 既有 <code>transformation</code> 操作也有 <code>action</code> 操作。<code>DataFrame</code> 的转换从本质上来说更具有关系，而 <code>DataSet API</code> 提供了更加函数式的 <code>API</code>。</p>
<hr>
<h3 id="2-2-1-创建-DataFrame"><a href="#2-2-1-创建-DataFrame" class="headerlink" title="2.2.1    创建 DataFrame"></a>2.2.1    创建 DataFrame</h3><p>在得到了 <code>SparkSession</code> 对象后，通过 <code>SparkSession</code> 有 2 种方式来创建 <code>DataFrame</code>：</p>
<ol>
<li> 加载数据源创建 <code>DataFrame</code>。常见的数据源有：<code>JDBC、json、scala集合、Hive 等等</code></li>
<li> 通过已知的 <code>RDD</code> 得到 <code>DataFrame</code></li>
</ol>
<hr>
<h4 id="2-2-1-1-通过数据源创建-DataFrame"><a href="#2-2-1-1-通过数据源创建-DataFrame" class="headerlink" title="2.2.1.1    通过数据源创建 DataFrame"></a>2.2.1.1    通过数据源创建 DataFrame</h4><p><code>Spark</code> 支持的数据源：</p>
<p><img src="/2021/12/11/Spark-SQL/image-20211217095851402.png" alt="image-20211217095851402"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取 json 文件</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/employees.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, salary: bigint]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 展示结果</span></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+-------+------+</span><br><span class="line">|   name|salary|</span><br><span class="line">+-------+------+</span><br><span class="line">|<span class="type">Michael</span>|  <span class="number">3000</span>|</span><br><span class="line">|   <span class="type">Andy</span>|  <span class="number">4500</span>|</span><br><span class="line">| <span class="type">Justin</span>|  <span class="number">3500</span>|</span><br><span class="line">|  <span class="type">Berta</span>|  <span class="number">4000</span>|</span><br><span class="line">+-------+------+</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="2-2-1-2-通过-RDD-进行转换"><a href="#2-2-1-2-通过-RDD-进行转换" class="headerlink" title="2.2.1.2    通过 RDD 进行转换"></a>2.2.1.2    通过 RDD 进行转换</h4><p>后面章节专门讨论</p>
<hr>
<h4 id="2-2-1-3-通过查询-Hive-表创建"><a href="#2-2-1-3-通过查询-Hive-表创建" class="headerlink" title="2.2.1.3    通过查询 Hive 表创建"></a>2.2.1.3    通过查询 Hive 表创建</h4><p>后面章节专门讨论</p>
<hr>
<h3 id="2-2-2-DataFrame-语法风格"><a href="#2-2-2-DataFrame-语法风格" class="headerlink" title="2.2.2    DataFrame 语法风格"></a>2.2.2    DataFrame 语法风格</h3><h4 id="2-2-2-1-SQL-语法风格（重要）"><a href="#2-2-2-1-SQL-语法风格（重要）" class="headerlink" title="2.2.2.1    SQL 语法风格（重要）"></a>2.2.2.1    SQL 语法风格（重要）</h4><p><code>SQL</code> 语法风格是指我们查询数据的时候使用 <code>SQL</code> 语句来查询。</p>
<ul>
<li>  <strong>这种风格的查询必须要有临时视图或者全局视图来辅助</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载指定数据源，创建 DataFrom 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个临时视图（临时表）</span></span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)	</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行查询</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from people&quot;</span>).show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<ul>
<li>  临时视图只能在当前 <code>Session</code> 有效，在新的 <code>Session</code> 中无效。</li>
<li>  可以创建全局视图。访问全局视图需要全路径，如：<code>global_temp.xxx</code></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载指定数据源，创建 DataFrom 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建全局视图</span></span><br><span class="line">scala&gt; df.createGlobalTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行查询</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from global_temp.people&quot;</span>)</span><br><span class="line">res31: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; res31.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line"><span class="comment">// 全局视图在新的 session 中同样生效</span></span><br><span class="line">scala&gt; spark.newSession.sql(<span class="string">&quot;select * from global_temp.people&quot;</span>)</span><br><span class="line">res33: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; res33.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure>





<hr>
<h4 id="2-2-2-2-DSL-语法风格（了解）"><a href="#2-2-2-2-DSL-语法风格（了解）" class="headerlink" title="2.2.2.2    DSL 语法风格（了解）"></a>2.2.2.2    DSL 语法风格（了解）</h4><ul>
<li>  <code>DataFrame</code> 提供一个特定领域语言（<code>domain-specific language，DSL</code>）去管理结构化的数据。可以在 <code>Scala</code>、 <code>Java</code>、 <code>Python</code> 和 <code>R</code> 中使用 <code>DSL</code>。</li>
<li>  使用 <code>DSL</code> 语法风格就不必创建临时视图了。</li>
</ul>
<h5 id="1-查看-Schema-信息"><a href="#1-查看-Schema-信息" class="headerlink" title="1    查看 Schema 信息"></a>1    查看 Schema 信息</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载指定数据源，创建 DataFrom 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查看df数据集的元数据信息（字段信息）</span></span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line">|-- age: long (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- name: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>



<hr>
<h5 id="2-使用-DSL-查询"><a href="#2-使用-DSL-查询" class="headerlink" title="2    使用 DSL 查询"></a>2    使用 DSL 查询</h5><ol>
<li><p>只查询 <code>name</code> 列数据</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select($<span class="string">&quot;name&quot;</span>).show</span><br><span class="line">+-------+</span><br><span class="line">|   name|</span><br><span class="line">+-------+</span><br><span class="line">|<span class="type">Michael</span>|</span><br><span class="line">|   <span class="type">Andy</span>|</span><br><span class="line">| <span class="type">Justin</span>|</span><br><span class="line">+-------+</span><br><span class="line"></span><br><span class="line">scala&gt; df.select(<span class="string">&quot;name&quot;</span>).show</span><br><span class="line">+-------+</span><br><span class="line">|   name|</span><br><span class="line">+-------+</span><br><span class="line">|<span class="type">Michael</span>|</span><br><span class="line">|   <span class="type">Andy</span>|</span><br><span class="line">| <span class="type">Justin</span>|</span><br><span class="line">+-------+</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询 <code>name</code> 和 <code>age</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).show</span><br><span class="line">+-------+----+</span><br><span class="line">|   name| age|</span><br><span class="line">+-------+----+</span><br><span class="line">|<span class="type">Michael</span>|<span class="literal">null</span>|</span><br><span class="line">|   <span class="type">Andy</span>|  <span class="number">30</span>|</span><br><span class="line">| <span class="type">Justin</span>|  <span class="number">19</span>|</span><br><span class="line">+-------+----+</span><br></pre></td></tr></table></figure>

</li>
<li><p>查询 <code>name </code>和 <code>age + 1</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select($<span class="string">&quot;name&quot;</span>, $<span class="string">&quot;age&quot;</span> + <span class="number">1</span>).show</span><br><span class="line">+-------+---------+</span><br><span class="line">|   name|(age + <span class="number">1</span>)|</span><br><span class="line">+-------+---------+</span><br><span class="line">|<span class="type">Michael</span>|     <span class="literal">null</span>|</span><br><span class="line">|   <span class="type">Andy</span>|       <span class="number">31</span>|</span><br><span class="line">| <span class="type">Justin</span>|       <span class="number">20</span>|</span><br><span class="line">+-------+---------+</span><br></pre></td></tr></table></figure></li>
<li><p>   <strong>注意：涉及到运算的时候，每列都必须使用 <code>$</code>。</strong></p>
</li>
<li><p>查询 <code>age &gt; 20</code> 的数据</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.filter($<span class="string">&quot;age&quot;</span> &gt; <span class="number">21</span>).show</span><br><span class="line">+---+----+</span><br><span class="line">|age|name|</span><br><span class="line">+---+----+</span><br><span class="line">| <span class="number">30</span>|<span class="type">Andy</span>|</span><br><span class="line">+---+----+</span><br></pre></td></tr></table></figure></li>
<li><p>按照 <code>age</code> 分组，查看数据条数</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.groupBy(<span class="string">&quot;age&quot;</span>).count.show</span><br><span class="line">+----+-----+</span><br><span class="line">| age|count|</span><br><span class="line">+----+-----+</span><br><span class="line">|  <span class="number">19</span>|    <span class="number">1</span>|</span><br><span class="line">|<span class="literal">null</span>|    <span class="number">1</span>|</span><br><span class="line">|  <span class="number">30</span>|    <span class="number">1</span>|</span><br><span class="line">+----+-----+</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="2-2-3-RDD-和-DataFrame-的交互"><a href="#2-2-3-RDD-和-DataFrame-的交互" class="headerlink" title="2.2.3    RDD 和 DataFrame 的交互"></a>2.2.3    RDD 和 DataFrame 的交互</h3><h4 id="1、RDD-gt-DataFrame"><a href="#1、RDD-gt-DataFrame" class="headerlink" title="1、RDD =&gt; DataFrame"></a>1、<code>RDD =&gt; DataFrame</code></h4><p>涉及到 <code>RDD，DataFrame，DataSet</code> 之间的操作时，需要先导入 <code>import spark.implicits._</code>，这里的 <code>spark</code> 不是包名，而是表示 <code>SparkSession</code> 对象。所以必须先创建 <code>SparkSession</code> 对象再导入，<code>implicits</code> 是一个内部 <code>object</code>。</p>
<ol>
<li><p>首先创建一个 <code>RDD</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.txt&quot;</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = /opt/module/spark-local/examples/src/main/resources/people.txt <span class="type">MapPartitionsRDD</span>[<span class="number">10</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h5 id="手动转换"><a href="#手动转换" class="headerlink" title="手动转换"></a>手动转换</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = rdd1.map(line =&gt; &#123; <span class="keyword">val</span> paras = line.split(<span class="string">&quot;, &quot;</span>); (paras(<span class="number">0</span>), paras(<span class="number">1</span>).toInt)&#125;)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">11</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 RDD 转换为 DataFrame。转换的时候需要手动指定每个数据对应的字段名</span></span><br><span class="line">scala&gt; rdd2.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).show</span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|<span class="type">Michael</span>| <span class="number">29</span>|</span><br><span class="line">|   <span class="type">Andy</span>| <span class="number">30</span>|</span><br><span class="line">| <span class="type">Justin</span>| <span class="number">19</span>|</span><br><span class="line">+-------+---+</span><br></pre></td></tr></table></figure>





<h5 id="通过样例类的反射机制转换（常用）"><a href="#通过样例类的反射机制转换（常用）" class="headerlink" title="通过样例类的反射机制转换（常用）"></a>通过样例类的反射机制转换（常用）</h5><ol>
<li><p>创建样例类</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name :<span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">People</span></span></span><br></pre></td></tr></table></figure>

</li>
<li><p>使用样例把 <code>RDD</code> 转换成 <code>DataFrame</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = rdd1.map(line =&gt; &#123; <span class="keyword">val</span> paras = line.split(<span class="string">&quot;, &quot;</span>); <span class="type">People</span>(paras(<span class="number">0</span>), paras(<span class="number">1</span>).toInt) &#125;)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">People</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at map at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.toDF.show</span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|<span class="type">Michael</span>| <span class="number">29</span>|</span><br><span class="line">|   <span class="type">Andy</span>| <span class="number">30</span>|</span><br><span class="line">| <span class="type">Justin</span>| <span class="number">19</span>|</span><br><span class="line">+-------+---+</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h5 id="通过-API-的方式转换（了解）"><a href="#通过-API-的方式转换（了解）" class="headerlink" title="通过 API 的方式转换（了解）"></a>通过 API 的方式转换（了解）</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">IntegerType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataFrameDemo2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">        .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        .appName(<span class="string">&quot;RDD2DF&quot;</span>)</span><br><span class="line">        .getOrCreate()</span><br><span class="line">        <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;lisi&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;zs&quot;</span>, <span class="number">20</span>), (<span class="string">&quot;zhiling&quot;</span>, <span class="number">40</span>)))</span><br><span class="line">        <span class="comment">// 映射出来一个 RDD[Row], 因为 DataFrame其实就是 DataSet[Row]</span></span><br><span class="line">        <span class="keyword">val</span> rowRdd: <span class="type">RDD</span>[<span class="type">Row</span>] = rdd.map(x =&gt; <span class="type">Row</span>(x._1, x._2))</span><br><span class="line">        <span class="comment">// 创建 StructType 类型：用于指定 ROW 中每列的列名和列类型</span></span><br><span class="line">        <span class="keyword">val</span> types = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;name&quot;</span>, <span class="type">StringType</span>), <span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>, <span class="type">IntegerType</span>)))</span><br><span class="line">        <span class="comment">// 创建 DF 对象</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.createDataFrame(rowRdd, types)</span><br><span class="line">        df.show</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="2、从-DataFrame-到-RDD"><a href="#2、从-DataFrame-到-RDD" class="headerlink" title="2、从 DataFrame 到 RDD"></a>2、从 DataFrame 到 RDD</h4><p>从 <code>DataFrame</code> 到 <code>RDD</code>，直接调用 <code>DataFrame</code> 的 <code>rdd</code> 方法就可以完成转换。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1、加载数据源，创建 DataFrame 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2、DataFrame =&gt; RDD</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = df.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at rdd at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res0: <span class="type">Array</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">Array</span>([<span class="literal">null</span>,<span class="type">Michael</span>], [<span class="number">30</span>,<span class="type">Andy</span>], [<span class="number">19</span>,<span class="type">Justin</span>])</span><br></pre></td></tr></table></figure>

<ul>
<li>  <strong>说明：</strong>转换后得到的 <code>RDD</code> 对象中存储的数据类型是 <code>Row</code>。</li>
</ul>
<hr>
<h2 id="2-3-使用-DataSet-进行编程"><a href="#2-3-使用-DataSet-进行编程" class="headerlink" title="2.3    使用 DataSet 进行编程"></a>2.3    使用 DataSet 进行编程</h2><p><code>DataSet</code> 和 <code>RDD</code> 类似，但是 <code>DataSet</code> 没有使用 <code>Java</code> 序列化或者 <code>Kryo</code> 序列化，而是使用了一种专门的编码器去序列化对象，然后在网络上传输。</p>
<p>虽然编码器和标准序列化都负责将对象转换成字节，但编码器是动态生成的代码，使用的格式允许 <code>Spark</code> 执行许多操作，如过滤、排序和哈希，而无需将字节反序列化回对象。</p>
<p><strong><code>DataSet</code> 是一种强类型的数据集合，需要提供对应的类型信息</strong>。而 <code>DataFrame</code> 是一种弱类型的数据集合，<code>DF</code> 中只能存储 <code>ROW</code> 类型的数据，至于 <code>ROW</code> 的具体细节 <code>DF</code> 是不知道的。</p>
<hr>
<h3 id="2-3-1-创建-DataSet"><a href="#2-3-1-创建-DataSet" class="headerlink" title="2.3.1    创建 DataSet"></a>2.3.1    创建 DataSet</h3><blockquote>
<ol>
<li> 通过 Scala 序列（集合）得到</li>
<li> 通过 RDD 转换得到</li>
<li> 通过 DF 转换得到</li>
<li> 通过 DS 转换得到新的 DS</li>
</ol>
</blockquote>
<h4 id="1、通过-Scala-序列（集合）得到"><a href="#1、通过-Scala-序列（集合）得到" class="headerlink" title="1、通过 Scala 序列（集合）得到"></a>1、通过 Scala 序列（集合）得到</h4><p><strong>Demo01</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 13:26</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CreateDS</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 Scala 序列</span></span><br><span class="line">    <span class="keyword">val</span> mylist: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 转换成 DS</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">Int</span>] = mylist.toDS()</span><br><span class="line">    <span class="comment">// 转换成 DF</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = mylist.toDF()</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 显然，将一个集合转换成 DS 后，我们可以通过解析样例类得到其数据的具体类型；</span></span><br><span class="line"><span class="comment">    * 而将集合转换为 DF 后，虽然我们同样可以得到这份数据，但是无法识别数据每个字段的真实类型，</span></span><br><span class="line"><span class="comment">    * 所有的数据都是 ROW 类型的。</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ds.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">+-----+</span></span><br><span class="line"><span class="comment">|value|</span></span><br><span class="line"><span class="comment">+-----+</span></span><br><span class="line"><span class="comment">|   10|</span></span><br><span class="line"><span class="comment">|   20|</span></span><br><span class="line"><span class="comment">|   30|</span></span><br><span class="line"><span class="comment">+-----+</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">我们可以发现，我们没有指定列名，Spark 自动用 value 作为列名了。</span></span><br><span class="line"><span class="comment">如果我们想要使用自己的列名，则可以使用样例类</span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br></pre></td></tr></table></figure>



<p><strong>Demo02</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 13:26</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CreateDS2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 Scala 序列</span></span><br><span class="line">    <span class="keyword">val</span> persons: <span class="type">List</span>[<span class="type">Person</span>] = <span class="type">List</span>(<span class="type">Person</span>(<span class="string">&quot;张三&quot;</span>, <span class="number">20</span>), <span class="type">Person</span>(<span class="string">&quot;李四&quot;</span>, <span class="number">30</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 转换成 DS</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">Person</span>] = persons.toDS()</span><br><span class="line">    <span class="comment">// 转换成 DF</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = persons.toDF()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ds.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">+----+---+</span></span><br><span class="line"><span class="comment">|name|age|</span></span><br><span class="line"><span class="comment">+----+---+</span></span><br><span class="line"><span class="comment">|  张三| 20|</span></span><br><span class="line"><span class="comment">|  李四| 30|</span></span><br><span class="line"><span class="comment">+----+---+</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">临时表的列名使用的是样例类中的属性</span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br></pre></td></tr></table></figure>



<ol>
<li><p>使用样例类的序列（集合）得到 <code>DataSet</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个样例类</span></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 为样例类创建一个编码器</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;lisi&quot;</span>, <span class="number">20</span>), <span class="type">Person</span>(<span class="string">&quot;zs&quot;</span>, <span class="number">21</span>)).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: int]</span><br><span class="line"></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">scala&gt; ds.show</span><br><span class="line">+----+---+</span><br><span class="line">|name|age|</span><br><span class="line">+----+---+</span><br><span class="line">|lisi| <span class="number">20</span>|</span><br><span class="line">| zs| <span class="number">21</span>|</span><br><span class="line">+----+---+</span><br></pre></td></tr></table></figure>

</li>
<li><p>使用基本类型的序列（集合）得到 <code>DataSet</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 基本类型的编码被自动创建. importing spark.implicits._</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Int</span>] = [value: int]</span><br><span class="line"></span><br><span class="line"><span class="comment">// </span></span><br><span class="line">scala&gt; ds.show</span><br><span class="line">+-----+</span><br><span class="line">|value|</span><br><span class="line">+-----+</span><br><span class="line">| <span class="number">1</span>|</span><br><span class="line">| <span class="number">2</span>|</span><br><span class="line">| <span class="number">3</span>|</span><br><span class="line">| <span class="number">4</span>|</span><br><span class="line">| <span class="number">5</span>|</span><br><span class="line">| <span class="number">6</span>|</span><br><span class="line">+-----+</span><br></pre></td></tr></table></figure>

<p>   说明：在实际使用的时候，很少用到把序列转换成 <code>DataSet</code>，更多的是通过 <code>RDD</code> 转换成 <code>DataSet</code>。</p>
</li>
</ol>
<hr>
<h3 id="2-3-2-RDD-和-DataSet-的交互"><a href="#2-3-2-RDD-和-DataSet-的交互" class="headerlink" title="2.3.2    RDD 和 DataSet 的交互"></a>2.3.2    RDD 和 DataSet 的交互</h3><h4 id="1、RDD-gt-DataSet"><a href="#1、RDD-gt-DataSet" class="headerlink" title="1、RDD =&gt; DataSet"></a>1、<code>RDD =&gt; DataSet</code></h4><p>使用反射来推断包含特定类型对象的 RDD 的 <code>schema</code>。</p>
<p>这种基于反射的方法可以生成更简洁的代码，并且当您在编写Spark应用程序时已经知道模式时，这种方法可以很好地工作。</p>
<p>为 <code>Spark SQL</code> 设计的 <code>Scala API</code> 可以自动的把包含样例类的 RDD 转换成 <code>DataSet</code>。</p>
<p>样例类定义了 <code>schema(表结构)</code>：样例类参数名通过反射被读到，然后成为列名。</p>
<p>样例类可以被嵌套，也可以包含复杂类型：像 <code>Seq</code> 或者 <code>Array</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> peopleRDD = sc.textFile(<span class="string">&quot;examples/src/main/resources/people.txt&quot;</span>)</span><br><span class="line">peopleRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = examples/src/main/resources/people.txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; peopleRDD.map(line =&gt; &#123;<span class="keyword">val</span> para = line.split(<span class="string">&quot;,&quot;</span>);<span class="type">Person</span>(para(<span class="number">0</span>),para(<span class="number">1</span>).trim.toInt)&#125;).toDS</span><br><span class="line">res0: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint]</span><br></pre></td></tr></table></figure>



<h4 id="2、DataSet-gt-RDD"><a href="#2、DataSet-gt-RDD" class="headerlink" title="2、DataSet =&gt; RDD"></a>2、<code>DataSet =&gt; RDD</code></h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">调用rdd方法即可</span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;lisi&quot;</span>, <span class="number">40</span>), <span class="type">Person</span>(<span class="string">&quot;zs&quot;</span>, <span class="number">20</span>)).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 把 ds 转换成 rdd</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = ds.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Person</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">8</span>] at rdd at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res5: <span class="type">Array</span>[<span class="type">Person</span>] = <span class="type">Array</span>(<span class="type">Person</span>(lisi,<span class="number">40</span>), <span class="type">Person</span>(zs,<span class="number">20</span>))</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="2-4-DataFrame-和-DataSet-之间的交互"><a href="#2-4-DataFrame-和-DataSet-之间的交互" class="headerlink" title="2.4    DataFrame 和 DataSet 之间的交互"></a>2.4    DataFrame 和 DataSet 之间的交互</h2><h3 id="2-4-1-DataFrame-gt-DataSet"><a href="#2-4-1-DataFrame-gt-DataSet" class="headerlink" title="2.4.1    DataFrame =&gt; DataSet"></a>2.4.1    <code>DataFrame =&gt; DataSet</code></h3><p><strong>spark-shell</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">People</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrame 转换成 DataSet</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = df.as[<span class="type">People</span>]</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">People</span>] = [age: bigint, name: string]</span><br></pre></td></tr></table></figure>

<p><strong>IDEA</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 13:49</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DF2DS</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、通过 SparkSession 创建 DataFrame 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 我们知道，相比于 DF 而言，DS 可以查看其内部保存数据的具体类型，</span></span><br><span class="line"><span class="comment">    * 而 DF 则是将所有的类型都用 ROW 类型来处理。</span></span><br><span class="line"><span class="comment">    * 所以如果想要实现 DF =&gt; DS 的转换，一般需要提供一个样例类用于将 ROW 类型的数据映射到样例类上</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="comment">// 5、DF =&gt; DS</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User</span>] = df.as[<span class="type">User</span>]</span><br><span class="line"></span><br><span class="line">    ds.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6、关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4、样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">  +---+----+</span></span><br><span class="line"><span class="comment">  |age|name|</span></span><br><span class="line"><span class="comment">  +---+----+</span></span><br><span class="line"><span class="comment">  | 15|  ls|</span></span><br><span class="line"><span class="comment">  | 21|  ww|</span></span><br><span class="line"><span class="comment">  | 22|  zs|</span></span><br><span class="line"><span class="comment">  | 23|  zl|</span></span><br><span class="line"><span class="comment">  +---+----+</span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br></pre></td></tr></table></figure>





<h3 id="2-4-2-DataSet-gt-DataFrame"><a href="#2-4-2-DataSet-gt-DataFrame" class="headerlink" title="2.4.2    DataSet =&gt; DataFrame"></a>2.4.2    <code>DataSet =&gt; DataFrame</code></h3><p><strong>DS =&gt; DF</strong></p>
<ol>
<li> 不需要隐式类型转换</li>
<li> 方法：<code>ds.toDF</code></li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;Andy&quot;</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = ds.toDF</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+----+---+</span><br><span class="line">|name|age|</span><br><span class="line">+----+---+</span><br><span class="line">|<span class="type">Andy</span>| <span class="number">32</span>|</span><br><span class="line">+----+---+</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="2-5-RDD-DataFrame-和-DataSet-之间的关系"><a href="#2-5-RDD-DataFrame-和-DataSet-之间的关系" class="headerlink" title="2.5    RDD, DataFrame 和 DataSet 之间的关系"></a>2.5    RDD, DataFrame 和 DataSet 之间的关系</h2><p>在 <code>SparkSQL</code> 为我们提供了两个新的抽象，分别是 <code>DataFrame</code> 和 <code>DataSet</code>。它们和 <code>RDD</code> 有什么区别呢？首先从版本的产生上来看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)</span><br></pre></td></tr></table></figure>

<p>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。</p>
<p>在后期的 <code>Spark</code> 版本中， <code>DataSet</code> 会逐步取代 <code>RDD</code> 和 <code>DataFrame</code> 成为唯一的 <code>API</code> 接口。</p>
<h3 id="2-5-1-三者的共性"><a href="#2-5-1-三者的共性" class="headerlink" title="2.5.1    三者的共性"></a>2.5.1    三者的共性</h3><ol>
<li>   <code>RDD</code>、 <code>DataFrame</code>、 <code>Dataset</code> 全都是 <code>Spark</code> 平台下的分布式弹性数据集，为处理超大型数据提供便利</li>
<li>   三者都有惰性机制，在进行创建、转换，如 <code>map</code> 方法时，不会立即执行，只有在遇到 <code>action</code> 时，三者才会开始遍历运算。</li>
<li>   三者都会根据 <code>Spark</code> 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</li>
<li>   三者都有 <code>partition</code> 的概念</li>
<li>   三者有许多共同的函数，如 <code>map</code>， <code>filter</code>，排序等</li>
<li>   在对 <code>DataFrame</code> 和 <code>Dataset</code> 进行操作许多操作都需要这个包进行支持 <code>import [spark].implicits._</code></li>
<li>   <code>DataFrame</code> 和 <code>Dataset</code> 均可使用模式匹配获取各个字段的值和类型</li>
</ol>
<h3 id="2-5-2-三者的区别"><a href="#2-5-2-三者的区别" class="headerlink" title="2.5.2    三者的区别"></a>2.5.2    三者的区别</h3><h4 id="2-5-2-1-RDD"><a href="#2-5-2-1-RDD" class="headerlink" title="2.5.2.1    RDD"></a>2.5.2.1    RDD</h4><ol>
<li>   <code>RDD</code> 一般和 <code>spark mllib</code> 同时使用</li>
<li>   <code>RDD</code> 不支持 <code>spark sql</code> 操作</li>
</ol>
<h4 id="2-5-2-2-DataFrame"><a href="#2-5-2-2-DataFrame" class="headerlink" title="2.5.2.2    DataFrame"></a>2.5.2.2    DataFrame</h4><ol>
<li>   与 <code>RDD</code> 和 <code>Dataset</code> 不同，<code>DataFrame</code>每一行的类型固定为 <code>Row</code>，每一列的值没法直接访问，只有通过解析才能获取各个字段的值</li>
<li>   <code>DataFrame</code> 与 <code>DataSet</code> 一般不与 <code>spark mlib</code> 同时使用</li>
<li>   <code>DataFrame</code> 与 <code>DataSet</code> 均支持 <code>SparkSQL</code> 的操作，比如 <code>select</code>， <code>groupby</code> 之类，还能注册临时表/视窗，进行 <code>sql</code> 语句操作</li>
<li>   <code>DataFrame</code> 与 <code>DataSet</code> 支持一些特别方便的保存方式，比如保存成 <code>csv</code>，可以带上表头，这样每一列的字段名一目了然（后面专门讲解）</li>
</ol>
<h4 id="2-5-2-3-DataSet"><a href="#2-5-2-3-DataSet" class="headerlink" title="2.5.2.3    DataSet"></a>2.5.2.3    DataSet</h4><ol>
<li>   <code>Dataset</code> 和 <code>DataFrame</code> 拥有完全相同的成员函数，区别只是每一行的数据类型不同。</li>
<li>   <code>DataFrame</code> 其实就是 <code>DataSet</code> 的一个特例</li>
<li>   <code>DataFrame</code> 也可以叫 <code>Dataset[Row]</code>，每一行的类型是 <code>Row</code>，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 <code>getAS</code> 方法或者共性中的第七条提到的模式匹配拿出特定字段。而 <code>Dataset</code> 中，每一行是什么类型是不一定的，在自定义了 <code>case class</code> 之后可以很自由的获得每一行的信息</li>
</ol>
<h3 id="2-5-3-三者的互相转换"><a href="#2-5-3-三者的互相转换" class="headerlink" title="2.5.3    三者的互相转换"></a>2.5.3    三者的互相转换</h3><ul>
<li><code>基础类型 =&gt; 高级类型</code>：<ol>
<li> 需要提供样例类</li>
<li> 需要导入隐式转换</li>
</ol>
</li>
<li><code>高级类型 =&gt; 基础类型</code>：<ol>
<li> 无需提供额外的信息，可以直接完成类型之间的转换</li>
</ol>
</li>
</ul>
<p><img src="/2021/12/11/Spark-SQL/image-20211221135048075.png" alt="image-20211221135048075"></p>
<hr>
<h2 id="2-6-使用-IDEA-创建-SparkSQL-程序"><a href="#2-6-使用-IDEA-创建-SparkSQL-程序" class="headerlink" title="2.6    使用 IDEA 创建 SparkSQL 程序"></a>2.6    使用 IDEA 创建 SparkSQL 程序</h2><h3 id="步骤1：添加-SparkSQL-依赖"><a href="#步骤1：添加-SparkSQL-依赖" class="headerlink" title="步骤1：添加 SparkSQL 依赖"></a>步骤1：添加 SparkSQL 依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="步骤2：具体代码"><a href="#步骤2：具体代码" class="headerlink" title="步骤2：具体代码"></a>步骤2：具体代码</h3><p><code>user.json</code></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;ls&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span>  <span class="number">15</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;ww&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span>  <span class="number">21</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;zs&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span>  <span class="number">22</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;zl&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span>  <span class="number">23</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>



<h4 id="1、创建-DF-对象"><a href="#1、创建-DF-对象" class="headerlink" title="1、创建 DF 对象"></a>1、创建 DF 对象</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/22 9:57</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 创建 DF 对象</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CreateDF</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 1、先创建一个 SparkSession 对象【构造器模式】</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;Word Count&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、通过 SparkSession 创建 DataFrame 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、对 DF 做各种操作</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建临时表</span></span><br><span class="line">    df.createTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">    <span class="comment">// 查询临时表</span></span><br><span class="line">    spark.sql(<span class="string">&quot;select * from user where age &gt; 19&quot;</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、关闭连接</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>Tip：</strong>在导入隐式转换时，使用到的 <code>spark</code> 关键字是由 <code>SparkSession</code> 的实例对象名来决定的，也就是说，如果我们创建的 <code>SparkSession</code> 实例名字叫 <code>hello</code>，那么在导入隐式转换时就应该导入 <code>import hello.implicits._</code></p>
<p><img src="/2021/12/11/Spark-SQL/image-20211224130331108.png" alt="image-20211224130331108"></p>
<h4 id="2、DF-gt-RDD"><a href="#2、DF-gt-RDD" class="headerlink" title="2、DF =&gt; RDD"></a>2、<code>DF =&gt; RDD</code></h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/22 10:23</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DF2RDD</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;Word Count&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个 sc 对象</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 得到一个 DF 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = (<span class="number">1</span> to <span class="number">10</span>).toDF(<span class="string">&quot;num&quot;</span>)</span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// df =&gt; rdd</span></span><br><span class="line">    <span class="comment">// 由 df 转换的来的 rdd 的泛型一定是 [Row]</span></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Row</span>] = df.rdd</span><br><span class="line">    rdd.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 取出 rdd 中每个 row 中的数据</span></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.map(row =&gt; row.getInt(<span class="number">0</span>))</span><br><span class="line">    rdd2.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h4 id="3、RDD-gt-DF"><a href="#3、RDD-gt-DF" class="headerlink" title="3、RDD =&gt; DF"></a>3、<code>RDD =&gt; DF</code></h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/22 10:08</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDD2DF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、先创建一个 SparkSession 对象【构造器模式】</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;Word Count&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、创建一个 sc 对象</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、导入用到的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、如果 RDD 中的数据保存在元组中，则在执行 toDF 方法时需要显式指定每列的名称</span></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.parallelize((<span class="string">&quot;ls&quot;</span>, <span class="number">10</span>) :: (<span class="string">&quot;zs&quot;</span>, <span class="number">20</span>) :: <span class="type">Nil</span>)</span><br><span class="line">    rdd.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5、如果 RDD 中保存的数据是一个样例类对象，则在执行 toDF 方法时无需手动指定每列的名称就可完成自动解析，</span></span><br><span class="line">    <span class="comment">// 因为样例中已经有了属性做列名</span></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">User</span>] = sc.parallelize(<span class="type">Array</span>(<span class="type">User</span>(<span class="string">&quot;路飞&quot;</span>, <span class="number">10</span>), <span class="type">User</span>(<span class="string">&quot;黄猿&quot;</span>, <span class="number">53</span>)))</span><br><span class="line">    rdd2.toDF().show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6、关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="2-7-自定义-SparkSQL-函数"><a href="#2-7-自定义-SparkSQL-函数" class="headerlink" title="2.7    自定义 SparkSQL 函数"></a>2.7    自定义 SparkSQL 函数</h2><p>在 <code>Shell</code> 窗口中可以通过 <code>spark.udf</code> 功能自定义函数。</p>
<hr>
<h3 id="2-7-1-自定义-UDF-函数"><a href="#2-7-1-自定义-UDF-函数" class="headerlink" title="2.7.1    自定义 UDF 函数"></a>2.7.1    自定义 UDF 函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个 udf 函数: toUpper是函数名, 第二个参数是函数的具体实现</span></span><br><span class="line">scala&gt; spark.udf.register(<span class="string">&quot;toUpper&quot;</span>, (s: <span class="type">String</span>) =&gt; s.toUpperCase)</span><br><span class="line">res1: org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span> = <span class="type">UserDefinedFunction</span>(&lt;function1&gt;,<span class="type">StringType</span>,<span class="type">Some</span>(<span class="type">List</span>(<span class="type">StringType</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 测试自定义的 UDF 函数</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select toUpper(name), age from people&quot;</span>).show</span><br><span class="line">+-----------------+----+</span><br><span class="line">|<span class="type">UDF</span>:toUpper(name)| age|</span><br><span class="line">+-----------------+----+</span><br><span class="line">|          <span class="type">MICHAEL</span>|<span class="literal">null</span>|</span><br><span class="line">|             <span class="type">ANDY</span>|  <span class="number">30</span>|</span><br><span class="line">|           <span class="type">JUSTIN</span>|  <span class="number">19</span>|</span><br><span class="line">+-----------------+----+</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="2-7-2-用户自定义聚合函数-UDAF"><a href="#2-7-2-用户自定义聚合函数-UDAF" class="headerlink" title="2.7.2    用户自定义聚合函数(UDAF)"></a>2.7.2    用户自定义聚合函数(UDAF)</h3><p>强类型的 <code>Dataset</code> 和弱类型的 <code>DataFrame</code> 都提供了相关的聚合函数，如 <code>count()</code>， <code>countDistinct()</code>， <code>avg()</code>， <code>max()</code>， <code>min()</code>。除此之外，用户还可以设定自己的自定义聚合函数。</p>
<p>自定义聚合函数需要继承 <code>UserDefinedAggregateFunction</code></p>
<p><strong>自定义聚合函数实现 sum</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataType</span>, <span class="type">DoubleType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 14:18</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *          自定义聚合函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UDAFDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 DF 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建临时视图</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册自定义函数</span></span><br><span class="line">    spark.udf.register(<span class="string">&quot;mySum&quot;</span>, <span class="keyword">new</span> <span class="type">CustomSum</span>())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用自定义的求和函数 mySum 执行聚合查询</span></span><br><span class="line">    spark.sql(<span class="string">&quot;select mySum(age) from user&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">  +------------------------------+</span></span><br><span class="line"><span class="comment">  |customsum(CAST(age AS DOUBLE))|</span></span><br><span class="line"><span class="comment">  +------------------------------+</span></span><br><span class="line"><span class="comment">  |                          81.0|</span></span><br><span class="line"><span class="comment">  +------------------------------+</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义 UDAF 函数</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 1、自定义 UDAF 函数需要继承 UserDefinedAggregateFunction 类</span></span><br><span class="line"><span class="comment"> * 2、CustomSum 函数的功能等价于 Sum</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomSum</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定输入的类型。</span></span><br><span class="line"><span class="comment">  * 比如，在 select mySum(age) from User 中，age 的类型就是输入类型</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="comment">// 在自定义函数 mySum(age) 中，只有一个参数，且指定类型为 Double</span></span><br><span class="line">    <span class="comment">// 如果有多个参数，则需要指定多个 StructField</span></span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定缓冲区数据的类型。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 在计算聚合函数时（以 sum 为例），需要先计算出前两数之和，将临时结果放入缓冲区，</span></span><br><span class="line"><span class="comment">  * 再使用缓冲区中的结果与下一个数进行求和。就类似于 flap</span></span><br><span class="line"><span class="comment">  * 缓冲区的类型也需要手动指定</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;tmp&quot;</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定聚合函数执行结束后，最终返回结果的类型</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = &#123;</span><br><span class="line">    <span class="type">DoubleType</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定查询结果是否具有幂等性</span></span><br><span class="line"><span class="comment">  * （即多次查询是否能得到统一结果）</span></span><br><span class="line"><span class="comment">  * 一般都设置为 true</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 初始化缓冲区</span></span><br><span class="line"><span class="comment">  *   对于求和函数而言，初始化值应该为 0.00</span></span><br><span class="line"><span class="comment">  *   并且由于缓冲区可能不只缓冲一个字段，所以缓冲区的数据结构为可变集合</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// buffer 就是缓冲区中数据的集合</span></span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>D</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 分区内聚合：</span></span><br><span class="line"><span class="comment">   * 由于 RDD 是分布式数据集，所以聚合又可分为分区内聚合和分区间聚合</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param buffer 缓冲区内的值</span></span><br><span class="line"><span class="comment">   * @param input 传入缓冲区中的值</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 分区内求和</span></span><br><span class="line">    input <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="comment">// 如果收到的参数是 Double 类型，则取出 input 对象中的值，通过模式匹配匹配到 age 中，执行累加</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(age: <span class="type">Double</span>) =&gt; buffer(<span class="number">0</span>) = buffer.getDouble(<span class="number">0</span>) + age</span><br><span class="line">      <span class="comment">// 其它情况</span></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 分区间聚合</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param buffer1 一个分区的聚合结果</span></span><br><span class="line"><span class="comment">   * @param buffer2 另一个分区的聚合结果</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 分区间求和</span></span><br><span class="line">    <span class="comment">// 把 buffer1 和 buffer2 临时结果聚合到一起，然后再把值写回到 buffer1</span></span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getDouble(<span class="number">0</span>) + buffer2.getDouble(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 返回最终的输出值</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer.getDouble(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>自定义聚合函数实现 avg</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> day01.udf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataType</span>, <span class="type">DoubleType</span>, <span class="type">LongType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 14:18</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *          自定义聚合函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UDAFDemo2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 DF 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建临时视图</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册自定义函数</span></span><br><span class="line">    spark.udf.register(<span class="string">&quot;myAvg&quot;</span>, <span class="keyword">new</span> <span class="type">CustomAvg</span>())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用自定义的求和函数 mySum 执行聚合查询</span></span><br><span class="line">    spark.sql(<span class="string">&quot;select myAvg(age) from user&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomAvg</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定输入的类型。</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="comment">// 在自定义函数 myAvg(age) 中，只有一个参数，且指定类型为 Double</span></span><br><span class="line">    <span class="comment">// 如果有多个参数，则需要指定多个 StructField</span></span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定缓冲区数据的类型。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 在计算聚合函数时（以 sum 为例），需要先计算出前两数之和，将临时结果放入缓冲区，</span></span><br><span class="line"><span class="comment">  * 再使用缓冲区中的结果与下一个数进行求和。就类似于 flap</span></span><br><span class="line"><span class="comment">  * 缓冲区的类型也需要手动指定</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="comment">// count 用于计算 User 的个数</span></span><br><span class="line">    <span class="comment">// sum 用于计算 User 中 age 的累加和</span></span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>) :: <span class="type">StructField</span>(<span class="string">&quot;sum&quot;</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定聚合函数执行结束后，最终返回结果的类型</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = &#123;</span><br><span class="line">    <span class="type">DoubleType</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定查询结果是否具有幂等性</span></span><br><span class="line"><span class="comment">  * （即多次查询是否能得到统一结果）</span></span><br><span class="line"><span class="comment">  * 一般都设置为 true</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 初始化缓冲区</span></span><br><span class="line"><span class="comment">  *   对于求和函数而言，初始化值应该为 0.00</span></span><br><span class="line"><span class="comment">  *   并且由于缓冲区可能不只缓冲一个字段，所以缓冲区的数据结构为可变集合</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// buffer 就是缓冲区中数据的集合</span></span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L  <span class="comment">// 缓冲区中 count 的初始值</span></span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>D  <span class="comment">// 缓冲区中 sum 的初始值</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 分区内聚合：</span></span><br><span class="line"><span class="comment">   * 由于 RDD 是分布式数据集，所以聚合又可分为分区内聚合和分区间聚合</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param buffer 缓冲区内的值</span></span><br><span class="line"><span class="comment">   * @param input 传入缓冲区中的值</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 分区内求和</span></span><br><span class="line">    input <span class="keyword">match</span> &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(age: <span class="type">Double</span>) =&gt; &#123;</span><br><span class="line">        buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + <span class="number">1</span>L</span><br><span class="line">        buffer(<span class="number">1</span>) = buffer.getDouble(<span class="number">1</span>) + age</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 其它情况</span></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 分区间聚合</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param buffer1 一个分区的聚合结果</span></span><br><span class="line"><span class="comment">   * @param buffer2 另一个分区的聚合结果</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 分区间求和</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 把 buffer1 和 buffer2 临时结果聚合到一起，然后再把值写回到 buffer1</span></span><br><span class="line">    buffer2 <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(count : <span class="type">Long</span>, age: <span class="type">Double</span>) =&gt; &#123;</span><br><span class="line">        buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + count</span><br><span class="line">        buffer1(<span class="number">1</span>) = buffer1.getDouble(<span class="number">1</span>) + age</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 其它情况</span></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 返回最终的输出值</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer.getDouble(<span class="number">1</span>) / buffer.getLong(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="第-3-章-SparkSQL-数据源"><a href="#第-3-章-SparkSQL-数据源" class="headerlink" title="第 3 章    SparkSQL 数据源"></a>第 3 章    SparkSQL 数据源</h1><p>本章介绍 <code>SparkSQL</code> 支持的各种数据源<code>(Data Sources)</code>。</p>
<p><code>Spark SQL</code> 的 <code>DataFrame</code> 接口支持操作多种数据源。一个 <code>DataFrame</code> 类型的对象可以像 <code>RDD</code> 那样操作（比如各种转换），也可以用来创建临时表。把 <code>DataFrame</code> 注册为一个临时表之后，就可以在它的数据上面执行 <code>SQL</code> 查询。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">读</span><br><span class="line">	通用读：spark.read.format(&quot;文件格式&quot;).load(&quot;文件路径&quot;)</span><br><span class="line">	专用读：spark.read.文件格式(&quot;文件路径&quot;)</span><br><span class="line"></span><br><span class="line">写</span><br><span class="line">	通用写：df.write.format(&quot;文件格式&quot;).save(&quot;文件路径&quot;)</span><br><span class="line">	专用写：df.write.文件格式(&quot;文件路径&quot;)</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="3-1-通用加载和保存函数"><a href="#3-1-通用加载和保存函数" class="headerlink" title="3.1    通用加载和保存函数"></a>3.1    通用加载和保存函数</h2><p>默认数据源格式是 <code>parquet</code>，我们也可以通过使用 <code>spark.sql.sources.default</code> 这个属性来设置默认的数据源格式。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载数据源中的数据到 DF 中</span></span><br><span class="line"><span class="keyword">val</span> usersDF = spark.read.load(<span class="string">&quot;examples/src/main/resources/users.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DF 中的数据写出到目的地</span></span><br><span class="line">usersDF.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;favorite_color&quot;</span>).write.save(<span class="string">&quot;namesAndFavColors.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>说明：</strong></p>
<ol>
<li>   <code>spark.read.load</code> 是加载数据的通用方法。</li>
<li>   <code>df.write.save</code> 是保存数据的通用方法。</li>
</ol>
<h3 id="3-1-1-手动指定选项"><a href="#3-1-1-手动指定选项" class="headerlink" title="3.1.1    手动指定选项"></a>3.1.1    手动指定选项</h3><p>也可以手动给数据源指定一些额外的选项。数据源应该用全名称来指定，但是对一些内置的数据源也可以使用短名称：<code>json</code>，<code>parquet</code>，<code>jdbc</code>，<code>orc,</code>libsvm<code>,</code>csv<code>，text</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// </span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// </span></span><br><span class="line">peopleDF.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).write.format(<span class="string">&quot;parquet&quot;</span>).save(<span class="string">&quot;namesAndAges.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>





<h3 id="3-1-2-在文件上直接运行-SQL"><a href="#3-1-2-在文件上直接运行-SQL" class="headerlink" title="3.1.2    在文件上直接运行 SQL"></a>3.1.2    在文件上直接运行 SQL</h3><p>我们前面都是使用 <code>read API</code> 先把文件加载到 <code>DataFrame</code>，然后再查询。其实，我们也可以直接在文件上进行查询。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from json.examples/src/main/resources/people.json&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>说明：<code>json</code> 表示文件的格式。后面的文件具体路径需要用反引号括起来。</p>
<h3 id="3-1-3-文件保存选项-SaveMode"><a href="#3-1-3-文件保存选项-SaveMode" class="headerlink" title="3.1.3    文件保存选项(SaveMode)"></a>3.1.3    文件保存选项(SaveMode)</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 方式一：</span></span><br><span class="line">df.write.format(<span class="string">&quot;文件格式&quot;</span>).mode(<span class="string">&quot;模式名称&quot;</span>).save(<span class="string">&quot;文件路径&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方式二：</span></span><br><span class="line">df.write.mode(<span class="string">&quot;模式名称&quot;</span>).文件格式(<span class="string">&quot;文件路径&quot;</span>)</span><br></pre></td></tr></table></figure>



<p>保存操作可以使用 <code>SaveMode</code>，用来指明如何处理数据。使用 <code>mode()</code> 方法来设置。</p>
<p>有一点很重要：这些 <code>SaveMode</code> 都是没有加锁的，也不是原子操作。还有，如果你执行的是 <code>Overwrite</code> 操作，在写入新的数据之前会先删除旧的数据。</p>
<table>
<thead>
<tr>
<th align="left">Scala/Java</th>
<th>Any Language</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>SaveMode.ErrorIfExists(default)</code></td>
<td><code>&quot;error&quot;(default)</code></td>
<td>如果文件已经存在则抛出异常</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Append</code></td>
<td><code>&quot;append&quot;</code></td>
<td>如果文件已经存在则追加</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Overwrite</code></td>
<td><code>&quot;overwrite&quot;</code></td>
<td>如果文件已经存在则覆盖</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Ignore</code></td>
<td><code>&quot;ignore&quot;</code></td>
<td>如果文件已经存在则忽略</td>
</tr>
</tbody></table>
<hr>
<h2 id="3-2-加载-JSON-文件"><a href="#3-2-加载-JSON-文件" class="headerlink" title="3.2    加载 JSON 文件"></a>3.2    加载 JSON 文件</h2><p><code>Spark SQL</code> 能够自动推测 <code>JSON</code> 数据集的结构，并将它加载为一个 <code>Dataset[Row]</code>。</p>
<p>可以通过 <code>SparkSession.read.json()</code> 去加载一个 <code>JSON</code> 文件。也可以通过 <code>SparkSession.read.format(&quot;json&quot;).load()</code> 来加载。</p>
<p>注意：这个 <code>JSON</code> 文件不是一个传统的 <code>JSON</code> 文件，每一行都必须是一个完整的 <code>JSON</code> 串。</p>
<p><strong>json 文件</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lisi&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span> <span class="punctuation">:</span> <span class="number">20</span><span class="punctuation">,</span> <span class="attr">&quot;friends&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;lisi&quot;</span><span class="punctuation">,</span> <span class="string">&quot;zs&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;zs&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span> <span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span> <span class="attr">&quot;friends&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;lisi&quot;</span><span class="punctuation">,</span> <span class="string">&quot;zs&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wangwu&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span> <span class="punctuation">:</span> <span class="number">15</span><span class="punctuation">,</span> <span class="attr">&quot;friends&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;lisi&quot;</span><span class="punctuation">,</span> <span class="string">&quot;zs&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p><strong>代码</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataSourceDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;target/classes/user.json&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User</span>] = df.as[<span class="type">User</span>]</span><br><span class="line">        ds.foreach(user =&gt; println(user.friends(<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age: <span class="type">Long</span>, friends: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span></span><br></pre></td></tr></table></figure>



<hr>
<h2 id="3-3-读取-Parquet-文件"><a href="#3-3-读取-Parquet-文件" class="headerlink" title="3.3    读取 Parquet 文件"></a>3.3    读取 Parquet 文件</h2><p><code>Parquet</code> 是一种流行的列式存储格式，可以高效地存储具有嵌套字段的记录。<code>Parquet</code> 格式经常在 <code>Hadoop</code> 生态圈中被使用，它也支持 <code>Spark SQL</code> 的全部数据类型。 <code>Spark SQL</code> 提供了直接读取和存储 <code>Parquet</code> 格式文件的方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataSourceDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> jsonDF: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;target/classes/user.json&quot;</span>)</span><br><span class="line">        jsonDF.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).parquet(<span class="string">&quot;target/classes/user.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> parDF: <span class="type">DataFrame</span> = spark.read.parquet(<span class="string">&quot;target/classes/user.parquet&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> userDS: <span class="type">Dataset</span>[<span class="type">User</span>] = parDF.as[<span class="type">User</span>]</span><br><span class="line">        userDS.map(user =&gt; &#123;user.name = <span class="string">&quot;zl&quot;</span>; user.friends(<span class="number">0</span>) = <span class="string">&quot;志玲&quot;</span>;user&#125;).show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">var name:<span class="type">String</span>, age: <span class="type">Long</span>, friends: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong> <code>Parquet</code> 格式的文件是 <code>Spark</code> 默认格式的数据源。所以，当使用通用的方式时可以直接保存和读取，而不需要使用 <code>format spark.sql.sources.default</code> 这个配置来修改默认数据源。</p>
<hr>
<h2 id="3-4-JDBC"><a href="#3-4-JDBC" class="headerlink" title="3.4    JDBC"></a>3.4    JDBC</h2><p><code>Spark SQL</code> 也支持使用 <code>JDBC</code> 从其它的数据库中读取数据。<code>JDBC</code> 数据源比使用 <code>JdbcRDD</code> 更爽一些，这是因为返回的结果直接就是一个 <code>DataFrame</code>，<code>DataFrame</code> 更加容易被处理或者与其它的数据源进行 <code>join</code>。</p>
<p><code>Spark SQL</code> 可以通过 <code>JDBC</code> 从关系型数据库中读取数据的方式创建 <code>DataFrame</code>，通过对 <code>DataFrame</code> 一系列的计算后，还可以将数据再写回关系型数据库中。</p>
<p>注意：如果想在 <code>spark-shell</code> 中操作 <code>jdbc</code>，需要把相关的 <code>jdbc</code> 驱动拷贝到 <code>jars</code> 目录下。</p>
<p><strong>导入依赖：</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.40<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="3-4-1-从-JDBC-读数据"><a href="#3-4-1-从-JDBC-读数据" class="headerlink" title="3.4.1    从 JDBC 读数据"></a>3.4.1    从 JDBC 读数据</h3><p>可以使用通用的 <code>load</code> 方法，也可以使用 <code>jdbc</code> 方法。</p>
<ol>
<li><p>使用通用的 <code>load</code> 方法加载</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 11:20</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">    <span class="keyword">val</span> user = <span class="string">&quot;root&quot;</span></span><br><span class="line">    <span class="keyword">val</span> password = <span class="string">&quot;123456&quot;</span></span><br><span class="line">    <span class="keyword">val</span> dbtable = <span class="string">&quot;users&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 通过 JDBC 读取数据到 DF 中</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">      .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;url&quot;</span>, url)</span><br><span class="line">      .option(<span class="string">&quot;user&quot;</span>, user)</span><br><span class="line">      .option(<span class="string">&quot;password&quot;</span>, password)</span><br><span class="line">      .option(<span class="string">&quot;dbtable&quot;</span>, dbtable)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
<li><p>专用方法加载数据</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 11:20</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCDemo2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">    <span class="keyword">val</span> user = <span class="string">&quot;root&quot;</span></span><br><span class="line">    <span class="keyword">val</span> password = <span class="string">&quot;123456&quot;</span></span><br><span class="line">    <span class="keyword">val</span> dbtable = <span class="string">&quot;users&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> properties: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.put(<span class="string">&quot;user&quot;</span>, user)</span><br><span class="line">    properties.put(<span class="string">&quot;password&quot;</span>, password)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过 JDBC 读取数据到 DF 中</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">      .jdbc(url, dbtable, properties)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="3-4-2-向-JDBC-写入数据"><a href="#3-4-2-向-JDBC-写入数据" class="headerlink" title="3.4.2    向 JDBC 写入数据"></a>3.4.2    向 JDBC 写入数据</h3><p>也分两种方法：通用 <code>write.save</code> 和 <code>write.jdbc</code></p>
<p><strong>通用写法</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 11:38</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCWriteDemo01</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">URL</span> = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">USER</span> = <span class="string">&quot;root&quot;</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">PASSWORD</span> = <span class="string">&quot;123456&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 1、先读取 json 文件中的数据</span></span><br><span class="line"><span class="comment">    * 2、再写入数据库中</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="comment">// 读</span></span><br><span class="line">    <span class="keyword">val</span> source: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 写。如果表不存在，会自动创建</span></span><br><span class="line">    source.write</span><br><span class="line">      .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;url&quot;</span>, <span class="type">URL</span>)</span><br><span class="line">      .option(<span class="string">&quot;user&quot;</span>, <span class="type">USER</span>)</span><br><span class="line">      .option(<span class="string">&quot;password&quot;</span>, <span class="type">PASSWORD</span>)</span><br><span class="line">      .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;new_users&quot;</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)	<span class="comment">// SaveMode 是一个枚举类</span></span><br><span class="line">      .save()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>  代码执行完毕后可在数据库中查询是否写数据成功</li>
</ul>
<p><strong>专用写法</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 11:38</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCWriteDemo02</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">URL</span> = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">USER</span> = <span class="string">&quot;root&quot;</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">PASSWORD</span> = <span class="string">&quot;123456&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 1、先读取 json 文件中的数据</span></span><br><span class="line"><span class="comment">    * 2、再写入数据库中</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="comment">// 读</span></span><br><span class="line">    <span class="keyword">val</span> source: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 写。如果表不存在，会自动创建</span></span><br><span class="line">    <span class="keyword">val</span> properties: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.put(<span class="string">&quot;user&quot;</span>, <span class="type">USER</span>)</span><br><span class="line">    properties.put(<span class="string">&quot;password&quot;</span>, <span class="type">PASSWORD</span>)</span><br><span class="line"></span><br><span class="line">    source.write</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">      .jdbc(<span class="type">URL</span>, table = <span class="string">&quot;new_users&quot;</span>, properties)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="3-5-Hive-数据库（重要）"><a href="#3-5-Hive-数据库（重要）" class="headerlink" title="3.5    Hive 数据库（重要）"></a>3.5    Hive 数据库（重要）</h2><p><code>Apache Hive</code> 是 <code>Hadoop</code> 上的 <code>SQL</code> 引擎，<code>Spark SQL</code> 编译时可以包含 <code>Hive</code> 支持，也可以不包含。</p>
<p>包含 <code>Hive</code> 支持的 <code>Spark SQL</code> 可以支持 <code>Hive</code> 表访问、 <code>UDF(用户自定义函数)</code> 以及 <code>Hive 查询语言(HiveQL/HQL)</code> 等。</p>
<p>需要强调的一点是，如果要在 <code>Spark SQL</code> 中包含 <code>Hive</code> 的库，并不需要事先安装 <code>Hive</code>。一般来说，最好还是在编译 <code>Spark SQL</code> 时引入 <code>Hive</code> 支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 <code>Spark</code>，它应该已经在编译时添加了 <code>Hive</code> 支持。</p>
<p>若要把 <code>Spark SQL</code> 连接到一个部署好的 <code>Hive</code> 上，你必须把 <code>hive-site.xml</code> 复制到 <code>Spark</code> 的配置文件目录中 <code>($SPARK_HOME/conf)</code>。即使没有部署好 <code>Hive</code>，<code>Spark SQL</code> 也可以运行。 </p>
<p>需要注意的是，如果你没有部署好 <code>Hive</code>，<code>Spark SQL</code> 会在当前的工作目录中创建出自己的 <code>Hive</code> 元数据仓库，叫作 <code>metastore_db</code>。此外，如果你尝试使用 <code>HiveQL</code> 中的 <code>CREATE TABLE</code>（并非 <code>CREATE EXTERNAL TABLE</code>）语句来创建表，这些表会被放在你默认的文件系统中的 <code>/user/hive/warehouse</code> 目录中（如果你的 <code>classpath</code> 中有配好的 <code>hdfs-site.xml</code>，默认的文件系统就是 <code>HDFS</code>，否则就是本地文件系统）。</p>
<hr>
<h3 id="3-5-1-使用内嵌的-Hive"><a href="#3-5-1-使用内嵌的-Hive" class="headerlink" title="3.5.1    使用内嵌的 Hive"></a>3.5.1    使用内嵌的 Hive</h3><p>如果使用 <code>Spark</code> 内嵌的 <code>Hive</code>，则什么都不用做，直接使用即可。</p>
<p><code>Hive</code> 的元数据存储在 <code>derby</code> 中，仓库地址：<code>$SPARK_HOME/spark-warehouse</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;create table aa(id int)&quot;</span>)</span><br><span class="line"><span class="number">19</span>/<span class="number">02</span>/<span class="number">09</span> <span class="number">18</span>:<span class="number">36</span>:<span class="number">10</span> <span class="type">WARN</span> <span class="type">HiveMetaStore</span>: <span class="type">Location</span>: file:/opt/module/spark-local/spark-warehouse/aa specified <span class="keyword">for</span> non-external table:aa</span><br><span class="line">res2: org.apache.spark.sql.<span class="type">DataFrame</span> = []</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| <span class="keyword">default</span>|       aa|      <span class="literal">false</span>|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line"><span class="comment">// 向表中加载本地数据数据</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;load data local inpath &#x27;./ids.txt&#x27; into table aa&quot;</span>)</span><br><span class="line">res8: org.apache.spark.sql.<span class="type">DataFrame</span> = []</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from aa&quot;</span>).show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|<span class="number">100</span>|</span><br><span class="line">|<span class="number">101</span>|</span><br><span class="line">|<span class="number">102</span>|</span><br><span class="line">|<span class="number">103</span>|</span><br><span class="line">|<span class="number">104</span>|</span><br><span class="line">|<span class="number">105</span>|</span><br><span class="line">|<span class="number">106</span>|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>

<p>然而在实际使用中，几乎没有任何人会使用内置的 <code>Hive</code>。</p>
<hr>
<h3 id="3-5-2-Spark集成外置Hive"><a href="#3-5-2-Spark集成外置Hive" class="headerlink" title="3.5.2 Spark集成外置Hive"></a>3.5.2 Spark集成外置Hive</h3><p><code>Spark</code> 和 <code>Hive</code> 集成的两种方式：</p>
<ol>
<li><code>Spark on Hive</code><ul>
<li>  只需要让 SparkSQL 找到 Hive 的元数据就 OK 了。</li>
</ul>
</li>
<li><code>Hive on Spark</code><ul>
<li>  了解</li>
</ul>
</li>
</ol>
<h4 id="3-5-2-1-集成步骤"><a href="#3-5-2-1-集成步骤" class="headerlink" title="3.5.2.1    集成步骤"></a>3.5.2.1    集成步骤</h4><ol>
<li>   <code>Spark</code> 要接管 <code>Hive</code> 需要把 <code>hive-site.xml</code> 拷贝到 Spark 的配置目录 <code>conf/</code> 下</li>
<li>   由于 <code>Hive</code> 的元数据保存在 <code>MySQL</code> 中，如果 <code>Spark</code> 想要访问元数据的话必须作为 <code>MySQL</code> 的客户端，所以还需要把 <code>Mysql</code> 的驱动拷贝到 Spark  的 <code>jars/</code> 目录下</li>
<li>   如果访问不到 <code>HDFS</code>，则还需要把 <code>core-site.xml</code> 和 <code>hdfs-site.xml</code> 拷贝到当前项目的 resource 目录下【可选】</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把 hive-site.xml 拷贝到 Spark 的配置目录 conf/ 下</span></span><br><span class="line">[lvnengdong@hadoop102 hive]$ <span class="built_in">cp</span> /opt/module/hive/conf/hive-site.xml /opt/module/spark-local/conf/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 Mysql 的驱动拷贝到 Spark  的 jars/ 目录下</span></span><br><span class="line">[lvnengdong@hadoop102 /]$ <span class="built_in">cp</span> /opt/software/mysql/mysql-connector-java-5.1.27/mysql-connector-java-5.1.27-bin.jar /opt/module/spark-local/jars</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/ 目录下（可选）</span></span><br></pre></td></tr></table></figure>



<h4 id="3-5-2-2-启动-spark-shell"><a href="#3-5-2-2-启动-spark-shell" class="headerlink" title="3.5.2.2    启动 spark-shell"></a>3.5.2.2    启动 spark-shell</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| <span class="keyword">default</span>|      emp|      <span class="literal">false</span>|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from emp&quot;</span>).show</span><br><span class="line"><span class="number">19</span>/<span class="number">02</span>/<span class="number">09</span> <span class="number">19</span>:<span class="number">40</span>:<span class="number">28</span> <span class="type">WARN</span> <span class="type">LazyStruct</span>: <span class="type">Extra</span> bytes detected at the end of the row! <span class="type">Ignoring</span> similar problems.</span><br><span class="line">+-----+-------+---------+----+----------+------+------+------+</span><br><span class="line">|empno|  ename|      job| mgr|  hiredate|   sal|  comm|deptno|</span><br><span class="line">+-----+-------+---------+----+----------+------+------+------+</span><br><span class="line">| <span class="number">7369</span>|  <span class="type">SMITH</span>|    <span class="type">CLERK</span>|<span class="number">7902</span>|<span class="number">1980</span><span class="number">-12</span><span class="number">-17</span>| <span class="number">800.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7499</span>|  <span class="type">ALLEN</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-2</span><span class="number">-20</span>|<span class="number">1600.0</span>| <span class="number">300.0</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7521</span>|   <span class="type">WARD</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-2</span><span class="number">-22</span>|<span class="number">1250.0</span>| <span class="number">500.0</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7566</span>|  <span class="type">JONES</span>|  <span class="type">MANAGER</span>|<span class="number">7839</span>|  <span class="number">1981</span><span class="number">-4</span><span class="number">-2</span>|<span class="number">2975.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7654</span>| <span class="type">MARTIN</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-9</span><span class="number">-28</span>|<span class="number">1250.0</span>|<span class="number">1400.0</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7698</span>|  <span class="type">BLAKE</span>|  <span class="type">MANAGER</span>|<span class="number">7839</span>|  <span class="number">1981</span><span class="number">-5</span><span class="number">-1</span>|<span class="number">2850.0</span>|  <span class="literal">null</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7782</span>|  <span class="type">CLARK</span>|  <span class="type">MANAGER</span>|<span class="number">7839</span>|  <span class="number">1981</span><span class="number">-6</span><span class="number">-9</span>|<span class="number">2450.0</span>|  <span class="literal">null</span>|    <span class="number">10</span>|</span><br><span class="line">| <span class="number">7788</span>|  <span class="type">SCOTT</span>|  <span class="type">ANALYST</span>|<span class="number">7566</span>| <span class="number">1987</span><span class="number">-4</span><span class="number">-19</span>|<span class="number">3000.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7839</span>|   <span class="type">KING</span>|<span class="type">PRESIDENT</span>|<span class="literal">null</span>|<span class="number">1981</span><span class="number">-11</span><span class="number">-17</span>|<span class="number">5000.0</span>|  <span class="literal">null</span>|    <span class="number">10</span>|</span><br><span class="line">| <span class="number">7844</span>| <span class="type">TURNER</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>|  <span class="number">1981</span><span class="number">-9</span><span class="number">-8</span>|<span class="number">1500.0</span>|   <span class="number">0.0</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7876</span>|  <span class="type">ADAMS</span>|    <span class="type">CLERK</span>|<span class="number">7788</span>| <span class="number">1987</span><span class="number">-5</span><span class="number">-23</span>|<span class="number">1100.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7900</span>|  <span class="type">JAMES</span>|    <span class="type">CLERK</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>| <span class="number">950.0</span>|  <span class="literal">null</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7902</span>|   <span class="type">FORD</span>|  <span class="type">ANALYST</span>|<span class="number">7566</span>| <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>|<span class="number">3000.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7934</span>| <span class="type">MILLER</span>|    <span class="type">CLERK</span>|<span class="number">7782</span>| <span class="number">1982</span><span class="number">-1</span><span class="number">-23</span>|<span class="number">1300.0</span>|  <span class="literal">null</span>|    <span class="number">10</span>|</span><br><span class="line">| <span class="number">7944</span>|zhiling|    <span class="type">CLERK</span>|<span class="number">7782</span>| <span class="number">1982</span><span class="number">-1</span><span class="number">-23</span>|<span class="number">1300.0</span>|  <span class="literal">null</span>|    <span class="number">50</span>|</span><br><span class="line">+-----+-------+---------+----+----------+------+------+------+</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="3-5-2-3-使用-spark-sql-客户端"><a href="#3-5-2-3-使用-spark-sql-客户端" class="headerlink" title="3.5.2.3    使用 spark-sql 客户端"></a>3.5.2.3    使用 spark-sql 客户端</h4><p>在 <code>spark-shell</code> 中执行 <code>Hive</code> 方面的查询比较麻烦。格式为：<code>spark.sql(&quot;SQL语句&quot;).show</code></p>
<p><code>Spark</code> 专门给我们提供了书写 <code>HiveQL</code> 的工具： <code>spark-sql cli</code></p>
<ol>
<li><p>启动 <code>spark-sql</code> 客户端：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-local]$ bin/spark-sql</span><br></pre></td></tr></table></figure></li>
<li><p>在 <code>spark-sql</code> 中执行 SQL 语句</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark<span class="operator">-</span><span class="keyword">sql</span> (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> movie_info;</span><br><span class="line">movie	category</span><br><span class="line">《疑犯追踪》	[&quot;悬疑&quot;,&quot;动作&quot;,&quot;科幻&quot;,&quot;剧情&quot;]</span><br><span class="line">《Lie <span class="keyword">to</span> me》	[&quot;悬疑&quot;,&quot;警匪&quot;,&quot;动作&quot;,&quot;心理&quot;,&quot;剧情&quot;]</span><br><span class="line">《战狼<span class="number">2</span>》	[&quot;战争&quot;,&quot;动作&quot;,&quot;灾难&quot;]</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.395</span> seconds, Fetched <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure></li>
<li><p>退出 <code>spark-sql</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-sql (default)&gt; quit;</span><br></pre></td></tr></table></figure>

</li>
<li><p> <code>spark-sql</code> 客户端一般用于测试，生产环境中不常用</p>
</li>
</ol>
<hr>
<h4 id="3-5-2-4-使用-hiveserver2-beeline"><a href="#3-5-2-4-使用-hiveserver2-beeline" class="headerlink" title="3.5.2.4    使用 hiveserver2 + beeline"></a>3.5.2.4    使用 <code>hiveserver2 + beeline</code></h4><p><code>spark-sql</code> 得到的结果不够友好，所以可以使用 <code>hiveserver2 + beeline</code></p>
<ol>
<li><p>启动 <code>thrift</code> 服务器</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以 local 模式启动 thriftserver 服务</span></span><br><span class="line">[lvnengdong@hadoop102 spark-local]$ sbin/start-thriftserver.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 yarn 模式启动 thriftserver 服务 </span></span><br><span class="line">sbin/start-thriftserver.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--hiveconf hive.server2.thrift.bind.host=hadoop102 \</span><br><span class="line">-–hiveconf hive.server2.thrift.port=10000 \</span><br></pre></td></tr></table></figure>

</li>
<li><p>启动 <code>beeline</code> 客户端</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">    [lvnengdong@hadoop102 spark-local]$ bin/beeline</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 然后输入</span></span><br><span class="line">    !connect jdbc:hive2://hadoop102:10000</span><br><span class="line">    <span class="comment"># 然后按照提示输入用户名和密码</span></span><br><span class="line">    admin</span><br><span class="line">123456</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看</p>
 <img src="/2021/12/11/Spark-SQL/image-20211225162805804.png" alt="image-20211225162805804" style="zoom: 67%;"></li>
</ol>
<hr>
<h3 id="3-5-3-在代码中访问-Hive"><a href="#3-5-3-在代码中访问-Hive" class="headerlink" title="3.5.3    在代码中访问 Hive"></a>3.5.3    在代码中访问 Hive</h3><h4 id="步骤1-拷贝-hive-site-xml-到-resources-目录下"><a href="#步骤1-拷贝-hive-site-xml-到-resources-目录下" class="headerlink" title="步骤1: 拷贝 hive-site.xml 到 resources 目录下"></a>步骤1: 拷贝 <code>hive-site.xml</code> 到 resources 目录下</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Hive连接哪个库来查找元数据--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 default 数据库的默认位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="步骤2-添加依赖"><a href="#步骤2-添加依赖" class="headerlink" title="步骤2: 添加依赖"></a>步骤2: 添加依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 添加 Spark 支持 Hive 的 jar 包--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="步骤3-代码"><a href="#步骤3-代码" class="headerlink" title="步骤3: 代码"></a>步骤3: 代码</h4><h5 id="从-Hive-中读数据"><a href="#从-Hive-中读数据" class="headerlink" title="从 Hive 中读数据"></a>从 Hive 中读数据</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 17:00</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HiveRead</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 如果 HDFS 提示权限不足，则可以修改文件属主</span></span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;lvnengdong&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .enableHiveSupport()  <span class="comment">// 添加支持外置 Hive，如果不设置默认使用 Spark 内置的 Hive</span></span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">import</span> spark.sql</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行 SQL</span></span><br><span class="line">    sql(<span class="string">&quot;show databases&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h5 id="向-Hive-中写数据"><a href="#向-Hive-中写数据" class="headerlink" title="向 Hive 中写数据"></a>向 Hive 中写数据</h5><ol>
<li> 使用 Hive 的 <code>insert</code> 语句</li>
<li> <code>df.saveAsTable(&quot;表名&quot;)</code>：将 DF 中的数据直接写出到 HIve 表中。使用列名分配 value</li>
<li> <code>df.write.insertInto(&quot;表名&quot;)</code>：基本等价于方式二的 <code>append</code> 模式，但是插入数据时要求对应的 Hive 表必须提前存在。使用位置分配 value</li>
</ol>
<h6 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h6><p><strong>注意：</strong></p>
<p>在 Spark 中创建数据库，默认情况下，元数据信息保存在 MySQL 中，而数据则会保存在本地。</p>
<p>如果想要创建的数据库数据保存在 HDFS 上，主要有两种解决方案：</p>
<ol>
<li><p>在创建数据库时，通过参数修改数据库仓库的地址</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://hadoop201:9000/user/hive/warehouse&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p> 不要使用 Spark 创建数据库，而是使用 Hive 去创建数据库</p>
</li>
</ol>
<p><strong>Demo</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 19:10</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HiveWrite</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      <span class="comment">// 显式配置数据仓库地址</span></span><br><span class="line">      .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://hadoop102:9000/spark_hive/warehouse&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、创建数据库</span></span><br><span class="line">    spark.sql(<span class="string">&quot;create database spark_hive01&quot;</span>).show()</span><br><span class="line">    <span class="comment">// 2、创建表</span></span><br><span class="line">    spark.sql(<span class="string">&quot;use spark_hive01&quot;</span>).show()</span><br><span class="line">    spark.sql(<span class="string">&quot;create table user_info(id int, name string)&quot;</span>).show()</span><br><span class="line">    <span class="comment">// 3、插入数据</span></span><br><span class="line">    spark.sql(<span class="string">&quot;insert into user_info values(10, &#x27;zs&#x27;)&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>  插入的数据保存在 HDFS 上</li>
</ul>
<p><img src="/2021/12/11/Spark-SQL/image-20211225195825371.png" alt="image-20211225195825371"></p>
<p><strong>Demo2</strong></p>
<p>把读取到的数据写入到 Hive 中，表可以存在也可以不存在</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 19:10</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HiveWrite2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      <span class="comment">// 显式配置数据仓库地址，默认保存在本地文件系统上</span></span><br><span class="line">      .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://hadoop102:9000/spark_hive/warehouse&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、读取数据</span></span><br><span class="line">    <span class="keyword">val</span> source: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、写出数据到Hive表中（若表不存在则自动创建）</span></span><br><span class="line">    spark.sql(<span class="string">&quot;use spark_hive01&quot;</span>)</span><br><span class="line">    source.write.mode(<span class="string">&quot;append&quot;</span>).saveAsTable(<span class="string">&quot;user_info2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 方式二：将 df 数据写出到 Hive 表中（若表不存在则抛出异常）</span></span><br><span class="line"><span class="comment">//    source.write.insertInto(&quot;user_info2&quot;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<ul>
<li><p>聚合后，默认分区数为 200。如果想要调整分区数，可以使用 <code>coalesce</code> 函数</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 调整分区数量</span></span><br><span class="line">df.coalesce(<span class="number">1</span>).write.saveAsTable(<span class="string">&quot;表名&quot;</span>)	</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/11/Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/11/Spark/" class="post-title-link" itemprop="url">Spark 基础</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-11 20:42:32" itemprop="dateCreated datePublished" datetime="2021-12-11T20:42:32+08:00">2021-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-01-16 13:05:22" itemprop="dateModified" datetime="2022-01-16T13:05:22+08:00">2022-01-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>  start</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">大数据</span><br><span class="line">1、数据传输</span><br><span class="line">	Flume	实时</span><br><span class="line">	Sqoop	离线</span><br><span class="line"></span><br><span class="line">2、数据计算</span><br><span class="line">	MapReduce</span><br><span class="line">	Spark：比MR快100倍</span><br><span class="line">	</span><br><span class="line">3、数据存储</span><br><span class="line">	HDFS</span><br></pre></td></tr></table></figure>



<hr>
<h1 id="第-1-章-Spark-内置模块介绍"><a href="#第-1-章-Spark-内置模块介绍" class="headerlink" title="第 1 章 Spark 内置模块介绍"></a>第 1 章 Spark 内置模块介绍</h1><p><img src="/2021/12/11/Spark/image-20211211214657898.png" alt="image-20211211214657898"></p>
<h2 id="1-Cluster-Manager"><a href="#1-Cluster-Manager" class="headerlink" title="1    Cluster Manager"></a>1    Cluster Manager</h2><blockquote>
<p>  <strong>Cluster Manager；集群管理器；资源调度器</strong></p>
</blockquote>
<p>主要负责调度管理程序运算时所需的软、硬件资源，如 CPU、内存等。有多种实现方式，常见的落地实现有：</p>
<ol>
<li> <code>Standalone</code>：Spark 内置的资源调度器，需要在集群中的每台节点上安装 Spark</li>
<li> <code>Hadoop YARN</code>：使用 Hadoop 的 Yarn 管理计算资源，在国内使用最广泛</li>
<li> <code>Apache Mesos</code>：国内使用较少, 国外使用较多</li>
</ol>
<p>Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。也就是说 Spark 可以在一个节点上计算，也可以利用上千个节点进行运算，并且这种转换是非常容易实现的。为了实现这样的要求，同时获得最大灵活性，Spark 支持在各种集群管理器(Cluster Manager)上运行。</p>
<h2 id="2-SparkCore"><a href="#2-SparkCore" class="headerlink" title="2    SparkCore"></a>2    SparkCore</h2><p>实现了 Spark 的基本功能，包括任务调度、内存管理、错误恢复、与存储系统交互等模块。SparkCore 中还包含了对弹性分布式数据集 <code>RDD(Resilient Distributed DataSet)</code> 的 API 定义。</p>
<h2 id="3-Spark-SQL"><a href="#3-Spark-SQL" class="headerlink" title="3    Spark SQL"></a>3    Spark SQL</h2><p>是 Spark 用来操作结构化数据的程序包。通过 SparkSQL，我们可以使用 SQL 或者Apache Hive 版本的 SQL 方言(HQL)来查询数据。Spark SQL 支持多种数据源，比如 Hive 表、Parquet 以及 JSON 等。</p>
<p>就是将 Hive 底层的执行引擎由 MapReduce 换成了 Spark。</p>
<h2 id="4-Spark-Streaming"><a href="#4-Spark-Streaming" class="headerlink" title="4  Spark Streaming"></a>4  Spark Streaming</h2><p>是 Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API，并且与 Spark Core 中的 RDD API 高度对应。</p>
<h2 id="5-Spark-MLlib"><a href="#5-Spark-MLlib" class="headerlink" title="5  Spark MLlib"></a>5  Spark MLlib</h2><p>提供常见的机器学习 (ML) 功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据导入等额外的支持功能。</p>
<hr>
<h1 id="第-2-章-Spark-运行模式"><a href="#第-2-章-Spark-运行模式" class="headerlink" title="第 2 章 Spark 运行模式"></a>第 2 章 Spark 运行模式</h1><p>本章介绍在各种运行模式如何运行 Spark 应用.</p>
<p>首先需要下载 Spark</p>
<p>1．官网地址 <a target="_blank" rel="noopener" href="http://spark.apache.org/">http://spark.apache.org/</a></p>
<p>2．文档查看地址 <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.1.1/">https://spark.apache.org/docs/2.1.1/</a></p>
<p>3．下载地址 <a target="_blank" rel="noopener" href="https://archive.apache.org/dist/spark/">https://archive.apache.org/dist/spark/</a></p>
<h2 id="2-1-Local-模式"><a href="#2-1-Local-模式" class="headerlink" title="2.1 Local 模式"></a>2.1 Local 模式</h2><p>Local 模式就是在一台计算机上利用多线程模拟多台服务器来运行 Spark。</p>
<h3 id="2-1-1-解压-Spark-安装包"><a href="#2-1-1-解压-Spark-安装包" class="headerlink" title="2.1.1    解压 Spark 安装包"></a>2.1.1    解压 Spark 安装包</h3><ol>
<li><p>把安装包上传到 <code>/opt/software/</code> 下，并解压到 <code>/opt/module/</code> 目录下</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 software]$ tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module</span><br></pre></td></tr></table></figure></li>
<li><p>复制一份刚刚解压得到的目录，并重命名为 <code>spark-local</code>。【这一步操作的意义在于测试多种运行模式时可以独立测试】</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 module]$ <span class="built_in">cp</span> -r spark-2.1.1-bin-hadoop2.7 spark-local</span><br></pre></td></tr></table></figure></li>
<li><p>查看 spark 的安装目录</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-2.1.1-bin-hadoop2.7]$ ll</span><br><span class="line">总用量 84</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong  4096 4月  26 2017 bin	<span class="comment"># 保存可执行的二进制脚本文件的目录</span></span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong   230 4月  26 2017 conf	<span class="comment"># 配置文件目录</span></span><br><span class="line">drwxr-xr-x. 5 lvnengdong lvnengdong    50 4月  26 2017 data	<span class="comment"># 提供一些用于测试的数据集</span></span><br><span class="line">drwxr-xr-x. 4 lvnengdong lvnengdong    29 4月  26 2017 examples	<span class="comment"># 提供一些写好的测试案例</span></span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong 12288 4月  26 2017 jars	<span class="comment"># 项目依赖的jar包</span></span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong 17811 4月  26 2017 LICENSE</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong  4096 4月  26 2017 licenses</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong 24645 4月  26 2017 NOTICE</span><br><span class="line">drwxr-xr-x. 8 lvnengdong lvnengdong   240 4月  26 2017 python	<span class="comment"># python调用相关</span></span><br><span class="line">drwxr-xr-x. 3 lvnengdong lvnengdong    17 4月  26 2017 R		<span class="comment"># R调用相关</span></span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong  3817 4月  26 2017 README.md</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   128 4月  26 2017 RELEASE</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong  4096 4月  26 2017 sbin	<span class="comment"># 群起集群时使用的一些命令</span></span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong    42 4月  26 2017 yarn	<span class="comment"># 整合Yarn相关</span></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="2-1-2-运行官方求PI的案例"><a href="#2-1-2-运行官方求PI的案例" class="headerlink" title="2.1.2 运行官方求PI的案例"></a>2.1.2 运行官方求PI的案例</h3><p><strong>命令：</strong><code>spark-submit</code>（含义：<code>spark</code> 客户端将任务提交给 <code>Cluster Manager</code> 去执行）  </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-2.1.1-bin-hadoop2.7]$ bin/spark-submit \</span><br><span class="line">&gt; --class org.apache.spark.examples.SparkPi \</span><br><span class="line">&gt; --master <span class="built_in">local</span>[2] \</span><br><span class="line">&gt; ./examples/jars/spark-examples_2.11-2.1.1.jar 100</span><br></pre></td></tr></table></figure>

<p><strong>分析：</strong>运行 <code>spark-examples_2.11-2.1.1.jar</code> 程序，程序中的主类名是 <code>org.apache.spark.examples.SparkPi</code>。</p>
<p><strong>结果展示：</strong></p>
<p><img src="/2021/12/11/Spark/image-20211211222524138.png" alt="image-20211211222524138"></p>
<p><strong>语法</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line"> --class &lt;main-class&gt; \				<span class="comment"># 参数1：应用程序的主类名（全限定类名）</span></span><br><span class="line"> --master &lt;master-url&gt; \			<span class="comment"># 参数2：Cluster Manager 的地址</span></span><br><span class="line"> --deploy-mode &lt;deploy-mode&gt; \		<span class="comment">#  </span></span><br><span class="line"> --conf &lt;key&gt;=&lt;value&gt; \				<span class="comment"># 显式指定配置信息</span></span><br><span class="line"> ... <span class="comment"># other options</span></span><br><span class="line"> &lt;application-jar&gt; \</span><br><span class="line"> [application-arguments]</span><br></pre></td></tr></table></figure>

<ul>
<li>  <code>--master</code>：<code>master</code> 是真正执行程序的 <code>Cluster Manager</code> 的地址。默认为 <code>local</code>，表示在本地机器上运行。</li>
<li>  <code>--class</code>：待执行的应用程序会被打成一个 jar 包，该参数用于指定 jar 包的启动类（如 <code>org.apache.spark.examples.SparkPi</code>）。</li>
<li>  <code>--deploy-mode</code>：是否发布你的驱动到 worker 节点(cluster 模式) 或者作为一个本地客户端 (client 模式) (default: client)</li>
<li>  <code>--conf</code>：在程序运行前显式指定配置信息。格式 <code>key=value</code>，如果值包含空格，可以加引号 <code>&quot;key=value&quot;</code>。</li>
<li>  <code>application-jar</code>：待执行任务打包成的 jar，包含依赖。这个 <code>URL</code> 需要在集群中全局可见。比如 <code>hdfs:// 共享存储系统</code>，如果是  <code>file:// path</code>，那么需要保证所有的节点的 <code>path</code> 下都包含相同的 jar。</li>
<li>  <code>application-arguments</code>：传给主类中启动方法 <code>main()</code> 的参数</li>
<li>  <code>--executor-memory:1G</code>：指定每个 <code>executor</code> 可用内存为 1G</li>
<li>  <code>--total-executor-cores:6</code>：指定所有 <code>executor</code> 使用的 cpu 核数为 6 个</li>
<li>   <code>--executor-cores</code>：表示每个 executor 使用的 cpu 的核数</li>
</ul>
<p><strong>关于 Master URL 的说明</strong></p>
<p><code>Master URL</code> 就是 Master 节点的 IP 地址和端口号。</p>
<table>
<thead>
<tr>
<th>Master URL</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td><code>local</code></td>
<td>用一个线程在本地运行 Spark（即完全没有并行性）</td>
</tr>
<tr>
<td><code>local[K]</code></td>
<td>使用 K 个线程在本地运行 Spark</td>
</tr>
<tr>
<td><code>local[*]</code></td>
<td>在本地运行 Spark，使用的线程数量为当前机器可以提供的最大线程数</td>
</tr>
<tr>
<td><code>spark://HOST:PORT</code></td>
<td>Connect to the  given <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.1.1/spark-standalone.html">Spark   standalone cluster</a> master. The port must be whichever one your master is  configured to use, which is 7077 by default.</td>
</tr>
<tr>
<td><code>mesos://HOST:PORT</code></td>
<td>Connect to the  given <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.1.1/running-on-mesos.html">Mesos</a>  cluster. The port must be whichever one your is configured to use, which is  5050 by default. Or, for a Mesos cluster using ZooKeeper, use mesos://zk://…. To submit with –deploy-mode cluster, the HOST:PORT  should be configured to connect to the <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.1.1/running-on-mesos.html#cluster-mode">MesosClusterDispatcher</a>.</td>
</tr>
<tr>
<td><code>yarn</code></td>
<td>Connect to a <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.1.1/running-on-yarn.html">YARN</a>cluster  in client or cluster mode depending on the value of  –deploy-mode. The  cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable.</td>
</tr>
</tbody></table>
<hr>
<h3 id="2-1-3-使用-spark-shell"><a href="#2-1-3-使用-spark-shell" class="headerlink" title="2.1.3    使用 spark-shell"></a>2.1.3    使用 spark-shell</h3><p><code>spark-shell</code> 是 Spark 提供的一个交互式命令窗口形式的客户端。本案例将会使用 <code>spark-shell</code> 统计文件中各个单词的数量.</p>
<ol>
<li><p> 创建 2 个文本文件<code>a.txt</code> 和 <code>b.txt</code>，分别在两个文件内输入一些单词。</p>
</li>
<li><p>打开 <code>spark-shell</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-2.1.1-bin-hadoop2.7]$ bin/spark-shell --master <span class="built_in">local</span>[2]	</span><br><span class="line"><span class="comment"># --master local[2]，该参数可加可不加，若不加默认启动就是local模式</span></span><br></pre></td></tr></table></figure></li>
<li><p>启动成功页面。</p>
<ul>
<li>  <code>spark-shell</code> 启动成功后，我们可以看到 <code>shell</code> 客户端自动帮我们创建了一个 <code>Spark Context</code> 对象（Spark 上下文对象）并将其命名为 <code>sc</code>，我们在 shell 客户端中调用 Spark 就需要使用这个对象来实现。</li>
<li>  并且提供了 UI 界面：<code>http://192.168.1.102:4040</code></li>
</ul>
<p> <img src="/2021/12/11/Spark/image-20211212102939164.png" alt="image-20211212102939164"></p>
<ul>
<li>  UI 界面</li>
</ul>
<p> <img src="/2021/12/11/Spark/image-20211212103539619.png" alt="image-20211212103539619"></p>
</li>
<li><p>退出 <code>spark-shell</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; :q</span><br></pre></td></tr></table></figure></li>
<li><p><strong>运行 <code>wordcount</code> 程序</strong></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取 `input/` 目录下的所有文件到 RDD 对象中</span></span><br><span class="line">scala&gt; sc.textFile(<span class="string">&quot;input/&quot;</span>)</span><br><span class="line">res0: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = input/ <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// RDD是一个集合，其中保存数据的基本单位是行（line），这一步是将每行的单词按空格分开</span></span><br><span class="line">scala&gt; .flatMap(e = e.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">res1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将每个单词映射为一个元组，key为单词本身，value为1，即每个单词出现的个数</span></span><br><span class="line">scala&gt; .map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line">res2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">29</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 按照key相同的进行聚合（key不变，value相加）</span></span><br><span class="line">scala&gt; .reduceByKey(_+_)</span><br><span class="line">res3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">4</span>] at reduceByKey at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 结果收集</span></span><br><span class="line">scala&gt; .collect</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((resource,<span class="number">2</span>), (created,<span class="number">2</span>), (<span class="keyword">this</span>,<span class="number">2</span>), (<span class="class"><span class="keyword">class</span>,,2), (<span class="params">load,2</span>), (<span class="params">is,4</span>), (<span class="params"><span class="type">Building</span>,2</span>), (<span class="params">can,4</span>), (<span class="params">file,,2</span>), (<span class="params">build,2</span>), (<span class="params">configuration,,2</span>), ...</span></span><br></pre></td></tr></table></figure></li>
<li><p><code>wordcount</code> 数据流程分析</p>
<p> <img src="/2021/12/11/Spark/image-20211212110617449.png" alt="image-20211212110617449"></p>
<ol>
<li> <code>textFile(&quot;input&quot;)</code>：读取本地文件input文件夹数据；</li>
<li> <code>flatMap(_.split(&quot; &quot;))</code>：先 <code>map</code> 再 <code>flatten</code>，按照空格分割符将一行数据映射成一个个单词；</li>
<li> <code>map((_,1))</code>：对每一个元素操作，将单词映射为元组；</li>
<li> <code>reduceByKey(_+_)</code>：将 key 相同的值进行聚合，相加；</li>
<li> <code>collect</code>：将数据收集到Driver端展示。</li>
</ol>
</li>
</ol>
<hr>
<h2 id="2-2-Spark-核心概念介绍"><a href="#2-2-Spark-核心概念介绍" class="headerlink" title="2.2    Spark 核心概念介绍"></a>2.2    Spark 核心概念介绍</h2><h3 id="2-2-1-Master"><a href="#2-2-1-Master" class="headerlink" title="2.2.1    Master"></a>2.2.1    Master</h3><p><code>Master</code> 其实就是上文提到过的 <code>Cluster Manager</code>，负责管理调度整个集群的软、硬件资源。如果 Spark 接入了 Hadoop Yarn，那么 Yarn 中的 <code>ResourceManager</code> 就是一个 <code>Master</code>。</p>
<p>主要功能：</p>
<ol>
<li>   接收 <code>Worker</code> 的注册并管理集群中所有的 <code>Worker</code>；</li>
<li>   接收 <code>Client</code> 提交的 <code>Application</code>，调度等待的 <code>Application</code> 并向 <code>Worker</code> 分配任务。</li>
<li>   管理 Worker、Application 等。</li>
</ol>
<blockquote>
<p>  Master 是一种角色，负责集群中的资源分配与调度，可以有多种实现，在 Spark 的 Yarn 模式下，Master 就是 ResourceManager。在 Spark 的 standalone 模式下，Master 是由 Spark 自己实现的 </p>
</blockquote>
<h3 id="2-2-2-Worker"><a href="#2-2-2-Worker" class="headerlink" title="2.2.2    Worker"></a>2.2.2    Worker</h3><p>Spark 资源调度系统的 Slave，有多个。每个 Slave 掌管着当前节点的资源信息，类似于 Yarn 框架中的 NodeManager，主要功能：</p>
<ol>
<li>   通过 RegisterWorker 注册到 Master；</li>
<li>   定时发送心跳给 Master；</li>
<li>   根据 Master 发送的 Application 配置进程环境，并启动 ExecutorBackend（执行 Task 所需的临时进程）</li>
</ol>
<hr>
<h3 id="2-2-3-Driver-Program"><a href="#2-2-3-Driver-Program" class="headerlink" title="2.2.3 Driver Program"></a>2.2.3 Driver Program</h3><blockquote>
<p>  <strong>Driver Program；驱动程序</strong></p>
</blockquote>
<p><strong>每个</strong> Spark 应用程序都包含一个驱动程序，驱动程序负责把并行操作（并行Task）发布到集群上。</p>
<p>驱动程序包含 Spark 应用程序中的主函数，定义了分布式数据集以应用在集群中。</p>
<p>在前面的 wordcount 案例集中，spark-shell 就是我们的驱动程序，所以我们可以在其中键入我们任何想要的操作，然后由他负责发布。</p>
<p>驱动程序通过 SparkContext 对象来访问 Spark，SparkContext 对象相当于一个到 Spark 集群的连接。</p>
<p>在 spark-shell 中, 会自动创建一个 SparkContext 对象，并把这个对象命名为 sc。</p>
<hr>
<h3 id="2-2-4-Executor"><a href="#2-2-4-Executor" class="headerlink" title="2.2.4  Executor"></a>2.2.4  Executor</h3><blockquote>
<p>  <strong>Executor；执行器</strong></p>
</blockquote>
<p><code>executor</code> 是一个运行在 <code>Worker</code> 节点上的用于执行具体任务的线程。</p>
<p>SparkContext 对象一旦成功连接到集群管理器（Master），就可以获取到集群中每个节点上的执行器（Executor）。</p>
<p>执行器是一个进程（进程名：ExecutorBackend，运行在 Worker 节点上），用来执行计算和为应用程序存储数据。（一个 Worker上会启动多个 Executor）</p>
<p>然后，Spark 会发送应用程序代码（比如:jar包）到每个执行器。最后，SparkContext 对象发送任务到执行器开始执行程序。</p>
<p><img src="/2021/12/11/Spark/image-20211212111132277.png" alt="image-20211212111132277"></p>
<hr>
<h3 id="1-2-4-RDDs"><a href="#1-2-4-RDDs" class="headerlink" title="1.2.4  RDDs"></a>1.2.4  RDDs</h3><blockquote>
<p>  <strong>Resilient Distributed Dataset；弹性分布式数据集</strong></p>
</blockquote>
<p>一旦拥有了 SparkContext 对象，就可以使用它来创建 RDD 了。在前面的例子中，我们调用 <code>sc.textFile(...)</code> 来创建了一个 RDD，表示文件中的每一行文本，我们可以对这些文本行运行各种各样的操作。</p>
<p>在第二部分的 SparkCore 中，我们重点就是学习 RDD。</p>
<hr>
<h3 id="1-2-5-Cluster-Managers"><a href="#1-2-5-Cluster-Managers" class="headerlink" title="1.2.5  Cluster Managers"></a>1.2.5  Cluster Managers</h3><blockquote>
<p>  <strong>Cluster Managers；集群管理器</strong></p>
</blockquote>
<p>为了在一个 Spark 集群上运行计算，SparkContext 对象可以连接到几种集群管理器。包括 Spark 自己的 standalone cluster manager、 Mesos 或者 Yarn 等。</p>
<p>集群管理器负责跨应用程序分配资源。</p>
<hr>
<h3 id="1-2-6-专业术语列表"><a href="#1-2-6-专业术语列表" class="headerlink" title="1.2.6  专业术语列表"></a>1.2.6  专业术语列表</h3><table>
<thead>
<tr>
<th>Term</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td>Application</td>
<td>User program  built on Spark. Consists of a <em>driver  program</em> and <em>executors</em> on the  cluster. (构建于 Spark 之上的应用程序. 包含驱动程序和运行在集群上的执行器)</td>
</tr>
<tr>
<td>Application jar</td>
<td>A jar  containing the user’s Spark application. In some cases users will want to create  an “uber jar” containing their application along with its dependencies. The  user’s jar should never include Hadoop or Spark libraries, however, these  will be added at runtime</td>
</tr>
<tr>
<td>Driver program</td>
<td>The thread  running the main() function of the application and creating the SparkContext</td>
</tr>
<tr>
<td>Cluster manager</td>
<td>An external  service for acquiring resources on the cluster (e.g. standalone manager,  Mesos, YARN)</td>
</tr>
<tr>
<td>Deploy mode<br>部署模式</td>
<td>Distinguishes  where the driver process runs. In “cluster” mode, the framework launches the  driver inside of the cluster. In “client” mode, the submitter launches the  driver outside of the cluster.<br>翻译：用于指定 driver 进程运行的位置。在“cluster”模式下，driver 进程运行在集群中；在“client”模式下，driver 进程运行在集群外部，也就是 client 所在的机器上。默认是 client 模式，Driver运行在Client客户机上</td>
</tr>
<tr>
<td>Worker node</td>
<td>Any node that  can run application code in the cluster</td>
</tr>
<tr>
<td>Executor</td>
<td>A process  launched for an application on a worker node, that runs tasks and keeps data  in memory or disk storage across them. Each application has its own  executors.</td>
</tr>
<tr>
<td>Task</td>
<td>A unit of work  that will be sent to one executor</td>
</tr>
<tr>
<td>Job</td>
<td>A parallel  computation consisting of multiple tasks that gets spawned in response to a  Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs.</td>
</tr>
<tr>
<td>Stage</td>
<td>Each job gets  divided into smaller sets of tasks called <em>stages</em>  that depend on each other (similar to the map and reduce stages in  MapReduce); you’ll see this term used in the driver’s logs.</td>
</tr>
</tbody></table>
<hr>
<h2 id="2-3-Standalone-模式"><a href="#2-3-Standalone-模式" class="headerlink" title="2.3 Standalone 模式"></a>2.3 Standalone 模式</h2><p><code>Standalone</code> 模式指使用 Spark 内置的 <code>Cluster Manager</code> 资源管理器搭建的集群。不需要借助其他的框架，是相对于 Yarn 和 Mesos 来说的。</p>
<p>构建一个由 Master + Slave 构成的 Spark 集群，Spark 运行在集群中。</p>
<h3 id="2-3-1-配置-Standalone-模式"><a href="#2-3-1-配置-Standalone-模式" class="headerlink" title="2.3.1 配置 Standalone 模式"></a>2.3.1 配置 Standalone 模式</h3><h4 id="步骤1-复制-spark-并命名为spark-standalone"><a href="#步骤1-复制-spark-并命名为spark-standalone" class="headerlink" title="步骤1: 复制 spark, 并命名为spark-standalone"></a>步骤1: 复制 spark, 并命名为spark-standalone</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> -r spark-2.1.1-bin-hadoop2.7 spark-standalone</span><br></pre></td></tr></table></figure>



<h4 id="步骤2-进入配置文件目录conf-配置spark-evn-sh"><a href="#步骤2-进入配置文件目录conf-配置spark-evn-sh" class="headerlink" title="步骤2: 进入配置文件目录conf, 配置spark-evn.sh"></a>步骤2: 进入配置文件目录conf, 配置spark-evn.sh</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> conf/</span><br><span class="line"><span class="built_in">cp</span> spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure>



<p>在 <code>spark-env.sh</code> 文件中配置如下内容：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SPARK_MASTER_HOST=hadoop102	<span class="comment"># 配置集群中Master的IP地址</span></span><br><span class="line">SPARK_MASTER_PORT=7077 <span class="comment"># 配置集群中Master的端口号。默认端口就是7077, 可以省略不配</span></span><br></pre></td></tr></table></figure>



<h4 id="步骤3-修改-slaves-文件-添加-worker-节点"><a href="#步骤3-修改-slaves-文件-添加-worker-节点" class="headerlink" title="步骤3: 修改 slaves 文件, 添加 worker 节点"></a>步骤3: 修改 slaves 文件, 添加 worker 节点</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> slaves.template slaves</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在slaves文件中配置如下内容（配置Slave节点的IP地址）:</span></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure>





<h4 id="步骤4-分发spark-standalone配置文件到整个集群中"><a href="#步骤4-分发spark-standalone配置文件到整个集群中" class="headerlink" title="步骤4: 分发spark-standalone配置文件到整个集群中"></a>步骤4: 分发spark-standalone配置文件到整个集群中</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="步骤5-启动-Spark-集群"><a href="#步骤5-启动-Spark-集群" class="headerlink" title="步骤5: 启动 Spark 集群"></a>步骤5: 启动 Spark 集群</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure>

<h4 id="步骤6-在网页中查看-Spark-集群情况"><a href="#步骤6-在网页中查看-Spark-集群情况" class="headerlink" title="步骤6: 在网页中查看 Spark 集群情况"></a>步骤6: 在网页中查看 Spark 集群情况</h4><p>由于 Master 节点配置在 hadoop102 上，所以要通过以下地址来访问 UI 界面: <a target="_blank" rel="noopener" href="http://hadoop201:8080/">http://hadoop201:8080</a></p>
<h3 id="2-3-2-使用-Standalone-模式运行计算-PI-的程序"><a href="#2-3-2-使用-Standalone-模式运行计算-PI-的程序" class="headerlink" title="2.3.2 使用 Standalone 模式运行计算 PI 的程序"></a>2.3.2 使用 Standalone 模式运行计算 PI 的程序</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop201:7077 \	<span class="comment"># 需要显式指定master的URL</span></span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 6 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar 100</span><br></pre></td></tr></table></figure>





<h3 id="2-3-3-在-Standalone-模式下启动-Spark-shell"><a href="#2-3-3-在-Standalone-模式下启动-Spark-shell" class="headerlink" title="2.3.3 在 Standalone 模式下启动 Spark-shell"></a>2.3.3 在 Standalone 模式下启动 Spark-shell</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell \</span><br><span class="line">--master spark://hadoop102:7077	<span class="comment"># 需要显式指定master</span></span><br></pre></td></tr></table></figure>



<p>执行 wordcount 程序</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">&quot;input/&quot;</span>).flatMap(.split(<span class="string">&quot; &quot;</span>)).map((,<span class="number">1</span>)).reduceByKey(+).collect</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((are,<span class="number">2</span>), (how,<span class="number">2</span>), (hello,<span class="number">4</span>), (atguigu,<span class="number">2</span>), (world,<span class="number">2</span>), (you,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<p><strong>注意:</strong></p>
<ul>
<li>  每个 worker 节点上要有相同的文件夹 <code>:input/</code>，否则会报文件不存在的异常</li>
</ul>
<hr>
<h2 id="2-4-Yarn-模式"><a href="#2-4-Yarn-模式" class="headerlink" title="2.4    Yarn 模式"></a>2.4    Yarn 模式</h2><h3 id="2-4-1-Yarn-模式概述"><a href="#2-4-1-Yarn-模式概述" class="headerlink" title="2.4.1    Yarn 模式概述"></a>2.4.1    Yarn 模式概述</h3><p><code>Spark Client</code> 可以直接连接 <code>Yarn</code>，利用 <code>Yarn</code> 管理的计算资源来进行 <code>Spark</code> 程序的运算，不需要额外构建 <code>Spark</code> 集群。</p>
<p>有 <code>client</code> 和 <code>cluster</code> 两种模式，主要区别在于：Driver 程序的运行节点不同。</p>
<ul>
<li>  <strong>client</strong>：Driver 程序运行在客户端，适用于交互、调试，希望立即看到 APP 的输出。</li>
<li>  <strong>cluster</strong>：Driver 程序运行在由 ResourceManager 启动的 AplicationMaster 上，适用于生产环境。</li>
</ul>
<p><strong>工作模式介绍：</strong></p>
<p><img src="/2021/12/11/Spark/image-20211213194827471.png" alt="image-20211213194827471"></p>
<hr>
<h3 id="2-4-2-Yarn-模式配置"><a href="#2-4-2-Yarn-模式配置" class="headerlink" title="2.4.2 Yarn 模式配置"></a>2.4.2 Yarn 模式配置</h3><p><strong>一、修改 Hadoop 集群的配置文件 yarn-site.xml，添加如下内容：</strong></p>
<p>Spark 对内存的要求特别高，所以 Spark 总是会检测到内存不够，把一些耗费内存大的进程杀死，而我们的测试环境内存比较小，我们需要把自动杀进程的配置给关掉。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>  修改后分发配置文件。</li>
</ul>
<p><strong>二、复制 spark 安装包，并重命名为 spark-yarn</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 module]$ <span class="built_in">cp</span> -r spark-2.1.1-bin-hadoop2.7 spark-yarn</span><br></pre></td></tr></table></figure>





<p><strong>三、修改 spark-evn.sh 文件</strong></p>
<p>Spark 想要在 Yarn 上运行就需要知道 Yarn 中 <code>ResourceManager</code> 的地址，而我们已经配置好了 Hadoop 的集群，所以只需要让 Spark 和 Hadoop 关联起来，Spark 就能间接地知道 <code>ResourceManager</code> 的地址了。</p>
<ul>
<li><p>复制 conf 包下的 <code>spark-env.sh.template</code> 为 <code>spark-env.sh</code></p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 conf]$ <span class="built_in">cp</span> spark-env.sh.template spark-env.sh</span><br><span class="line">[lvnengdong@hadoop102 conf]$ vim spark-env.sh</span><br></pre></td></tr></table></figure></li>
<li><p>并添加如下配置：告诉 Spark 客户端 Yarn 的相关配置</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><strong>四、配置历史日志服务器</strong></p>
<ol>
<li><p>复制 conf 目录下的 <code>spark-defaults.conf.template</code> 为 <code>spark-defaults.conf</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 conf]$ <span class="built_in">cp</span> spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure></li>
<li><p>配置 <code>spark-default.conf</code> 文件，开启 Log</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Before</span></span><br><span class="line"><span class="comment"># spark.eventLog.enabled           true</span></span><br><span class="line"><span class="comment"># spark.eventLog.dir               hdfs://namenode:8021/directory</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># After</span></span><br><span class="line">spark.eventLog.enabled           <span class="literal">true</span></span><br><span class="line">spark.eventLog.<span class="built_in">dir</span>               hdfs://hadoop102:9000/spark-job-log	<span class="comment"># 保存历史日志的地址</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>hdfs://hadoop102:9000/spark-job-log</code> 目录必须提前存在，目录名字可以随便起。所以我们在启动 HDFS 后，必须在 HDFS 的根目录下创建 spark-job-log 这个目录。</li>
</ul>
</li>
<li><p>在 <code>spark-env.sh</code> 文件中添加如下配置 </p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">&quot;-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://hadoop102:9000/spark-job-log&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 说明：</span></span><br><span class="line"><span class="comment"># -Dspark.history.ui.port=18080 ：历史日志服务器的UI端口号</span></span><br><span class="line"><span class="comment"># -Dspark.history.retainedApplications=30</span></span><br><span class="line"><span class="comment"># -Dspark.history.fs.logDirectory=hdfs://hadoop102:9000/spark-job-log&quot;	历史日志服务器保存文件的地址</span></span><br></pre></td></tr></table></figure>

</li>
<li><p>启动 Hadoop 集群（包括 HDFS、Yarn 等等）</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 群起HDFS</span></span><br><span class="line">[lvnengdong@hadoop103 conf]$ start-dfs.sh</span><br><span class="line"><span class="comment"># 群起Yarn</span></span><br><span class="line">[lvnengdong@hadoop103 apache-zookeeper-3.5.7-bin]$ start-yarn.sh</span><br><span class="line"><span class="comment"># 启动Hadoop的历史日志服务器</span></span><br><span class="line">[lvnengdong@hadoop102 conf]$ mr-jobhistory-daemon.sh start historyserver</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看启动的JPS进程</span></span><br><span class="line">[lvnengdong@hadoop103 apache-zookeeper-3.5.7-bin]$ xcall jps</span><br><span class="line">要执行的命令是jps</span><br><span class="line">---------------------hadoop102-----------------</span><br><span class="line">34515 NodeManager</span><br><span class="line">33958 NameNode</span><br><span class="line">34151 DataNode</span><br><span class="line">34921 Jps</span><br><span class="line">34811 JobHistoryServer	<span class="comment"># Hadoop的历史日志服务器</span></span><br><span class="line">---------------------hadoop103-----------------</span><br><span class="line">82307 Jps</span><br><span class="line">81637 ResourceManager</span><br><span class="line">81467 DataNode</span><br><span class="line">81786 NodeManager</span><br><span class="line">---------------------hadoop104-----------------</span><br><span class="line">81984 NodeManager</span><br><span class="line">81831 SecondaryNameNode</span><br><span class="line">82311 Jps</span><br><span class="line">81673 DataNode</span><br></pre></td></tr></table></figure>

</li>
<li><p>在 HDFS 的根目录下创建  <code>spark-job-log</code> 目录，用于保存 Spark 运行时产生的日志。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-yarn]$ hadoop fs -<span class="built_in">mkdir</span> /spark-job-log</span><br></pre></td></tr></table></figure>

<p> <img src="/2021/12/11/Spark/image-20211213211239915.png" alt="image-20211213211239915"></p>
</li>
<li><p> 启动 Spark 的历史日志服务器（需要先启动 HDFS，因为日志文件保存在 HDFS 上）</p>
</li>
<li><p>启动 Yarn 的历史日志服务器</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动 Spark 的历史日志服务器</span></span><br><span class="line">[lvnengdong@hadoop102 spark-yarn]$ sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动Hadoop的历史日志服务器</span></span><br><span class="line">[lvnengdong@hadoop102 spark-yarn]$ mr-jobhistory-daemon.sh start historyserver</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看JPS进程详情</span></span><br><span class="line">[lvnengdong@hadoop102 spark-yarn]$ jps -l</span><br><span class="line">34515 org.apache.hadoop.yarn.server.nodemanager.NodeManager</span><br><span class="line">33958 org.apache.hadoop.hdfs.server.namenode.NameNode</span><br><span class="line">34151 org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line">35306 org.apache.spark.deploy.history.HistoryServer		<span class="comment"># spark的历史日志服务器</span></span><br><span class="line">34811 org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer	<span class="comment"># Hadoop的历史日志服务器</span></span><br><span class="line">35563 sun.tools.jps.Jps</span><br></pre></td></tr></table></figure>

<ul>
<li>  Spark 历史日志服务器的 UI 地址：<code>http://hadoop102:18080/</code></li>
<li>  Hadoop 历史日志服务器的 UI 地址：</li>
<li>  <strong>注意：</strong>因为 Spark 需要访问 HDFS 来读/写日志信息，所以一定要保证 Spark 有权限读写 HDFS 文件系统。</li>
</ul>
</li>
<li><p>执行一段程序</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar 100</span><br></pre></td></tr></table></figure>

<p> 在 Yarn 的 ResourceManager 提供的 UI 页面上可以查看日志：地址为<code>http://hadoop103:8088/</code></p>
<p> <img src="/2021/12/11/Spark/image-20211213224417923.png" alt="image-20211213224417923"></p>
</li>
<li><p>日志服务</p>
<p> 在第8步的页面上点击 <code>history</code> 无法直接连接到 Spark 的日志。可以在 <code>spark-default.conf</code> 中添加如下配置达到上述目的。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.yarn.historyServer.address=hadoop102:18080</span><br><span class="line">spark.history.ui.port=18080</span><br></pre></td></tr></table></figure></li>
<li><p>启动 Yarn 模式下的 Spark-shell</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-yarn]$ bin/spark-shell --master yarn</span><br></pre></td></tr></table></figure>

</li>
<li><p> 注意：在 Yarn 模式下启动 Spark 后，处理的文件就会直接从 HDFS 上读取，而不是本地磁盘系统。</p>
</li>
</ol>
<hr>
<h2 id="2-6-几种运行模式的对比"><a href="#2-6-几种运行模式的对比" class="headerlink" title="2.6 几种运行模式的对比"></a>2.6 几种运行模式的对比</h2><table>
<thead>
<tr>
<th>模式</th>
<th>Spark安装机器数</th>
<th>需启动的进程</th>
<th>所属者</th>
</tr>
</thead>
<tbody><tr>
<td>Local</td>
<td>1</td>
<td>无</td>
<td>Spark</td>
</tr>
<tr>
<td>Standalone</td>
<td>多台</td>
<td>Master及Worker</td>
<td>Spark</td>
</tr>
<tr>
<td>Yarn</td>
<td>1</td>
<td>Yarn及HDFS</td>
<td>Hadoop</td>
</tr>
</tbody></table>
<h1 id="入门案例"><a href="#入门案例" class="headerlink" title="入门案例"></a>入门案例</h1><p><code>spark shell</code> 仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在 IDE 中编制程序，然后打成 jar 包，然后提交到集群，最常用的是创建一个 Maven 项目，利用 Maven 来管理 jar 包的依赖。</p>
<h2 id="1、创建-maven-项目-导入依赖"><a href="#1、创建-maven-项目-导入依赖" class="headerlink" title="1、创建 maven 项目, 导入依赖"></a>1、创建 maven 项目, 导入依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Spark核心依赖--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 打包插件, 否则 scala 类不会编译并打包进去 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="2、创建WordCount-scala"><a href="#2、创建WordCount-scala" class="headerlink" title="2、创建WordCount.scala"></a>2、创建WordCount.scala</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> lnd</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2021/12/14 10:49</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Version</span> 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 1、创建一个 SparkContext</span></span><br><span class="line"><span class="comment"> * 2、从数据源得到一个RDD</span></span><br><span class="line"><span class="comment"> * 3、对RDD做各种转换</span></span><br><span class="line"><span class="comment"> * 4、执行一个行动算子</span></span><br><span class="line"><span class="comment"> * 5、关闭SparkContext</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">object WordCount &#123;</span><br><span class="line"></span><br><span class="line">  def <span class="title function_">main</span><span class="params">(args: Array[String])</span>: Unit = &#123;</span><br><span class="line">    <span class="comment">// 1、创建一个 SparkContext</span></span><br><span class="line">    val sparkConf: SparkConf = <span class="keyword">new</span> <span class="title class_">SparkConf</span>()</span><br><span class="line">    sparkConf.setMaster(<span class="string">&quot;local[2]&quot;</span>)	<span class="comment">// 本地模式</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        	如果想要使用 Yarn 环境运行 WordCount 程序，需要把该代码打成一个 jar 包</span></span><br><span class="line"><span class="comment">        	上传到 Linux 中去执行，打包的时候，需要把 Master 的设置去掉，在 Linux 下</span></span><br><span class="line"><span class="comment">        	提交的时候再使用 --master 来显式设置 master 为 Yarn</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">    sparkConf.setAppName(<span class="string">&quot;wc&quot;</span>)</span><br><span class="line">    val sc: SparkContext = <span class="keyword">new</span> <span class="title class_">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、从数据源得到一个RDD</span></span><br><span class="line">    <span class="type">val</span> <span class="variable">lineRDD</span> <span class="operator">=</span> sc.textFile(args(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、对RDD做各种转换</span></span><br><span class="line">    <span class="type">val</span> <span class="variable">resultRDD</span> <span class="operator">=</span> lineRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、执行一个行动算子【Collect：把各个节点计算后的数据，拉取到驱动端】</span></span><br><span class="line">    val wordCountArr: Array[(String, Int)] = resultRDD.collect()</span><br><span class="line">    wordCountArr.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5、关闭SparkContext</span></span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="3、测试"><a href="#3、测试" class="headerlink" title="3、测试"></a>3、测试</h2><h3 id="本地模式测试"><a href="#本地模式测试" class="headerlink" title="本地模式测试"></a>本地模式测试</h3><p>使用 local 模式执行，相当于代码是在 window 下执行的。</p>
<p>在执行 WordCount 程序时，我们文件的路径 path 是从 main() 方法的 args(0) 中获取的，下图为向 args() 参数类表中传参的过程。</p>
<p><img src="/2021/12/11/Spark/image-20211214110732541.png" alt="image-20211214110732541"></p>
<h3 id="打包到-Linux-下测试"><a href="#打包到-Linux-下测试" class="headerlink" title="打包到 Linux 下测试"></a>打包到 Linux 下测试</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sparkConf.setMaster(<span class="string">&quot;local[2]&quot;</span>)	<span class="comment">// 本地模式</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    	如果想要使用 Yarn 环境运行 WordCount 程序，需要把该代码打成一个 jar 包</span></span><br><span class="line"><span class="comment">    	上传到 Linux 中去执行，打包的时候，需要把 Master 的设置去掉，在 Linux 下</span></span><br><span class="line"><span class="comment">    	提交的时候再使用 --master 来显式设置 master 为 Yarn</span></span><br><span class="line"><span class="comment">    */</span></span><br></pre></td></tr></table></figure>



<ol>
<li><p> 打包：<code>Maven --&gt; package</code></p>
</li>
<li><p>这里我们只需对 spark-core 项目进行打包即可，因为我们代码相关的东西都在这个包下</p>
<p> <img src="/2021/12/11/Spark/image-20211214112248174.png" alt="image-20211214112248174"></p>
</li>
<li><p>打包成功后，在 <code>target</code> 目录下会多出来一个 jar 包</p>
<p> <img src="/2021/12/11/Spark/image-20211214112620970.png" alt="image-20211214112620970"></p>
</li>
<li><p> 将 jar 包上传到 Linux 中 Spark 的安装目录下，即<code>/opt/module/spark-yarn</code></p>
</li>
<li><p>执行如下命令</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-yarn]$ bin/spark-submit --master yarn --deploy-mode client --class WordCount ./spark-core-1.0-SNAPSHOT.jar hdfs://hadoop102:9000/wcinput</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 命令分解</span></span><br><span class="line">bin/spark-submit	<span class="comment"># 提交任务</span></span><br><span class="line">--master yarn 	<span class="comment"># 指定Master</span></span><br><span class="line">--deploy-mode client <span class="comment"># 指定deploy-mode</span></span><br><span class="line">--class WordCount <span class="comment"># 全限定类名</span></span><br><span class="line">./spark-core-1.0-SNAPSHOT.jar 	<span class="comment"># Application应用jar包的位置</span></span><br><span class="line">hdfs://hadoop102:9000/wcinput	<span class="comment"># main() 方法中传入的参数</span></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/07/Scala/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/07/Scala/" class="post-title-link" itemprop="url">Scala</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-07 11:34:21" itemprop="dateCreated datePublished" datetime="2021-12-07T11:34:21+08:00">2021-12-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-12-23 14:36:07" itemprop="dateModified" datetime="2021-12-23T14:36:07+08:00">2021-12-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Scala/" itemprop="url" rel="index"><span itemprop="name">Scala</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h1><h2 id="6-高阶函数"><a href="#6-高阶函数" class="headerlink" title="6    高阶函数"></a>6    高阶函数</h2><p>说明</p>
<ul>
<li>  定义：<strong>参数为函数的函数称为高阶函数</strong> </li>
</ul>
<p>案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//高阶函数————函数作为参数</span></span><br><span class="line">        <span class="comment">/* </span></span><br><span class="line"><span class="comment">        	参数a，类型是 Int</span></span><br><span class="line"><span class="comment">        	参数b，类型是 Int</span></span><br><span class="line"><span class="comment">        	参数operater，类型是函数</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">calculator</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>, operater: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">            operater(a, b)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//函数————求和</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">plus</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">            x + y</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//方法————求积</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">multiply</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">            x * y</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//函数作为参数</span></span><br><span class="line">        println(calculator(<span class="number">2</span>, <span class="number">3</span>, plus))</span><br><span class="line">        println(calculator(<span class="number">2</span>, <span class="number">3</span>, multiply))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<hr>
<h2 id="8-函数柯里化-amp-闭包"><a href="#8-函数柯里化-amp-闭包" class="headerlink" title="8 函数柯里化&amp;闭包"></a>8 函数柯里化&amp;闭包</h2><p><strong>说明</strong></p>
<p>函数柯里化：将一个接收多个参数的函数转化成一个接受一个参数的函数过程，可以简单的理解为一种<strong>特殊的参数列表声明方式</strong>。</p>
<p>闭包：就是<strong>一个函数</strong>和与<strong>其相关的引用环境（变量）</strong>组合的一个<strong>整体</strong>(实体)</p>
<p>【闭包会阻止外部局部变量的销毁，会把使用到的局部变量的生命周期延长到函数的外部】</p>
<p><strong>案例实操</strong></p>
<ol>
<li><p>闭包</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">    </span><br></pre></td></tr></table></figure>

</li>
<li><p>柯里化</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestFunction</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 匿名函数，接收三个参数</span></span><br><span class="line">    <span class="keyword">val</span> sum = (x: <span class="type">Int</span>, y: <span class="type">Int</span>, z: <span class="type">Int</span>) =&gt; x + y + z</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 分别接收三个参数</span></span><br><span class="line">    <span class="keyword">val</span> sum1 = (x: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">        y: <span class="type">Int</span> =&gt; &#123;</span><br><span class="line">            z: <span class="type">Int</span> =&gt; &#123;</span><br><span class="line">                x + y + z</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 方式二</span></span><br><span class="line">    <span class="keyword">val</span> sum2 = (x: <span class="type">Int</span>) =&gt; (y: <span class="type">Int</span>) =&gt; (z: <span class="type">Int</span>) =&gt; x + y + z</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sum3</span></span>(x: <span class="type">Int</span>)(y: <span class="type">Int</span>)(z: <span class="type">Int</span>) = x + y + z</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 测试</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        sum(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">        sum1(<span class="number">1</span>)(<span class="number">2</span>)(<span class="number">3</span>)</span><br><span class="line">        sum2(<span class="number">1</span>)(<span class="number">2</span>)(<span class="number">3</span>)</span><br><span class="line">        sum3(<span class="number">1</span>)(<span class="number">2</span>)(<span class="number">3</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="11-惰性求值"><a href="#11-惰性求值" class="headerlink" title="11 惰性求值"></a>11 惰性求值</h2><p>1）说明</p>
<p>当函数返回值被声明为lazy时，函数的执行将被推迟，直到我们首次对此取值，该函数才会执行。这种函数我们称之为惰性函数。</p>
<p>2）案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// lazy修饰</span></span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> res = sum(<span class="number">10</span>, <span class="number">30</span>)</span><br><span class="line">    println(<span class="string">&quot;----------------&quot;</span>)</span><br><span class="line">    println(<span class="string">&quot;res=&quot;</span> + res)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(n1: <span class="type">Int</span>, n2: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    println(<span class="string">&quot;sum被执行。。。&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> n1 + n2</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意：lazy不能修饰var类型的变量</p>
<hr>
<h1 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h1><h2 id="类和对象"><a href="#类和对象" class="headerlink" title="类和对象"></a>类和对象</h2><h3 id="6-2-1-定义类"><a href="#6-2-1-定义类" class="headerlink" title="6.2.1 定义类"></a>6.2.1 定义类</h3><ol>
<li><p>基本语法</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[修饰符] <span class="class"><span class="keyword">class</span> <span class="title">类名</span> </span>&#123;</span><br><span class="line">  类体</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<ul>
<li>说明<ol>
<li> Scala语法中，类并<strong>不能声明为public</strong>，所有这些类默认就是public</li>
<li> 一个Scala源文件可以包含多个类</li>
</ol>
</li>
</ul>
</li>
<li><p>案例实操</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">    </span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="6-2-2-属性"><a href="#6-2-2-属性" class="headerlink" title="6.2.2 属性"></a>6.2.2 属性</h3><p>属性是类的一个组成部分</p>
<ol>
<li> 基本语法</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[修饰符] <span class="keyword">var</span> 属性名称 [：类型] = 属性值</span><br></pre></td></tr></table></figure>

<ul>
<li>  注：Bean属性（@BeanPropetry），可以自动生成规范的setXxx/getXxx方法</li>
</ul>
<ol start="2">
<li> 案例实操</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.scala.test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.beans.<span class="type">BeanProperty</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> name: <span class="type">String</span> = <span class="string">&quot;bobo&quot;</span> <span class="comment">//定义属性</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> age: <span class="type">Int</span> = _ <span class="comment">// _表示给属性一个默认值</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//Bean属性（@BeanProperty）</span></span><br><span class="line">  <span class="meta">@BeanProperty</span> <span class="keyword">var</span> sex: <span class="type">String</span> = <span class="string">&quot;男&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> person = <span class="keyword">new</span> <span class="type">Person</span>()</span><br><span class="line">    println(person.name)</span><br><span class="line"></span><br><span class="line">    person.setSex(<span class="string">&quot;女&quot;</span>)</span><br><span class="line">    println(person.getSex)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="6-2-4-创建对象"><a href="#6-2-4-创建对象" class="headerlink" title="6.2.4 创建对象"></a>6.2.4 创建对象</h3><p>1）基本语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val | var 对象名 [：类型] = new 类型()</span><br></pre></td></tr></table></figure>



<p>2）案例实操</p>
<p>（1）val修饰对象，不能改变对象的引用（即：内存地址），可以改变对象属性的值。</p>
<p>（2）var修饰对象，可以修改对象的引用和修改对象的属性值</p>
<hr>
<h3 id="6-2-5-构造器"><a href="#6-2-5-构造器" class="headerlink" title="6.2.5 构造器"></a>6.2.5 构造器</h3><p>和Java一样，Scala构造对象也需要调用构造方法，并且可以有任意多个构造方法。</p>
<p>Scala类的构造器包括：<strong>主构造器和辅助构造器</strong></p>
<p>1）基本语法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">类名</span>(<span class="params">形参列表</span>) </span>&#123;  <span class="comment">// 主构造器</span></span><br><span class="line">    <span class="comment">// 类体</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">this</span></span>(形参列表) &#123;  <span class="comment">// 辅助构造器</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">this</span></span>(形参列表) &#123;  <span class="comment">//辅助构造器可以有多个...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>



<p>说明：</p>
<p>（1）辅助构造器，函数的名称this，可以有多个，编译器通过参数的个数来区分。</p>
<p>（2）<strong>辅助构造方法不能直接构建对象，必须直接或者间接调用主构造方法</strong>。</p>
<p>2）案例实操</p>
<p>（1）<strong>如果主构造器无参数，小括号可省略</strong>，构建对象时调用的构造方法的小括号也可以省略。</p>
<h3 id="6-2-6-构造器参数"><a href="#6-2-6-构造器参数" class="headerlink" title="6.2.6 构造器参数"></a>6.2.6 构造器参数</h3><p>1）说明</p>
<p>Scala类的主构造器函数的形参包括三种类型：未用任何修饰、var修饰、val修饰</p>
<p>（1）未用任何修饰符修饰，这个参数就是一个局部变量</p>
<p>（2）var修饰参数，作为类的成员属性使用，可以修改</p>
<p>（3）val修饰参数，作为类只读属性使用，不能修改</p>
<p>2）案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, var age: <span class="type">Int</span>, val sex: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">var</span> person = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">&quot;bobo&quot;</span>, <span class="number">18</span>, <span class="string">&quot;男&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// （1）未用任何修饰符修饰，这个参数就是一个局部变量，在类之外的地方不能被访问到</span></span><br><span class="line">        <span class="comment">// printf(person.name)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// （2）var修饰参数，作为类的成员属性使用，可以修改</span></span><br><span class="line">        person.age = <span class="number">19</span></span><br><span class="line">        println(person.age)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// （3）val修饰参数，作为类的只读属性使用，不能修改</span></span><br><span class="line">        <span class="comment">// person.sex = &quot;女&quot;</span></span><br><span class="line">        println(person.sex)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="6-5-抽象属性和抽象方法"><a href="#6-5-抽象属性和抽象方法" class="headerlink" title="6.5 抽象属性和抽象方法"></a>6.5 抽象属性和抽象方法</h2><h3 id="6-5-1-抽象属性和抽象方法"><a href="#6-5-1-抽象属性和抽象方法" class="headerlink" title="6.5.1 抽象属性和抽象方法"></a>6.5.1 抽象属性和抽象方法</h3><p>一、基本语法</p>
<p>  1.定义抽象类：abstract class Person{} //通过abstract关键字标记抽象类</p>
<ol start="2">
<li> 定义抽象属性：val|var name:String //一个属性没有初始化，就是抽象属性</li>
</ol>
<p>​    （3）定义抽象方法：def hello():String //只声明而没有实现的方法，就是抽象方法</p>
<p>二、继承&amp;重写</p>
<p>java 中只支持方法重写，而 scala 中还支持属性重写</p>
<p>（1）如果父类为抽象类，那么子类需要将抽象的属性和方法实现，否则子类也需声明为抽象类</p>
<p>（2）重写非抽象方法需要用override修饰，重写抽象方法则可以不加override。</p>
<p>（3）子类中调用父类的方法使用super关键字</p>
<p>（4）<strong>属性重写只支持val类型，而不支持var</strong>。 </p>
<hr>
<h2 id="6-6-单例对象（伴生对象）"><a href="#6-6-单例对象（伴生对象）" class="headerlink" title="6.6 单例对象（伴生对象）"></a>6.6 单例对象（伴生对象）</h2><p>Scala语言是完全面向对象的语言，所以并没有静态的操作（即在Scala中没有静态的概念）。但是为了能够和 JVM 交互（因为 JVM 字节码中有静态概念），就产生了一种特殊的对象来模拟类对象，该对象为<strong>单例对象</strong>。若单例对象名与类名一致，则称该单例对象这个类的<strong>伴生对象</strong>，这个类的所有“静态”内容都可以放置在它的伴生对象中声明。</p>
<p><strong>伴生对象和伴生类</strong>：编译成字节码之后，伴生对象中的成员就是一个 .class 文件中的静态成员，伴生类中的成员就是该文件中的普通成员。并且伴生类和伴生对象可以互相访问对方的私有成员。</p>
<p>1）基本语法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">    <span class="keyword">val</span> country:<span class="type">String</span>=<span class="string">&quot;China&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>2）说明</p>
<p>（1）单例对象采用object关键字声明</p>
<p>（2）单例对象对应的类称之为伴生类，伴生对象的名称应该和伴生类名一致。</p>
<p>（3）<strong>单例对象中的属性和方法都可以通过伴生对象名（类名）直接调用访问。</strong></p>
<h3 id="6-6-2-apply方法"><a href="#6-6-2-apply方法" class="headerlink" title="6.6.2 apply方法"></a>6.6.2 apply方法</h3><p>1）说明</p>
<p>（1）通过伴生对象的apply方法，实现不使用new方法创建对象。</p>
<p>（2）如果想让主构造器变成私有的，可以在()之前加上private。</p>
<p>（3）apply方法可以重载。</p>
<p>（4）Scala中 <code>obj(arg)</code> 语句实际是在调用该对象的 <code>apply()</code> 方法，即 <code>obj.apply(arg)</code>。用以同一面向对象编程和函数式编程的风格。</p>
<p>2）案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//（1）通过伴生对象的apply方法，实现不使用new关键字创建对象。</span></span><br><span class="line">        <span class="keyword">val</span> p1 = <span class="type">Person</span>()</span><br><span class="line">        println(<span class="string">&quot;p1.name=&quot;</span> + p1.name)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> p2 = <span class="type">Person</span>(<span class="string">&quot;bobo&quot;</span>)</span><br><span class="line">        println(<span class="string">&quot;p2.name=&quot;</span> + p2.name)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//（2）如果想让主构造器变成私有的，可以在()之前加上private</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="title">private</span>(<span class="params">cName: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> name: <span class="type">String</span> = cName</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(): <span class="type">Person</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;apply空参被调用&quot;</span>)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">&quot;xx&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(name: <span class="type">String</span>): <span class="type">Person</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;apply有参被调用&quot;</span>)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Person</span>(name)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="trait"><a href="#trait" class="headerlink" title="trait"></a>trait</h2><p><strong>trait</strong>：特质。类似于 Java 中的接口，编译后就会成为 JVM 中的接口</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">抽象类中有的东西，trait 中都会有。trait 与抽象类不同之处在于多继承。</span><br></pre></td></tr></table></figure>

<p>Scala 语言中，采用特质trait（特征）来代替接口的概念，也就是说，多个类具有相同的特征（特征）时，就可以将这个特质（特征）独立出来，采用关键字 <code>trait</code> 声明。</p>
<p>Scala 中的 <code>trait</code> 中既可以有抽象属性和方法，也可以有具体的属性和方法<strong>，</strong>一个类可以混入（mixin）多个特质。</p>
<p>Scala引入trait特征，第一可以替代Java的接口，第二个也是对单继承机制的一种补充。</p>
<h3 id="6-7-1-特质声明"><a href="#6-7-1-特质声明" class="headerlink" title="6.7.1 特质声明"></a>6.7.1 特质声明</h3><p>1）基本语法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">特质名</span> </span>&#123;</span><br><span class="line">	<span class="class"><span class="keyword">trait</span><span class="title">体</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PersonTrait</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 声明属性</span></span><br><span class="line">    <span class="keyword">var</span> name:<span class="type">String</span> = _</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 声明方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eat</span></span>():<span class="type">Unit</span>=&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 抽象属性</span></span><br><span class="line">    <span class="keyword">var</span> age:<span class="type">Int</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 抽象方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">say</span></span>():<span class="type">Unit</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="6-7-2-特质基本语法"><a href="#6-7-2-特质基本语法" class="headerlink" title="6.7.2 特质基本语法"></a>6.7.2 特质基本语法</h3><p>一个类具有某种特质（特征），就意味着这个类满足了这个特质（特征）的所有要素，所以在使用时，也采用了extends关键字，如果有多个特质或存在父类，那么需要采用with关键字连接。</p>
<p>1）基本语法：</p>
<p><strong>没有父类</strong>：class 类名 <strong>extends</strong> 特质1  <strong>with</strong>  特质2  <strong>with</strong>  特质3 …</p>
<p><strong>有父类</strong>：class 类名  <strong>extends</strong> 父类  <strong>with</strong> 特质1  <strong>with</strong>  特质2 <strong>with</strong> 特质3…</p>
<p>2）说明</p>
<p>  1.类和特质的关系：使用继承的关系。</p>
<ol start="2">
<li> 当一个类去继承特质时，第一个连接词是extends，后面是with。</li>
</ol>
<p>​    （3）如果一个类在继承特质和父类时，应当把父类写在extends后。</p>
<p>3）案例实操</p>
<p>（1）<strong>特质可以同时拥有抽象方法和具体方法</strong></p>
<p>（2）一个类可以混入（mixin）多个特质</p>
<p>（3）所有的Java接口都可以当做Scala特质使用</p>
<p>（4）<strong>动态混入</strong>：可灵活的扩展类的功能</p>
<ul>
<li>  （4.1）动态混入：在创建对象的时候再混入trait，而无需直接在类上混入该trait</li>
<li>  （4.2）如果混入的trait中有未实现的方法，则需要实现</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PersonTrait</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//（1）特质可以同时拥有抽象方法和具体方法</span></span><br><span class="line">  <span class="comment">// 声明属性</span></span><br><span class="line">  <span class="keyword">var</span> name: <span class="type">String</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 抽象属性</span></span><br><span class="line">  <span class="keyword">var</span> age: <span class="type">Int</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 声明方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eat</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">&quot;eat&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 抽象方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">say</span></span>(): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">SexTrait</span> </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> sex: <span class="type">String</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//（2）一个类可以实现/继承多个特质</span></span><br><span class="line"><span class="comment">//（3）所有的Java接口都可以当做Scala特质使用</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Teacher</span> <span class="keyword">extends</span> <span class="title">PersonTrait</span> <span class="keyword">with</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">say</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">&quot;say&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">var</span> age: <span class="type">Int</span> = _</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestTrait</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> teacher = <span class="keyword">new</span> <span class="type">Teacher</span></span><br><span class="line"></span><br><span class="line">    teacher.say()</span><br><span class="line">    teacher.eat()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//（4）动态混入：可灵活的扩展类的功能</span></span><br><span class="line">    <span class="keyword">val</span> t2 = <span class="keyword">new</span> <span class="type">Teacher</span> <span class="keyword">with</span> <span class="type">SexTrait</span> &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="keyword">var</span> sex: <span class="type">String</span> = <span class="string">&quot;男&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//调用混入trait的属性</span></span><br><span class="line">    println(t2.sex)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<hr>
<h3 id="6-7-3-特质叠加"><a href="#6-7-3-特质叠加" class="headerlink" title="6.7.3 特质叠加"></a>6.7.3 特质叠加</h3><p>由于一个类可以混入（mixin）多个 trait，且 trait 中可以有具体的属性和方法，若混入的特质中具有相同的方法（方法名，参数列表，返回值均相同），必然会出现继承冲突问题。冲突分为以下两种：</p>
<p>第一种，一个类（Sub）混入的两个trait（TraitA，TraitB）中具有相同的具体方法，且两个trait之间没有任何关系，解决这类冲突问题，直接在类（Sub）中重写冲突方法。</p>
<p><img src="/2021/12/07/Scala/image-20211210134503220.png" alt="image-20211210134503220"></p>
<p>​                               </p>
<p>第二种，一个类（Sub）混入的两个trait（TraitA，TraitB）中具有相同的具体方法，且两个trait继承自相同的trait（TraitC），及所谓的“钻石问题”，解决这类冲突问题，Scala采用了<strong>特质叠加</strong>的策略。</p>
<p> <img src="/2021/12/07/Scala/image-20211210134518909.png" alt="image-20211210134518909"></p>
<p>所谓的<strong>特质叠加</strong>，就是将混入的多个trait中的冲突方法叠加起来，案例如下，</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Ball</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">describe</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="string">&quot;ball&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Color</span> <span class="keyword">extends</span> <span class="title">Ball</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">describe</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="string">&quot;blue-&quot;</span> + <span class="keyword">super</span>.describe()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Category</span> <span class="keyword">extends</span> <span class="title">Ball</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">describe</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="string">&quot;foot-&quot;</span> + <span class="keyword">super</span>.describe()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyBall</span> <span class="keyword">extends</span> <span class="title">Category</span> <span class="keyword">with</span> <span class="title">Color</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">describe</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="string">&quot;my ball is a &quot;</span> + <span class="keyword">super</span>.describe()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestTrait</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(<span class="keyword">new</span> <span class="type">MyBall</span>().describe())</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 结果为：my ball is a blue-foot-ball</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="6-7-4-特质叠加执行顺序"><a href="#6-7-4-特质叠加执行顺序" class="headerlink" title="6.7.4 特质叠加执行顺序"></a>6.7.4 特质叠加执行顺序</h3><p><strong>思考：</strong>上述案例中的super.describe()调用的是父trait中的方法吗？</p>
<p>当一个类混入多个特质的时候，scala会对所有的特质及其父特质按照一定的顺序进行排序，而此案例中的super.describe()调用的实际上是排好序后的下一个特质中的describe()方法。，排序规则如下：</p>
<p><img src="/2021/12/07/Scala/image-20211210134904813.png" alt="image-20211210134904813"></p>
<p><strong>结论：</strong></p>
<p>（1）案例中的super，不是表示其父特质对象，而是表示上述叠加顺序中的下一个特质，即，MyClass中的super指代Color，Color中的super指代Category，Category中的super指代Ball</p>
<p>（2）如果想要调用某个指定的混入特质中的方法，可以增加约束：super[]，例如super[Category].describe()。</p>
<hr>
<h3 id="6-7-5-特质自身类型"><a href="#6-7-5-特质自身类型" class="headerlink" title="6.7.5 特质自身类型"></a>6.7.5 特质自身类型</h3><p>1）说明</p>
<p>​    自身类型可实现依赖注入的功能。</p>
<p>2）案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">val name: <span class="type">String</span>, val age: <span class="type">Int</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Dao</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(user: <span class="type">User</span>) = &#123;</span><br><span class="line">        println(<span class="string">&quot;insert into database :&quot;</span> + user.name)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">APP</span> </span>&#123;</span><br><span class="line">    _: <span class="type">Dao</span> =&gt;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span></span>(user: <span class="type">User</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;login :&quot;</span> + user.name)</span><br><span class="line">        insert(user)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyApp</span> <span class="keyword">extends</span> <span class="title">APP</span> <span class="keyword">with</span> <span class="title">Dao</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        login(<span class="keyword">new</span> <span class="type">User</span>(<span class="string">&quot;bobo&quot;</span>, <span class="number">11</span>))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="6-8-3-Type定义新类型"><a href="#6-8-3-Type定义新类型" class="headerlink" title="6.8.3 Type定义新类型"></a>6.8.3 Type定义新类型</h3><p>1）说明</p>
<p>使用type关键字可以定义新的数据数据类型名称，本质上就是类型的一个别名</p>
<p>2）案例实操</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="class"><span class="keyword">type</span> <span class="title">S=String</span></span></span><br><span class="line">        <span class="keyword">var</span> v:<span class="type">S</span>=<span class="string">&quot;abc&quot;</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">test</span></span>():<span class="type">S</span>=<span class="string">&quot;xyz&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>









<hr>
<h1 id="隐式转换"><a href="#隐式转换" class="headerlink" title="隐式转换"></a>隐式转换</h1><h2 id="隐式函数"><a href="#隐式函数" class="headerlink" title="隐式函数"></a>隐式函数</h2><p><strong>说明：</strong>隐式转换可以在无需修改源代码的情况下，扩展某个类的功能。</p>
<p><strong>Demo：</strong></p>
<p>需求：通过隐式转换为 <code>Int</code> 类增加方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> test</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/23 11:18</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestImplicitFunction</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 隐式转换，接收一个 Int 类型的参数，转换成 MyRichInt 类型的参数</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">convert</span></span>(arg: <span class="type">Int</span>) : <span class="type">MyRichInt</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MyRichInt</span>(arg)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="number">2.</span>myMax(<span class="number">6</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRichInt</span>(<span class="params">val num: <span class="type">Int</span></span>)</span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">myMax</span></span>(x : <span class="type">Int</span>) = &#123;</span><br><span class="line">    <span class="keyword">if</span> (x &gt; num)  </span><br><span class="line">      x</span><br><span class="line">    <span class="keyword">else</span> </span><br><span class="line">      num</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="隐式参数"><a href="#隐式参数" class="headerlink" title="隐式参数"></a>隐式参数</h2><p>普通方法或者函数可以通过 <strong>implicit</strong> 关键字声明隐式参数，调用该方法时，就可以传入该参数，编译器会在相应的作用域寻找符合条件的隐式值。</p>
<p><strong>说明：</strong></p>
<ol>
<li> 同一个作用域中，相同类型的隐式值只能有一个</li>
<li> <strong>编译器按照隐式参数的类型去寻找对应类型的隐式值，与隐式值的名称无关</strong>。</li>
<li> 隐式参数优先于默认参数</li>
</ol>
<p><strong>Demo：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> test</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/23 11:27</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestImplicitParameter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> str: <span class="type">String</span> = <span class="string">&quot;hello world!&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这里的 implicit 用于声明使用隐式参数。即如果有隐式参数就使用，若无则不使用</span></span><br><span class="line">  <span class="comment">// &quot;good bey world!&quot; 是为参数 arg 提供的默认值</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hello</span></span>(<span class="keyword">implicit</span> arg: <span class="type">String</span> = <span class="string">&quot;good bey world!&quot;</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(arg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 在调用 hello 方法时，若不传递参数，由于 hello 方法声明使用了隐式值，</span></span><br><span class="line">    <span class="comment">// 则会从上下文中先查找是否有隐式值，若有则使用，若无则再考虑默认值</span></span><br><span class="line">    hello</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">good bey world!</span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br></pre></td></tr></table></figure>



<hr>
<h2 id="隐式类"><a href="#隐式类" class="headerlink" title="隐式类"></a>隐式类</h2><p>从 <code>Scala2.10</code> 开始提供了隐式类，使用 <code>implicit</code> 修饰类，隐式类非常强大，同样可以扩展类的功能，在集合中隐式类会发挥重要的作用。</p>
<p><strong>隐式类说明</strong></p>
<ol>
<li> 其所带的构造参数有且只能有一个</li>
<li> 隐式类必须被定义在“类”或“伴生对象”或“包对象”里，即隐式类不能是<strong>顶级的</strong>。</li>
</ol>
<p><strong>Demo</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/23 11:35</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *          隐式类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestImplicitClass</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 声明隐式类：</span></span><br><span class="line"><span class="comment">  *   该隐式类接收一个 Int 类型的参数，即在代码的上下文中，</span></span><br><span class="line"><span class="comment">  * 如果某个对象调用的方法的参数列表只有一个 Int 类型，在执行时</span></span><br><span class="line"><span class="comment">  * 就会检查是否是调用该隐式类中的方法。</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRichInt</span>(<span class="params">arg: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">myMax</span></span>(i: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (arg &lt; i) i <span class="keyword">else</span> arg</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">myMin</span></span>(i: <span class="type">Int</span>) = &#123;</span><br><span class="line">      <span class="keyword">if</span> (arg &lt; i) arg <span class="keyword">else</span> i</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="number">1.</span>myMax(<span class="number">3</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<hr>
<h2 id="隐式解析机制"><a href="#隐式解析机制" class="headerlink" title="隐式解析机制"></a>隐式解析机制</h2><p><strong>说明</strong></p>
<ol>
<li> 首先会在当前代码作用域下查找隐式实体（隐式方法、隐式类、隐式对象）。（一般是这种情况）</li>
<li> 如果第一条规则查找隐式实体失败，会继续在隐式参数的类型的作用域里查找。类型的作用域是指与<strong>该类型相关联的全部伴生对象</strong>以及<strong>该类型所在包的包对象</strong>。</li>
</ol>
<p><strong>Demo：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//（2）如果第一条规则查找隐式实体失败，会继续在隐式参数的类型的作用域里查找。类型的作用域是指与该类型相关联的全部伴生模块，</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestTransform</span> <span class="keyword">extends</span> <span class="title">PersonTrait</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//（1）首先会在当前代码作用域下查找隐式实体</span></span><br><span class="line">        <span class="keyword">val</span> teacher = <span class="keyword">new</span> <span class="type">Teacher</span>()</span><br><span class="line">        teacher.eat()</span><br><span class="line">        teacher.say()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Teacher</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">eat</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println(<span class="string">&quot;eat...&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PersonTrait</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PersonTrait</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 隐式类 : 类型1 =&gt; 类型2</span></span><br><span class="line">    <span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">Person5</span>(<span class="params">user:<span class="type">Teacher</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">say</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println(<span class="string">&quot;say...&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/06/Mahout/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/06/Mahout/" class="post-title-link" itemprop="url">Mahout</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-06 10:28:21" itemprop="dateCreated datePublished" datetime="2021-12-06T10:28:21+08:00">2021-12-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-12-07 11:32:54" itemprop="dateModified" datetime="2021-12-07T11:32:54+08:00">2021-12-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>  参考视频时间：</p>
<ul>
<li>  mahout01d==09：00</li>
<li></li>
</ul>
</blockquote>
<h2 id="传统工具的困境"><a href="#传统工具的困境" class="headerlink" title="传统工具的困境"></a>传统工具的困境</h2><ul>
<li>  处理数据量受限于内存，因此无法处理海量数据</li>
<li>  能处理海量数据的软件，却又缺乏有效快速专业的分析功能</li>
<li>  可以采用抽样等方法，但有局限性，比如对于聚类，推荐系统则无法使用抽样</li>
<li>  解决方向：<strong>Hadoop集群和Map-Reduce并行计算</strong></li>
</ul>
<p><strong>抽样的局限性</strong>，需要使用全量数据分析</p>
<p>使用全量数据进行建模。将模型求解出来后，将模型部署到真实的生产环境中去。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/04/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/04/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/" class="post-title-link" itemprop="url">装饰器模式</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-04 21:57:27" itemprop="dateCreated datePublished" datetime="2021-12-04T21:57:27+08:00">2021-12-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-12-05 10:23:18" itemprop="dateModified" datetime="2021-12-05T10:23:18+08:00">2021-12-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/" itemprop="url" rel="index"><span itemprop="name">装饰器模式</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<ul>
<li>  装饰者</li>
<li>  被装饰者</li>
<li>  装饰</li>
<li>  委托</li>
<li>  组合</li>
<li></li>
</ul>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/29/ZooKeeper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/11/29/ZooKeeper/" class="post-title-link" itemprop="url">ZooKeeper</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-11-29 21:37:18" itemprop="dateCreated datePublished" datetime="2021-11-29T21:37:18+08:00">2021-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-09-17 01:05:52" itemprop="dateModified" datetime="2022-09-17T01:05:52+08:00">2022-09-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ZooKeeper/" itemprop="url" rel="index"><span itemprop="name">ZooKeeper</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>  官网地址：<a target="_blank" rel="noopener" href="https://zookeeper.apache.org/">https://zookeeper.apache.org/</a></p>
</blockquote>
<h1 id="ZooKeeper-理论基础"><a href="#ZooKeeper-理论基础" class="headerlink" title="ZooKeeper 理论基础"></a>ZooKeeper 理论基础</h1><h2 id="ZooKeeper-工作机制"><a href="#ZooKeeper-工作机制" class="headerlink" title="ZooKeeper 工作机制"></a>ZooKeeper 工作机制</h2><p>从设计模式角度来看，Zookeeper 是一个基于<strong>观察者模式</strong>的分布式服务管理框架。</p>
<p>它负责存储和管理大家（多个服务）都关心的数据，并接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper 会将变化通知给那些订阅服务的观察者。</p>
<h2 id="ZooKeeper-特点"><a href="#ZooKeeper-特点" class="headerlink" title="ZooKeeper 特点"></a>ZooKeeper 特点</h2><p><strong>一主多从</strong></p>
<ul>
<li>Zookeeper 是由一个 Leader（领导者）和多个 Follower（跟随者）组成的集群。</li>
</ul>
<p><strong>一半一半</strong></p>
<ul>
<li>在 ZK 集群中，只要有半数以上节点存活，集群就能正常提供服务。所以 ZK 集群一般会安装奇数台服务器。比如 8 个节点的集群只有 4 个节点存活就不能正常运行，而 7 个节点的集群只需要 4 个节点存活就能正常运行。</li>
</ul>
<p><strong>全局数据一致（最终一致性）</strong></p>
<ul>
<li>每个服务器节点保存一份相同的数据副本，Client 无论连接到哪个节点数据都是一致的。</li>
</ul>
<p><strong>更新请求顺序执行</strong>（TODO）</p>
<ul>
<li>来自同一个 Client 的更新请求按其发送顺序依次执行。</li>
</ul>
<p><strong>数据更新原子性</strong></p>
<ul>
<li>一次数据更新要么成功，要么失败。</li>
</ul>
<p><strong>实时性</strong></p>
<ul>
<li><p>在一定时间范围内，Client 能读到最新数据。</p>
<p>  ​                                    </p>
</li>
</ul>
<h2 id="ZooKeeper-文件存储结构"><a href="#ZooKeeper-文件存储结构" class="headerlink" title="ZooKeeper 文件存储结构"></a>ZooKeeper 文件存储结构</h2><p>ZooKeeper 的文件存储结构有点类似于 Linux，可以看作是一棵树，从根路径 <code>/</code> 出发可以到达任意位置。每个节点称为一个 <strong>ZNode</strong>。每个 ZNode 默认能够存储 1MB 的数据，每个 ZNode 都可以通过其路径唯一标识。</p>
<p><img src="/2021/11/29/ZooKeeper/image-20211129222922413.png" alt="image-20211129222922413"></p>
<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>ZooKeeper 提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。</p>
<h3 id="软负载均衡-vs-硬负载均衡"><a href="#软负载均衡-vs-硬负载均衡" class="headerlink" title="软负载均衡 vs. 硬负载均衡"></a>软负载均衡 vs. 硬负载均衡</h3><p><strong>软负载均衡</strong>是指，假设集群中现有 3 台机器，A 机器承担了 60% 的流量，B 机器承担了 30% 的流量，C 机器只承担了 10% 的流量，即集群中功能相同的 3 台机器的负载不均衡，为了让负载能均衡一点，接下来的大部分请求就都会被分配到 C 机器上进行处理。所谓的软负载均衡，就是在软件层面，通过一些算法，让集群中多台机器承担的流量尽可能地相近。</p>
<p>而<strong>硬负载均衡</strong>则是在硬件上的突破，即如果某台机器承担的流量非常大，那么就新增一台机器去分担它的流量，达到负载均衡的目的。</p>
<hr>
<h1 id="ZK-Shell-客户端操作"><a href="#ZK-Shell-客户端操作" class="headerlink" title="ZK Shell 客户端操作"></a>ZK Shell 客户端操作</h1><ol>
<li><p>启动 ZK 客户端（启动客户端前必须先启动 ZK 服务器端）</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ bin/zkCli.sh -server localhost:2181</span><br><span class="line"></span><br><span class="line"><span class="comment"># 命令分析</span></span><br><span class="line">bin/zkCli.sh -server localhost:2181</span><br><span class="line">[脚本名称] -server [ZK服务器地址:端口号]</span><br><span class="line"><span class="comment"># 如果不加 `-server localhost:2181`，默认走的就是 localhost:2181 这个地址，</span></span><br><span class="line"><span class="comment"># 如果你自己的ZK服务不是这个地址，在启动ZK客户端时一定要显式指定ZK服务器端的通讯地址</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>ZK服务器集群中的任一节点都可以处理请求，所以Client将请求发送到集群中的任一节点都是OK的</strong></li>
</ul>
</li>
<li><p>查看帮助文档</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] <span class="built_in">help</span></span><br><span class="line">ZooKeeper -server host:port cmd args</span><br><span class="line">	addauth scheme auth</span><br><span class="line">	close <span class="comment"># 退出客户端，并关闭session</span></span><br><span class="line">	config [-c] [-w] [-s]</span><br><span class="line">	connect host:port</span><br><span class="line">	create [-s] [-e] [-c] [-t ttl] path [data] [acl]	<span class="comment"># 增</span></span><br><span class="line">	delete [-v version] path	<span class="comment"># 删</span></span><br><span class="line">	deleteall path	<span class="comment"># 删</span></span><br><span class="line">	delquota [-n|-b] path</span><br><span class="line">	get [-s] [-w] path	<span class="comment"># 查</span></span><br><span class="line">	getAcl [-s] path</span><br><span class="line">	<span class="built_in">history</span> </span><br><span class="line">	listquota path	<span class="comment"># 查</span></span><br><span class="line">	<span class="built_in">ls</span> [-s] [-w] [-R] path	<span class="comment"># 查</span></span><br><span class="line">	ls2 path [watch]</span><br><span class="line">	printwatches on|off</span><br><span class="line">	quit <span class="comment"># 退出客户端，但不关闭session</span></span><br><span class="line">	reconfig [-s] [-v version] [[-file path] | [-members serverID=host:port1:port2;port3[,...]*]] | [-add serverId=host:port1:port2;port3[,...]]* [-remove serverId[,...]*]</span><br><span class="line">	redo cmdno	</span><br><span class="line">	removewatches path [-c|-d|-a] [-l]</span><br><span class="line">	rmr path	<span class="comment"># 删</span></span><br><span class="line">	<span class="built_in">set</span> [-s] [-v version] path data	<span class="comment"># 改</span></span><br><span class="line">	setAcl [-s] [-v version] [-R] path acl</span><br><span class="line">	setquota -n|-b val path</span><br><span class="line">	<span class="built_in">stat</span> [-w] path	<span class="comment"># 查</span></span><br><span class="line">	<span class="built_in">sync</span> path</span><br><span class="line"></span><br><span class="line"><span class="comment">#============================================================</span></span><br><span class="line">-w	表示注册监听，不带-w默认不会注册为监听器</span><br></pre></td></tr></table></figure></li>
<li><p>查看当前 znode 中所包含的内容</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] <span class="built_in">ls</span> /</span><br><span class="line">[zookeeper]</span><br></pre></td></tr></table></figure></li>
<li><p>查看当前节点详细数据</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] <span class="built_in">ls</span> -s /</span><br><span class="line">[zookeeper]cZxid = 0x0</span><br><span class="line">ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid = 0x0</span><br><span class="line">cversion = -1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure></li>
<li><p>分别创建2个普通节点</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] create /my_node1 <span class="string">&quot;hello&quot;</span></span><br><span class="line">Created /my_node1</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] create /my_node2 <span class="string">&quot;hi&quot;</span></span><br><span class="line">Created /my_node2</span><br></pre></td></tr></table></figure>

</li>
<li><p>获得节点的值</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] get /my_node1</span><br><span class="line">hello1</span><br></pre></td></tr></table></figure>

</li>
<li><p>创建<strong>临时</strong>节点</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 5] create -e /tmp <span class="string">&quot;This is a temporary node&quot;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>（1）在当前客户端是可以看到的</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 9] <span class="built_in">ls</span> /</span><br><span class="line">[my_node1, my_node2, tmp, zookeeper]</span><br></pre></td></tr></table></figure></li>
<li><p>（2）退出当前客户端后再重启客户端</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 10] quit</span><br><span class="line">[lvnengdong@hadoop102 apache-zookeeper-3.5.7-bin]$ bin/zkCli.sh -server localhost:2181</span><br></pre></td></tr></table></figure></li>
<li><p>（3）再次查看发现该临时节点已经被删除了</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] <span class="built_in">ls</span> /</span><br><span class="line">[my_node1, my_node2, zookeeper]</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="常见的-Shell-客户端命令"><a href="#常见的-Shell-客户端命令" class="headerlink" title="常见的 Shell 客户端命令"></a>常见的 Shell 客户端命令</h3><table>
<thead>
<tr>
<th>命令基本语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>help</code></td>
<td>显示所有操作命令</td>
</tr>
<tr>
<td><code>ls path</code></td>
<td>使用 ls 命令来查看当前znode的子节点  -w 监听子节点变化  -s  附加次级信息</td>
</tr>
<tr>
<td><code>create</code></td>
<td>普通创建  -s 含有序列  -e 临时（重启或者超时消失）</td>
</tr>
<tr>
<td><code>get path</code></td>
<td>获得节点的值  -w 监听节点内容变化  -s  附加次级信息</td>
</tr>
<tr>
<td><code>set</code></td>
<td>设置节点的具体值</td>
</tr>
<tr>
<td><code>stat</code></td>
<td>查看节点状态</td>
</tr>
<tr>
<td><code>delete</code></td>
<td>删除节点</td>
</tr>
<tr>
<td><code>deleteall</code></td>
<td>递归删除节点</td>
</tr>
</tbody></table>
<h3 id="节点的状态和节点类型"><a href="#节点的状态和节点类型" class="headerlink" title="节点的状态和节点类型"></a>节点的状态和节点类型</h3><p>查看当前节点详细数据</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] <span class="built_in">ls</span> -s /my_node1</span><br><span class="line">[]cZxid = 0x6	</span><br><span class="line">ctime = Tue Nov 30 21:30:35 CST 2021</span><br><span class="line">mZxid = 0x6</span><br><span class="line">mtime = Tue Nov 30 21:30:35 CST 2021</span><br><span class="line">pZxid = 0x6</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 6</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li>  <code>c：create</code></li>
<li>  <code>Z：ZooKeeper</code></li>
<li>  <code>x：事务</code></li>
<li>  <code>m：modify，修改</code></li>
<li>  <code>p：parent</code></li>
</ul>
</blockquote>
<ul>
<li>  <code>cZxid</code>：创建该 ZooKeeper 节点时事务的 id（事务id是一个十六进制的数）。</li>
<li>  <code>ctime</code>：该 ZK 节点的创建时间</li>
<li>  <code>mZxid</code>：该 ZK 节点在修改时的事务 id</li>
<li>  <code>mtime</code>：该 ZK 节点的修改时间</li>
<li>  <code>pZxid</code>：当前节点中最新发生的创建子节点事务的 id。</li>
<li>  <code>cversion</code>：</li>
<li>  <code>dataVersion</code>：数据版本，当前节点中的数据发生变化后，该版本号就 +1。一般用于乐观锁。</li>
<li>  <code>aclVersion</code>：权限控制相关，每当节点的权限发生变化后，该版本号就 +1。</li>
<li>  <code>ephemeralOwner</code> ：该节点是否是一个临时节点。<code>0x0</code>表示当前节点是一个永久节点，非0表示当前节点是一个临时节点</li>
<li>  <code>numChildren</code>：当前节点的子节点个数</li>
</ul>
<h1 id="ZK-常见原理"><a href="#ZK-常见原理" class="headerlink" title="ZK 常见原理"></a>ZK 常见原理</h1><h2 id="监听器原理"><a href="#监听器原理" class="headerlink" title="监听器原理"></a>监听器原理</h2><p><img src="/2021/11/29/ZooKeeper/image-20211201102045752.png" alt="image-20211201102045752"></p>
<ol>
<li> 作为 ZooKeeper 的使用者，程序员在调用 ZK 时，首先会在 Main 线程中创建 ZK 客户端实例。ZK 客户端实例会维护两个线程 <strong>Listener</strong> 和 <strong>connect</strong>，一个负责与 ZK 服务器进行网络连接通信（connect），另一个负责监听（Listener）。</li>
<li> 客户端通过 connect 线程向 ZK 发送注册、监听请求，比如 <code>getChildren(&quot;/&quot;, true)</code>，这个请求的含义是获取根目录 <code>/</code> 下的所有子节点，第二个 boolean 类型的参数表示是否注册成为监听器，true 表示注册成为监听器，这时 ZK 集群就会把客户端信息（IP、port、监听路径、客户端名字等信息） 记录下来，当客户端监听的 path 发生变化时，就会通知客户端。【监听器机制】</li>
<li> 对于服务器而言，ZK 会将新注册的监听事件添加到监听列表中，当监听列表中的监听事件被触发时（也就是监听路径下的数据或目录结构发生了变化），就会将变化情况发送给客户端的 Listener 线程。</li>
<li> Listener 线程内部再调用 <code>process()</code> 方法进行相应的处理。</li>
</ol>
<p><strong>常见的监听：</strong></p>
<ol>
<li><p>监听节点数据的变化</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get path[watch]</span><br></pre></td></tr></table></figure>

</li>
<li><p>监听子节点增减的变化</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> path[watch]</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>注意：</strong></p>
<ul>
<li>  观察者只是单次有效的。</li>
</ul>
<hr>
<h2 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h2><p><strong>半数机制：</strong>只有当集群中半数<strong>以上</strong>的机器存活时，集群才可用。</p>
<p>ZooKeeper 虽然没有在配置文件中指定 Master 和 Slaver，但是 ZK 集群在运行时只能有一个节点为 Leader，其余节点均为 Follower，这个 Leader 是 ZK 运行时通过内部选举机制产生的。</p>
<p>下面以一个简单的例子来说明选举 Leader 的过程：</p>
<p>假设有五台服务器组成的 Zookeeper 集群，它们的 id 为从1~5，同时它们都是新建的，没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器按照 id 从小到大<strong>依次启动</strong>，来看看会发生什么。</p>
<p><img src="/2021/11/29/ZooKeeper/image-20211201195906285.png" alt="image-20211201195906285"></p>
<ol>
<li> <code>Server1</code> 率先启动，发起一次选举，并且投自己一票（默认每个服务器投票时都会优先选自己）。此时集群中只有一台机器，<code>Server1</code> 得到一票，但是总票数小于3票，不够半数以上，选举无法完成，服务器保持 Looking 状态。</li>
<li> <code>Server2</code> 启动，再发起一次选举。<code>Server1</code> 和 <code>Server2</code> 首先分别投自己一票并交换选举信息。此时<code>Server1</code> 发现<code>Server2</code> 的ID比自己目前投票推举的服务器id大，更改选票为推举<code>Server2</code> 。此时<code>Server1</code> 得 0 票，<code>Server2</code> 得 2 票，没有半数以上的结果。选举无法完成。服务器1、2状态保持 Looking。</li>
<li> <code>Server3</code> 启动，发起一次选举，此时<code>Server1</code> 和 <code>Server2</code> 在交换完选票信息后都会更改选票为 <code>Server3</code> 。此次投票结束后：<code>Server1</code> 为0票，<code>Server2</code> 为0票，<code>Server3</code> 为3票，此时 <code>Server3</code>  的票数已经超过半数，<code>Server3</code> 当选为 Leader，服务器1、2更改状态为 Following，<code>Server3</code> 更改状态为 Leading。</li>
<li> <code>Server4</code> 启动，发起一次选举，此时服务器1、2、3已经不是 Looking 状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票，此时服务器4服从多数，更改选票信息为服务器3，并更改状态为Following。</li>
<li> 服务器5启动，同服务器4一样当小弟。</li>
</ol>
<hr>
<h2 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h2><p><img src="/2021/11/29/ZooKeeper/image-20211202102918703.png" alt="image-20211202102918703"></p>
<p>ZooKeeper 并没有实现读写分离。</p>
<ol>
<li> Client 向 ZK 集群中的 <code>Server1</code> 发送写请求，如果 <code>Server1</code> 不是 Leader，那么 <code>Server1</code> 首先会把接收到的请求进一步转发给 Leader。</li>
<li> Leader 会将写请求广播到集群中的每个节点，各个节点都会将这个写请求加入到待写队列，并向 Leader 发送写成功的 ACK。</li>
<li> 当 Leader 收到半数以上节点的成功信息，就说明该写操作可以执行成功。Leader 会继续向各个节点发送提交信息，各个 Server 收到该消息后会落实待写队列中的写请求，此时写成功。如果 Leader 没有收到半数以上的成功信息，各个节点中待写队列中对应的数据在一定时间后会被清除。</li>
<li> <code>Server1</code> 会进一步通知 Client 数据写成功了，这时候就认为整个写操作成功。</li>
</ol>
<hr>
<h1 id="ZK-Java客户端操作"><a href="#ZK-Java客户端操作" class="headerlink" title="ZK Java客户端操作"></a>ZK Java客户端操作</h1><h2 id="创建Maven项目，导入依赖"><a href="#创建Maven项目，导入依赖" class="headerlink" title="创建Maven项目，导入依赖"></a>创建Maven项目，导入依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.5.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--这个版本号最好和你安装的ZooKeeper的版本号一致--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h2 id="API-测试"><a href="#API-测试" class="headerlink" title="API 测试"></a>API 测试</h2><h3 id="创建-ZooKeeper客户端"><a href="#创建-ZooKeeper客户端" class="headerlink" title="创建 ZooKeeper客户端"></a>创建 ZooKeeper客户端</h3><blockquote>
<p><strong>ZooKeeper的一个构造函数：</strong></p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 构造函数</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ZooKeeper</span> <span class="keyword">implements</span> <span class="title class_">AutoCloseable</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> connectString ZK集群的地址</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> sessionTimeout client与ZK服务器会话超时时间 </span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> watcher 观察者实例，ZK持有观察者实例以便在监听事件发生时通知观察者</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">ZooKeeper</span><span class="params">(String connectString, <span class="type">int</span> sessionTimeout, Watcher watcher)</span></span><br><span class="line">        <span class="keyword">throws</span> IOException</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">this</span>(connectString, sessionTimeout, watcher, <span class="literal">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>测试</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ZKTest</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 格式：`IP地址:端口号`，如果ZK是一个集群，多个地址之间用逗号隔开</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">connectString</span> <span class="operator">=</span> <span class="string">&quot;hadoop101:2181, hadoop102:2181,&quot;</span>;</span><br><span class="line">    <span class="comment">// 会话超时时间（单位：ms）</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">sessionTimeout</span> <span class="operator">=</span> <span class="number">10000</span>;</span><br><span class="line">    <span class="comment">// 观察者实例，一旦被观察的path发生了变更，服务端就会通知观察者（客户端），观察者收到通知后就会触发回调，执行process()方法</span></span><br><span class="line">    <span class="type">Watcher</span> <span class="variable">watcher</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line">			System.out.println(<span class="string">&quot;您观察的路径已经发生了变化&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="type">ZooKeeper</span> <span class="variable">zkClient</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 创建ZK客户端对象</span></span><br><span class="line">    <span class="meta">@BeforeEach</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 创建一个ZK客户端对象</span></span><br><span class="line">        zkClient = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(connectString, sessionTimeout, watcher);</span><br><span class="line">        Thread.sleep(<span class="number">2000</span>);	<span class="comment">// 让主线程睡眠一段时间，确保TCP连接建立成功后再往下执行</span></span><br><span class="line">        <span class="keyword">if</span> (zkClient != <span class="literal">null</span>) System.out.println(<span class="string">&quot;init success&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 使用完毕后释放 zkClient</span></span><br><span class="line">    <span class="meta">@AfterEach</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="keyword">if</span> (zkClient != <span class="literal">null</span>)&#123;</span><br><span class="line">            zkClient.close();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">&quot;end&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="可能会出现的异常"><a href="#可能会出现的异常" class="headerlink" title="可能会出现的异常"></a>可能会出现的异常</h3><p>在创建了一个客户端对象后，就可以通过这个客户端对象去调用对应的 API 查看 ZK 中的一些信息了，但是在测试环境中，我们一般需要在创建完客户端之后等待几秒再去调用其它的 API。因为 <code>new ZooKeeper()</code> 语句只是创建出了一个 ZKClient 实例，只是建立了客户端与服务端之间的会话，但是此时 TCP 连接可能还未建立完成，如果这时向 ZooKeeper 集群发出操作命令的话就可能出现连接丢失异常，虽然这种概率相对较小。</p>
<p>所以一般情况下创建完 ZKClient 实例后，我们需要等待几秒钟再去执行其它操作就不会出问题了。</p>
<h3 id="创建子节点"><a href="#创建子节点" class="headerlink" title="创建子节点"></a>创建子节点</h3><p><strong>源码：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> path 节点的位置</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> data[] 节点中的数据，以 byte[] 数组存储</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> acl 权限列表</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> createMode 节点的类型</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> String <span class="title function_">create</span><span class="params">(<span class="keyword">final</span> String path, <span class="type">byte</span> data[], List&lt;ACL&gt; acl,CreateMode createMode)</span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>测试：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException, IOException &#123;</span><br><span class="line">   </span><br><span class="line">    <span class="type">String</span> <span class="variable">path</span> <span class="operator">=</span> <span class="string">&quot;/idea&quot;</span>;</span><br><span class="line">    <span class="type">byte</span>[] data = <span class="string">&quot;HelloZooKeeper&quot;</span>.getBytes();</span><br><span class="line">    List&lt;ACL&gt; acl = ZooDefs.Ids.OPEN_ACL_UNSAFE; <span class="comment">//ZooDefs.Ids是一个枚举类，从中选择了最高权限`OPEN_ACL_UNSAFE`赋值给acl</span></span><br><span class="line">    <span class="type">CreateMode</span> <span class="variable">createMode</span> <span class="operator">=</span> CreateMode.PERSISTENT; <span class="comment">//CreateMode也是一个枚举类，这里选择了节点类型为永久节点</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> zkClient.create(path, data, acl, createMode);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="获取子节点并监听节点变化"><a href="#获取子节点并监听节点变化" class="headerlink" title="获取子节点并监听节点变化"></a>获取子节点并监听节点变化</h3><p><strong>源码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment">	getChildren 方法用于查看某一路径下的所有节点，该方法可以向ZK服务器传递</span></span><br><span class="line"><span class="comment">	一个监听器(watcher)对象，此后如果Client监听的路径下的数据发生了变化时，</span></span><br><span class="line"><span class="comment">	ZK服务器就会触发回调，执行watcher中的process()方法。</span></span><br><span class="line"><span class="comment">	</span></span><br><span class="line"><span class="comment">	监听器既可以使用ZK客户端默认给定的watcher实例，也可以自定义一个watch实例。</span></span><br><span class="line"><span class="comment">	</span></span><br><span class="line"><span class="comment">	该方法存在多个重载方法</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment">	如果第二个参数传递一个 Boolean 类型的值，如果为true监听器对象就会</span></span><br><span class="line"><span class="comment">	自动注册成为ZKClient创建时指定的监听器，如果为 false 监听器对象就</span></span><br><span class="line"><span class="comment">	为 null，表示不需要监听路径path</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> List&lt;String&gt; <span class="title function_">getChildren</span><span class="params">(String path, <span class="type">boolean</span> watch)</span></span><br><span class="line">    <span class="keyword">throws</span> KeeperException, InterruptedException &#123;</span><br><span class="line">    <span class="keyword">return</span> getChildren(path, watch ? watchManager.defaultWatcher : <span class="literal">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果想要使用自定义的监听器，也可以直接传入一个监听器对象</span></span><br><span class="line"><span class="keyword">public</span> List&lt;String&gt; <span class="title function_">getChildren</span><span class="params">(<span class="keyword">final</span> String path, Watcher watcher)</span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>测试：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ls</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">ls</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException, IOException &#123;</span><br><span class="line">   </span><br><span class="line">    System.out.println(zkClient);</span><br><span class="line">    List&lt;String&gt; children = zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="literal">null</span>);</span><br><span class="line">    System.out.println(children);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="判断-ZNode-是否存在"><a href="#判断-ZNode-是否存在" class="headerlink" title="判断 ZNode 是否存在"></a>判断 ZNode 是否存在</h3><p><strong>源码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用 ZKClient 默认的监听器</span></span><br><span class="line"><span class="keyword">public</span> Stat <span class="title function_">exists</span><span class="params">(String path, <span class="type">boolean</span> watch)</span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用自定义的监听器，返回结果直接回调给观察者</span></span><br><span class="line"><span class="keyword">public</span> Stat <span class="title function_">exists</span><span class="params">(<span class="keyword">final</span> String path, Watcher watcher)</span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>测试</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">exist</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">ZooKeeper</span> <span class="variable">zkClient</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(connectString, sessionTimeout, watcher);</span><br><span class="line">    <span class="type">Stat</span> <span class="variable">stat</span> <span class="operator">=</span> zkClient.exists(<span class="string">&quot;/idea&quot;</span>, <span class="literal">false</span>);</span><br><span class="line">    System.out.println(stat == <span class="literal">null</span> ? <span class="string">&quot;不存在&quot;</span> : <span class="string">&quot;存在&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="使用-JavaAPI-设置观察者"><a href="#使用-JavaAPI-设置观察者" class="headerlink" title="使用 JavaAPI 设置观察者"></a>使用 JavaAPI 设置观察者</h2><p>在 ZK 提供的客户端 API 中，有一些可以设置观察者，有一些不能设置观察者。我们以部分方法为例展示观察者模式的使用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testObserver</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">// 由connect线程调用</span></span><br><span class="line">    zkClient = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(connectString, sessionTimeout, watcher);</span><br><span class="line"></span><br><span class="line">    List&lt;String&gt; list = zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">        <span class="comment">// 由Listener线程调用</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line">            <span class="comment">// 当 `/` 路径下的内容发生变化时，就会触发该方法</span></span><br><span class="line">        zkClient.getChildren(<span class="string">&quot;/&quot;</span>, (event) -&gt; System.out.println(<span class="string">&quot;路径 &quot;</span> + event.getPath() + <span class="string">&quot;  发生了变化&quot;</span>));</span><br><span class="line">        <span class="comment">/*注意：对于这段代码，仅能监听到根目录`/`下文件的变更，如果根目录下已经有了`/idea`目录，你在`/idea`目录中新增一个文件是无法被感知到的*/</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 为了保证客户端能收到回调信息，ZKClient进程要一直运行，不能关闭</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line">        Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;我还活着&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>此时，当 ZK 中根目录 <code>/</code> 下的内容发生变化时，就会触发回调，客户端的回调函数 <code>process()</code> 方法就会被执行。</p>
<p><strong>注意：</strong></p>
<ul>
<li>一般情况下，我们不会使用 ZKClient 中默认的监听器，因为让多种不同的操作执行同一个监听器的逻辑是不合理的，一般情况下我们会针对每种情况都设置自己的监听器。</li>
</ul>
<hr>
<h2 id="持续监听"><a href="#持续监听" class="headerlink" title="持续监听"></a>持续监听</h2><p>在 ZK 中，观察者默认模式是<strong>单次有效</strong>的，也就是说，当监听路径下的内容第一次发生改变时，会触发监听事件，但是当监听事件触发过之后，该路径下的内容再次发生变化时就不会触发监听事件了。如果想要每次内容发生变化时都触发监听事件，就需要设置持续监听。</p>
<p><strong>持续监听原理：</strong></p>
<ol>
<li> ZKClient 在向服务器发送请求时，可以设置一个观察者 watcher；</li>
<li> 当 ZKClient 监听的路径发生变化时，服务器会调用 ZKClient 的回调方法 <code>process()</code></li>
<li> 如果我们想要实现持续监听功能的话，只需要在 <code>process()</code> 中重新向 ZK 服务器发送监听请求并设置一个新的监听器就可以了。</li>
</ol>
<p><strong>错误写法：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 持续监听【错误写法】</span></span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testObserving</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">       <span class="comment">// 由connect线程调用</span></span><br><span class="line">       List&lt;String&gt; list = zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">           <span class="comment">// 由Listener线程调用</span></span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line">               <span class="comment">// 当 `/` 路径下的内容发生变化时，就会触发该方法</span></span><br><span class="line">               System.out.println(<span class="string">&quot;路径 &quot;</span> + event.getPath() + <span class="string">&quot;  发生了变化&quot;</span>);</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   <span class="comment">// 递归调用，重新设置观察者</span></span><br><span class="line">                   testObserving();</span><br><span class="line">               &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       <span class="comment">// 为了保证客户端能收到回调信息，ZKClient进程要一直运行，不能关闭</span></span><br><span class="line">       <span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line">           Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">           System.out.println(<span class="string">&quot;我还活着&quot;</span>);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p>我们来分析一下上面这段代码：</p>
<ul>
<li>  当ZK服务器 <code>/</code> 路径下的内容发生变化时，就会触发zkClient的回调函数，执行 <code>process()</code> 方法，在该方法中会递归地调用 <code>testObserving()</code> 方法，进行重新向 ZK 服务器发送一个带有新的观察者的请求。看起来一切都很美好，持续监听的功能也能够实现。但是这段代码在实际运行时并不能得到想要的结果。</li>
</ul>
<p>原因如下：</p>
<ol>
<li> 第一次调用 <code>testObserving()</code> 方法的线程是 connect 线程，在本例中也就是 main 线程，为了保证客户端能够收到回调信息，main线程会一直保持运行。</li>
<li> 但是 <code>process()</code> 方法是由 Listener 线程执行的，也就是说第二次调用 <code>testObserving()</code> 方法是由 Listener 线程执行的，那么 Listener 线程在执行到第 22 行的时候，就会陷入死循环。</li>
<li> ZK 服务器端数据发生变化时需要通知 Listener 线程来执行 <code>process()</code> 方法，而此时的 Listener 线程陷入了死循环中，也就是说无法收到服务器的回调，自然也就无法继续执行 <code>process()</code> 方法了，所以上面的代码并不能实现持续监听的效果。</li>
</ol>
<p><strong>正确写法：</strong></p>
<p>想要解决上面的问题，我们只要让 Listen 线程不陷入死循环就可以解决了。</p>
<p>解决方法很简单，缩小递归方法的范围即可。把包含 Listener 线程的方法单独抽取到一个独立方法中就可以了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 持续监听【正确写法】</span></span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testObserving</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">       observing();</span><br><span class="line">       </span><br><span class="line">       <span class="comment">// 为了保证客户端能收到回调信息，ZKClient进程要一直运行，不能关闭</span></span><br><span class="line">       <span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line">           Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">           System.out.println(<span class="string">&quot;我还活着&quot;</span>);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">observing</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException &#123;</span><br><span class="line">       <span class="comment">// 由connect线程调用</span></span><br><span class="line">       zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">           <span class="comment">// 由Listener线程调用</span></span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line">               <span class="comment">// 当 `/` 路径下的内容发生变化时，就会触发该方法</span></span><br><span class="line">               System.out.println(<span class="string">&quot;路径 &quot;</span> + event.getPath() + <span class="string">&quot;  发生了变化&quot;</span>);</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   <span class="comment">// 递归调用，重新设置观察者</span></span><br><span class="line">                   testObserving();</span><br><span class="line">               &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>





<h2 id="监听服务器节点动态上下线案例"><a href="#监听服务器节点动态上下线案例" class="headerlink" title="监听服务器节点动态上下线案例"></a>监听服务器节点动态上下线案例</h2><p><strong>需求</strong>：</p>
<p>某分布式系统中，有多台服务器，可以动态上下线，要求任意一台客户端都能实时感知到服务器节点的上下线。</p>
<p><strong>需求分析</strong>：</p>
<p>想要客户端能动态监控服务器节点的上线、下线，那么要求服务器节点一定不能是永久节点，对于永久节点来说，一旦创建了就会被持久化，即使下线了在 ZK 集群中也不会失去这个节点的信息。所以需要把服务器节点设置称为临时节点，这样当服务器上/下线的时候，ZK 集群就会动态的创建/删除节点。</p>
<p><img src="/2021/11/29/ZooKeeper/image-20211213115812008.png" alt="image-20211213115812008"></p>
<p><strong>具体实现</strong></p>
<ol>
<li><p>先在集群上创建 <code>/servers</code> 目录</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 5] create /servers &quot;servers&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>服务器端向 Zookeeper 注册的代码</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.xsyu.zoo_keeper_demo.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> lnd</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2021/12/13 12:06</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Version</span> 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 服务器节点</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 需求：</span></span><br><span class="line"><span class="comment"> *      每次启动后，在执行自己的核心业务之前，需要先向ZK集群注册一个临时节点，</span></span><br><span class="line"><span class="comment"> *      并向临时节点中保存一些关键信息</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Server</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 格式：`IP地址:端口号`，如果ZK是一个集群，多个地址之间用逗号隔开</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">connectString</span> <span class="operator">=</span> <span class="string">&quot;hadoop102:2181, hadoop103:2181,&quot;</span>;</span><br><span class="line">    <span class="comment">// session的超时时间（单位：ms）</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">sessionTimeout</span> <span class="operator">=</span> <span class="number">10000</span>;</span><br><span class="line">    <span class="comment">// 监听器线程对象，一旦watcher观察的path发生了变更，服务端就会通知客户端，客户端收到通知后就会自动调用process()方法</span></span><br><span class="line">    <span class="type">Watcher</span> <span class="variable">watcher</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="type">String</span> <span class="variable">basePath</span> <span class="operator">=</span> <span class="string">&quot;/servers&quot;</span>;</span><br><span class="line">    <span class="type">ZooKeeper</span> <span class="variable">zkClient</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化客户端对象</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建一个ZK客户端对象</span></span><br><span class="line">        zkClient = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(connectString, sessionTimeout, watcher);</span><br><span class="line">        System.out.println(zkClient);</span><br><span class="line">        System.out.println(<span class="string">&quot;init&quot;</span>);</span><br><span class="line">        Thread.sleep(<span class="number">3000</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;创建ZKClient成功&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 使用zkClient在ZK集群上为Server注册临时节点，</span></span><br><span class="line"><span class="comment">     * 并向临时节点中保存一些关键信息</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">register</span><span class="params">(String info)</span> <span class="keyword">throws</span> KeeperException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 创建带有序号的临时节点</span></span><br><span class="line">        <span class="comment">// 创建出来的节点类似于： /servers/server1;    /servers/server2 这种</span></span><br><span class="line">        zkClient.create(basePath+<span class="string">&quot;/server&quot;</span>, info.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line">        System.out.println(<span class="string">&quot;注册节点成功&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Server节点其它的业务功能</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doOthers</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 让Server持续运行</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;working....&quot;</span>);</span><br><span class="line">            Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 主方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Server</span> <span class="variable">server</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Server</span>();</span><br><span class="line">        <span class="comment">// 创建ZK客户端对象</span></span><br><span class="line">        server.init();</span><br><span class="line">        <span class="comment">// 将Server节点注册到ZK中</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">info</span> <span class="operator">=</span> <span class="string">&quot;我是Sever1，我的库存服务节点&quot;</span>;</span><br><span class="line">        server.register(info);</span><br><span class="line">        <span class="comment">// Server执行其它的业务功能</span></span><br><span class="line">        server.doOthers();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
<li><p>客户端代向 Zookeeper 监听服务器上下线情况的代码</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.xsyu.zoo_keeper_demo.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.KeeperException;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> lnd</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2021/12/13 15:42</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Version</span> 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 从ZK集群持续监听Server节点的变化，一旦有变化，重新获取Server进程的信息</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Client</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 格式：`IP地址:端口号`，如果ZK是一个集群，多个地址之间用逗号隔开</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">connectString</span> <span class="operator">=</span> <span class="string">&quot;hadoop102:2181, hadoop103:2181,&quot;</span>;</span><br><span class="line">    <span class="comment">// session的超时时间（单位：ms）</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">sessionTimeout</span> <span class="operator">=</span> <span class="number">10000</span>;</span><br><span class="line">    <span class="comment">// 监听器线程对象，一旦watcher观察的path发生了变更，服务端就会通知客户端，客户端收到通知后就会自动调用process()方法</span></span><br><span class="line">    <span class="type">Watcher</span> <span class="variable">watcher</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="type">String</span> <span class="variable">basePath</span> <span class="operator">=</span> <span class="string">&quot;/servers&quot;</span>;</span><br><span class="line">    <span class="type">ZooKeeper</span> <span class="variable">zkClient</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化客户端对象</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建一个ZK客户端对象</span></span><br><span class="line">        zkClient = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(connectString, sessionTimeout, watcher);</span><br><span class="line">        System.out.println(zkClient);</span><br><span class="line">        System.out.println(<span class="string">&quot;init&quot;</span>);</span><br><span class="line">        Thread.sleep(<span class="number">3000</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;创建ZKClient成功&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 持续监听当前启动的Server进程有哪些，获取到Server进程的信息</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;String&gt; <span class="title function_">getInfo</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; children = zkClient.getChildren(basePath, <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line">                System.out.println(event.getPath() + <span class="string">&quot;路径下发生了以下事件&quot;</span> + event.getType());</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">// 递归，持续监听</span></span><br><span class="line">                    getInfo();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取每个节点（server）中保存的信息</span></span><br><span class="line">        ArrayList&lt;String&gt; info = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">            <span class="type">byte</span>[] data = zkClient.getData(basePath + <span class="string">&quot;/&quot;</span> + child, <span class="literal">null</span>, <span class="literal">null</span>);</span><br><span class="line">            info.add(<span class="keyword">new</span> <span class="title class_">String</span>(data));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;最新读到的信息是:&quot;</span> + info);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> info;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 其它业务功能</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doOthers</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 让Client持续运行</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;working....&quot;</span>);</span><br><span class="line">            Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Client</span> <span class="variable">client</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Client</span>();</span><br><span class="line">        <span class="comment">// 同样首先也要先创建ZKClient，并向ZK集群注册监听事件</span></span><br><span class="line">        client.init();</span><br><span class="line">        <span class="comment">// 获取数据</span></span><br><span class="line">        client.getInfo();</span><br><span class="line">        <span class="comment">// client 的其它工作（保持client不会关闭）</span></span><br><span class="line">        client.doOthers();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  





</body>
</html>
