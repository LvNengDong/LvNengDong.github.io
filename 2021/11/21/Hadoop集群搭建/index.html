<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Hadoop 运行环境搭建安装前的注意事项 Hadoop项目是用Java语言实现的，Hadoop运行时依赖JDK相关的类库，所以安装Hadoop前必须保证当前机器已经安装了JDK，并且配置了 JAVA_HOME 全局变量。  Hadoop 集群本质上通过多个不同类型的进程配合工作，如 NameNode，DataNode，ResourceManager，NodeManager 等，这些进程往往部署在">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop集群搭建">
<meta property="og:url" content="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Hadoop 运行环境搭建安装前的注意事项 Hadoop项目是用Java语言实现的，Hadoop运行时依赖JDK相关的类库，所以安装Hadoop前必须保证当前机器已经安装了JDK，并且配置了 JAVA_HOME 全局变量。  Hadoop 集群本质上通过多个不同类型的进程配合工作，如 NameNode，DataNode，ResourceManager，NodeManager 等，这些进程往往部署在">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122102444283.png">
<meta property="og:image" content="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122104518498.png">
<meta property="og:image" content="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122105055584.png">
<meta property="og:image" content="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122105640671.png">
<meta property="og:image" content="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122173553541.png">
<meta property="og:image" content="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122173729381.png">
<meta property="og:image" content="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122174007581.png">
<meta property="og:image" content="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122183127209.png">
<meta property="og:image" content="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122194846938.png">
<meta property="article:published_time" content="2021-11-21T03:57:51.000Z">
<meta property="article:modified_time" content="2022-02-16T02:32:21.443Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122102444283.png">


<link rel="canonical" href="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","path":"2021/11/21/Hadoop集群搭建/","title":"Hadoop集群搭建"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Hadoop集群搭建 | Hexo</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hexo</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Hadoop-%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="nav-number">1.</span> <span class="nav-text">Hadoop 运行环境搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E5%89%8D%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">1.1.</span> <span class="nav-text">安装前的注意事项</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%99%AE%E9%80%9A%E7%94%A8%E6%88%B7%E6%93%8D%E4%BD%9C-Hadoop"><span class="nav-number">1.2.</span> <span class="nav-text">使用普通用户操作 Hadoop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Hadoop"><span class="nav-number">1.3.</span> <span class="nav-text">安装 Hadoop</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hadoop-%E7%9A%84%E7%9B%AE%E5%BD%95%E4%BB%8B%E7%BB%8D"><span class="nav-number">2.</span> <span class="nav-text">Hadoop 的目录介绍</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hadoop-%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F"><span class="nav-number">3.</span> <span class="nav-text">Hadoop 运行模式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS-%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE"><span class="nav-number">3.1.</span> <span class="nav-text">HDFS 运行模式配置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MapReduce-%E7%9A%84%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F"><span class="nav-number">3.2.</span> <span class="nav-text">MapReduce 的运行模式</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hadoop-%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">4.</span> <span class="nav-text">Hadoop 的配置文件</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E4%B8%AA%E9%BB%98%E8%AE%A4%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">4.1.</span> <span class="nav-text">四个默认配置文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E4%B8%AA%E7%94%A8%E6%88%B7%E8%87%AA%E5%AE%9A%E4%B9%89%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">4.2.</span> <span class="nav-text">四个用户自定义配置文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E4%BD%BF%E7%94%A8%E5%B0%8F%E6%8A%80%E5%B7%A7"><span class="nav-number">4.3.</span> <span class="nav-text">配置文件使用小技巧</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%AC%E5%9C%B0-HDFS-%E4%BD%BF%E7%94%A8%E6%B5%8B%E8%AF%95"><span class="nav-number">5.</span> <span class="nav-text">本地 HDFS 使用测试</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F-HDFS-%E4%BD%BF%E7%94%A8%E6%B5%8B%E8%AF%95"><span class="nav-number">6.</span> <span class="nav-text">分布式 HDFS 使用测试</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F-HDFS-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="nav-number">6.1.</span> <span class="nav-text">分布式 HDFS 常用命令</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8%E4%B8%8E%E5%81%9C%E6%AD%A2"><span class="nav-number">6.1.1.</span> <span class="nav-text">启动与停止</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B-NameNode-DataNode-%E8%BF%9B%E7%A8%8B%E7%8A%B6%E6%80%81"><span class="nav-number">6.1.2.</span> <span class="nav-text">查看 NameNode&#x2F;DataNode 进程状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%BC%E5%BC%8F%E5%8C%96-Namenode"><span class="nav-number">6.1.3.</span> <span class="nav-text">格式化 Namenode</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NameNode-%E9%80%9A%E4%BF%A1%E5%9C%B0%E5%9D%80%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90"><span class="nav-number">6.2.</span> <span class="nav-text">NameNode 通信地址原理分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NameNode-%E7%94%9F%E6%88%90%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE%E7%9A%84%E7%9B%AE%E5%BD%95%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90"><span class="nav-number">6.3.</span> <span class="nav-text">NameNode 生成保存数据的目录原理分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E6%93%8D"><span class="nav-number">6.4.</span> <span class="nav-text">实操</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95"><span class="nav-number">6.5.</span> <span class="nav-text">测试</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%AC%E5%9C%B0-MapReduce-%E6%B5%8B%E8%AF%95"><span class="nav-number">7.</span> <span class="nav-text">本地 MapReduce 测试</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9C%A8-YARN-%E4%B8%8A%E8%BF%90%E8%A1%8C-MapReduce"><span class="nav-number">8.</span> <span class="nav-text">在 YARN 上运行 MapReduce</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1%E3%80%81%E4%BF%AE%E6%94%B9-MapReduce-%E7%9A%84%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F"><span class="nav-number">8.1.</span> <span class="nav-text">1、修改 MapReduce 的运行模式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2%E3%80%81%E5%90%AF%E5%8A%A8-Yarn"><span class="nav-number">8.2.</span> <span class="nav-text">2、启动 Yarn</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3%E3%80%81%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1"><span class="nav-number">8.3.</span> <span class="nav-text">3、提交任务</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">235</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">70</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Hadoop集群搭建 | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Hadoop集群搭建
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-11-21 11:57:51" itemprop="dateCreated datePublished" datetime="2021-11-21T11:57:51+08:00">2021-11-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-02-16 10:32:21" itemprop="dateModified" datetime="2022-02-16T10:32:21+08:00">2022-02-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="Hadoop-运行环境搭建"><a href="#Hadoop-运行环境搭建" class="headerlink" title="Hadoop 运行环境搭建"></a>Hadoop 运行环境搭建</h1><h2 id="安装前的注意事项"><a href="#安装前的注意事项" class="headerlink" title="安装前的注意事项"></a>安装前的注意事项</h2><ol>
<li><p>Hadoop项目是用Java语言实现的，Hadoop运行时依赖JDK相关的类库，所以安装Hadoop前必须保证当前机器已经安装了JDK，并且配置了 <code>JAVA_HOME</code> 全局变量。</p>
</li>
<li><p>Hadoop 集群本质上通过多个不同类型的进程配合工作，如 NameNode，DataNode，ResourceManager，NodeManager 等，这些进程往往部署在不同的机器上，并且进程之间需要互相通信。既然要通信时就需要知道彼此的 IP 地址和端口号，为了方便记忆和书写，我们往往会选择使用主机名进行通信，这时候就需要修改 <code>hosts</code> 文件配置主机名到 IP 的映射了。当然直接使用 IP 进行通信的话就无需配置主机名到 IP 的映射了。</p>
<ul>
<li>  host 文件在不同系统下的位置：</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="quote">&gt; Linux：/etc/hosts</span></span><br><span class="line"><span class="quote">&gt; Windows：C:\Windows\System32\drivers\etc\hosts</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>文件权限</strong>：Hadoop 在运行时会产生很多数据（包括 HDFS 存储的数据和每个节点的日志文件等），对于保存这些数据的目录，我们必须设置让当前启动 Hadoop 进程的用户拥有写权限。</p>
</li>
<li><p>关闭防火墙，并且关闭防火墙开机自启。</p>
</li>
</ol>
<hr>
<h2 id="使用普通用户操作-Hadoop"><a href="#使用普通用户操作-Hadoop" class="headerlink" title="使用普通用户操作 Hadoop"></a>使用普通用户操作 Hadoop</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. 创建普通用户 Xxx</span><br><span class="line">    useradd Xxx</span><br><span class="line">2. 为 Xxx 设置密码</span><br><span class="line">    passwd 123456</span><br><span class="line">3. 赋予 Xxx 用户 root 权限</span><br><span class="line">    vim /etc/sudoers</span><br><span class="line">4. 将 /opt 目录下创建的 software 目录和 module 目录的属主改为 Xxx</span><br><span class="line">    <span class="built_in">chown</span> -R Xxx:Xxx /opt/software /opt/module</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="安装-Hadoop"><a href="#安装-Hadoop" class="headerlink" title="安装 Hadoop"></a>安装 Hadoop</h2><ol>
<li><p> 将 <code>hadoop-2.7.2.tar.gz</code> 导入到 <code>opt/software</code> 目录下；</p>
</li>
<li><p>解压安装文件到 <code>/opt/module</code> 下面；</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.7.2.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li>
<li><p><strong>将 HADOOP_HOME 添加到环境变量</strong></p>
<ul>
<li>  （1）获取 Hadoop 安装路径</li>
</ul>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure>

<ul>
<li><p>（2）编辑 <code>/etc/profile.d/my_env.sh</code> 文件</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure></li>
<li><p>（3）在文件末尾添加 <code>HADOOP_HOME</code></p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>测试配置是否生效</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop version</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h1 id="Hadoop-的目录介绍"><a href="#Hadoop-的目录介绍" class="headerlink" title="Hadoop 的目录介绍"></a>Hadoop 的目录介绍</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 module]$ ll hadoop-2.7.2/</span><br><span class="line">总用量 28</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong   194 5月  22 2017 bin</span><br><span class="line">drwxr-xr-x. 3 lvnengdong lvnengdong    20 5月  22 2017 etc</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong   106 5月  22 2017 include</span><br><span class="line">drwxr-xr-x. 3 lvnengdong lvnengdong    20 5月  22 2017 lib</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong   239 5月  22 2017 libexec</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong 15429 5月  22 2017 LICENSE.txt</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   101 5月  22 2017 NOTICE.txt</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong  1366 5月  22 2017 README.txt</span><br><span class="line">drwxr-xr-x. 2 lvnengdong lvnengdong  4096 5月  22 2017 sbin</span><br><span class="line">drwxr-xr-x. 4 lvnengdong lvnengdong    31 5月  22 2017 share</span><br></pre></td></tr></table></figure>

<ul>
<li>  <strong>bin</strong>：存放对 Hadoop 相关服务（HDFS、MR 和 YARN ）进行操作的脚本。</li>
<li>  <strong>sbin</strong> ：存放管理员启动和停止集群时使用的命令脚本。</li>
<li>  <strong>etc</strong>：存放 Hadoop 的配置文件。</li>
<li>  <strong>lib</strong>：存放 Hadoop 的本地库</li>
<li>  <strong>share</strong>：存放 Hadoop 的依赖 jar 包、文件和官方案例。</li>
</ul>
<hr>
<h1 id="Hadoop-运行模式"><a href="#Hadoop-运行模式" class="headerlink" title="Hadoop 运行模式"></a>Hadoop 运行模式</h1><p>Hadoop 提供了两类三种运行模式，分别是：</p>
<ul>
<li>  <strong>本地模式</strong></li>
<li><strong>分布式模式</strong><ul>
<li>  伪分布式模式：如果 NN 和 DN 都在一台机器，且只有一个 DN 节点，称为伪分布式！</li>
<li>  完全分布式模式</li>
</ul>
</li>
</ul>
<p>这两种运行模式本质上其实就是 HDFS 的运行模式，分别对应了：</p>
<ul>
<li>  1️⃣在本地机器上使用 HDFS，即使用本地机器的文件系统。 </li>
<li>  2️⃣在多台机器上使用 HDFS，即使用了一个分布式的文件系统。</li>
</ul>
<p>Hadoop 安装后默认使用的本地模式，如果想要使用分布式模式，需要修改对应的配置文件中的配置参数。</p>
<h2 id="HDFS-运行模式配置"><a href="#HDFS-运行模式配置" class="headerlink" title="HDFS 运行模式配置"></a>HDFS 运行模式配置</h2><p>HDFS 采用哪种运行模式，取决于配置参数 <strong>fs.defaultFS</strong>，该参数要在 <code>core-default.xml</code> 配置文件中进行配置。默认使用的是本地模式。</p>
<p><strong>本地模式</strong></p>
<ul>
<li><p>  概念：使用当前计算机的文件系统作为 HDFS 的文件系统。</p>
</li>
<li><p>参数：</p>
  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- core-default.xml --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- fs.defaultFS=file:///    （默认）【file:/// 是一种本地文件系统协议】 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>分布式模式</strong></p>
<ul>
<li><p>  概念：要使用的文件系统是一个分布式的文件系统！一个分布式的文件系统，必须由 NameNode，DataNode 等若干进程共同运行完成文件系统的读写操作！</p>
</li>
<li><p>参数：</p>
  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- core-default.xml --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- fs.defaultFS=hdfs://     【hdfs:// 代表一种分布式文件系统协议】 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h2 id="MapReduce-的运行模式"><a href="#MapReduce-的运行模式" class="headerlink" title="MapReduce 的运行模式"></a>MapReduce 的运行模式</h2><ol>
<li><p>按照 MR 的规范编写一个程序；</p>
</li>
<li><p>将程序打成一个 jar 包</p>
</li>
<li><p>运行 jar 包。</p>
<p>运行 jar 包可以选择两种模式，分别是：1️⃣本地模式；2️⃣在 Yarn 上运行（将 jar 包提交给 Yarn，由 Yarn 调度资源完成运算）。具体使用哪一种模式由参数由参数 <code>mapreduce.framework.name</code> 决定，该参数也需要在 <code>core-default.xml</code> 配置文件中进行配置。默认值是：<code>mapreduce.framework.name=local</code>，表示在本地模式下运行。</p>
</li>
</ol>
<p><strong>MapReduce 的两种运行方式</strong></p>
<ol>
<li><p><strong>本地模式</strong>：在本地运行 MR，即在本机使用多线程的方式模拟多个 Task 的运行。对应的在  <code>mapred-default.xml</code> 配置文件中的配置信息为：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- mapred-default.xml --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- mapreduce.framework.name=local --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>local<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>在 YARN 上运行</strong>：需要启动 YARN，YARN 由 RM 和 NM 进程组成，所以还需要启动 RM 和 NM。将 MR 生成的 Job 提交给 YARN ，由 YARN 负责将 Job 中拆分成多个 Task 并分配到多台机器中运算。对应的在  <code>mapred-default.xml</code> 配置文件中的配置信息为：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- mapred-default.xml --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- mapreduce.framework.name=yarn --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>local<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h1 id="Hadoop-的配置文件"><a href="#Hadoop-的配置文件" class="headerlink" title="Hadoop 的配置文件"></a>Hadoop 的配置文件</h1><p>同大多数的项目一样，Hadoop 安装后，为我们提供了 4 个默认的配置文件，通过加载默认配置文件已经可以正常启动一个 Hadoop 项目。但是，如果想要使用自定义配置信息的话，有两种方法：</p>
<ul>
<li>  <strong>方式一</strong>：Hadoop 同时也提供了 4 个自定义配置文件支持用户使用自定义的配置文件覆盖默认配置信息。Hadoop 在启动时，会先加载 4 个默认的配置文件，再加载用户自定义的配置文件，如果自定义的配置文件中出现了和默认配置文件中同名的参数，就会覆盖默认配置文件中的值。</li>
<li>  <strong>方式二</strong>：通过显式指定配置参数的方法启动Hadoop，这种方法的弊端是仅在当前次有效，当集群重启后还是会以配置文件中的配置信息为准。</li>
</ul>
<h2 id="四个默认配置文件"><a href="#四个默认配置文件" class="headerlink" title="四个默认配置文件"></a>四个默认配置文件</h2><p> <strong>默认配置文件所在目录：</strong>默认配置文件直接内嵌在Hadoop依赖的类库中，具体目录为：</p>
<ul>
<li>  <code>$HADOOP_HOME/share</code>：该目录下保存 着 Hadoop 启动时需要加载的所有 jar 包。</li>
<li>  <code>$HADOOP_HOME/share/hadoop/.../xxx.jar/xxx-default.xml</code> ：默认配置文件的保存路径，4个配置文件分别保存在4个不同的 jar 包的类路径下</li>
</ul>
<p><strong>默认配置文件：</strong></p>
<ul>
<li><strong>core-default.xml</strong>    保存 Hadoop 最核心的配置参数</li>
<li><strong>hdfs-default.xml</strong>    保存 HDFS 相关的参数</li>
<li><strong>mapred-default.xm</strong>l   保存 MapReduce 相关的参数</li>
<li><strong>yarn-default.xml</strong>    保存 YARN 相关的参数</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop-common-2.7.7.jar  ==&gt; core-default.xml <span class="comment"># core-default.xml 配置文件保存在  hadoop-common-2.7.2.jar 这个jar包中</span></span><br><span class="line">hadoop-hdfs-2.7.2.jar ==&gt; hdfs-default.xml</span><br><span class="line">hadoop-mapreduce-client-core-2.7.2.jar ==&gt; mapred-default.xml</span><br><span class="line">hadoop-yarn-common-2.7.2.jar ==&gt; yarn-default.xml</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="四个用户自定义配置文件"><a href="#四个用户自定义配置文件" class="headerlink" title="四个用户自定义配置文件"></a>四个用户自定义配置文件</h2><p><strong>注意事项：</strong></p>
<ul>
<li>  4个用户自定义配置文件必须命名为 <code>xxx-site.xml</code>，只有这样<strong>用户自定义的配置文件才会覆盖默认配置文件中的同名参数。</strong></li>
<li>  Hadoop 在启动时，首先会加载 4 个默认的配置文件，再加载用户自定义的配置文件，如果用户自定义的配置文件中有和默认配置文件中同名的参数，可以覆盖之前已经加载的值！</li>
</ul>
<p><strong>自定义配置文件所在目录：</strong><code>$HADOOP_HOME/etc/hadoop/</code> </p>
<p><strong>自定义配置文件：</strong></p>
<ul>
<li>  <strong>core-site.xml</strong>    保存用户自定义的 Hadoop 最核心的配置参数</li>
<li>  <strong>hdfs-site.xml</strong>    保存用户自定义的 HDFS 相关的参数</li>
<li>  <strong>mapred-site.xml</strong>    保存用户自定义的 MapReduce 相关的参数</li>
<li>  <strong>yarn-site.xml</strong>    保存用户自定义的 YARN 相关的参数</li>
</ul>
<hr>
<h2 id="配置文件使用小技巧"><a href="#配置文件使用小技巧" class="headerlink" title="配置文件使用小技巧"></a>配置文件使用小技巧</h2><p><strong>小技巧1：</strong></p>
<p>在 <code>$HADOOP_HOME/etc/hadoop/</code> 目录下保存了四个自定义配置信息的模板，我们在需要使用自定义配置的时候可以直接拷贝这几个配置文件并加以修改使用。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ ll <span class="variable">$HADOOP_HOME</span>/etc/hadoop/ | grep site.xml</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   774 5月  22 2017 core-site.xml</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   775 5月  22 2017 hdfs-site.xml</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   620 5月  22 2017 httpfs-site.xml</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong  5511 5月  22 2017 kms-site.xml</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   758 5月  22 2017 mapred-site.xml.template</span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong   690 5月  22 2017 yarn-site.xml</span><br></pre></td></tr></table></figure>





<p><strong>小技巧2</strong></p>
<ol>
<li><p><strong>在使用 Hadoop 命令时可以手动指定加载哪个目录下的配置文件</strong></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在Linux终端中键入 `hadoop`，会提示如下信息 </span></span><br><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop</span><br><span class="line">Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]</span><br><span class="line"></span><br><span class="line"><span class="comment"># [--config confdir] 表示在使用 hadoop 命令时可以手动指定要加载哪个目录中的自定义配置文件</span></span><br></pre></td></tr></table></figure>

<ul>
<li>  <strong>如果未显式指定配置文件的目录，默认读取 <code>$HADOOP_HOME/etc/hadoop</code> 目录下对应的自定义配置文件！</strong></li>
</ul>
</li>
</ol>
<hr>
<h1 id="本地-HDFS-使用测试"><a href="#本地-HDFS-使用测试" class="headerlink" title="本地 HDFS 使用测试"></a>本地 HDFS 使用测试</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用：hadoop fs 命令 文件路径</span><br><span class="line"><span class="comment"># 表示使用 hadoop 的 file system 执行操作</span></span><br></pre></td></tr></table></figure>

<ul>
<li>  <code>hadoop fs</code> 常见命令</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop fs</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-<span class="built_in">cat</span> [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-checksum &lt;src&gt; ...]</span><br><span class="line">	[-<span class="built_in">chgrp</span> [-R] GROUP PATH...]</span><br><span class="line">	[-<span class="built_in">chmod</span> [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">	[-<span class="built_in">chown</span> [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">	[-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-count [-q] [-h] &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">cp</span> [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">	[-<span class="built_in">df</span> [-h] [&lt;path&gt; ...]]</span><br><span class="line">	[-<span class="built_in">du</span> [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">	[-expunge]</span><br><span class="line">	[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">	[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">	[-getmerge [-<span class="built_in">nl</span>] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-<span class="built_in">help</span> [cmd ...]]</span><br><span class="line">	[-<span class="built_in">ls</span> [-d] [-h] [-R] [&lt;path&gt; ...]]</span><br><span class="line">	[-<span class="built_in">mkdir</span> [-p] &lt;path&gt; ...]</span><br><span class="line">	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-<span class="built_in">mv</span> &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]	<span class="comment"># 上传文件，&lt;localsrc&gt;表示文件源地址；&lt;dst&gt;表示目的地址</span></span><br><span class="line">	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">	[-<span class="built_in">rm</span> [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">	[-<span class="built_in">rmdir</span> [--ignore-fail-on-non-empty] &lt;<span class="built_in">dir</span>&gt; ...]</span><br><span class="line">	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--<span class="built_in">set</span> &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">stat</span> [format] &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">tail</span> [-f] &lt;file&gt;]</span><br><span class="line">	[-<span class="built_in">test</span> -[defsz] &lt;path&gt;]</span><br><span class="line">	[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-touchz &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">truncate</span> [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">	[-usage [cmd ...]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<ol>
<li><p>创建一个 testUpload 文件用于测试文件上传</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ vim /opt/module/hadoop-2.7.2/etc/hadoop/testUpload</span><br><span class="line"></span><br><span class="line"><span class="comment"># testUpload 文件内容为：</span></span><br><span class="line">测试文件上传</span><br></pre></td></tr></table></figure></li>
<li><p>在当前目录创建一个保存自定义配置文件的目录 <code>myconfig</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> myconfig</span><br></pre></td></tr></table></figure></li>
<li><p>运行 <code>hadoop fs</code> 命令上传文件</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop --config myconfig fs -put testUpload /</span><br><span class="line"><span class="comment"># 表示加载 myconfig 目录下的配置文件，完成将当前目录下的 testUpload 文件上传到 HDFS 的根目录（/）下</span></span><br><span class="line"><span class="comment"># 因为在本地模式下HDFS使用的就是当前Linux系统的文件系统，所以 / 就表示当前Linux系统的根目录</span></span><br></pre></td></tr></table></figure></li>
<li><p>报错信息分析</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># core-site.xml not found，因为我们创建的 myconfig 目录是一个空目录，没有任何的自定义配置文件，</span></span><br><span class="line"><span class="comment"># 而如果配置了 --config，则至少需要有一个自定义的 core-site.xml 文件。</span></span><br><span class="line"><span class="comment"># 解决办法是在 myconfig 目录下创建一个 core-site.xml 自定义配置文件</span></span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">&quot;main&quot;</span> java.lang.RuntimeException: core-site.xml not found</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2566)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2492)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.<span class="built_in">set</span>(Configuration.java:1143)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.<span class="built_in">set</span>(Configuration.java:1115)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1451)</span><br><span class="line">	at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:321)</span><br><span class="line">	at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:487)</span><br><span class="line">	at org.apache.hadoop.util.GenericOptionsParser.&lt;init&gt;(GenericOptionsParser.java:170)</span><br><span class="line">	at org.apache.hadoop.util.GenericOptionsParser.&lt;init&gt;(GenericOptionsParser.java:153)</span><br><span class="line">	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:64)</span><br><span class="line">	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)</span><br><span class="line">	at org.apache.hadoop.fs.FsShell.main(FsShell.java:340)</span><br></pre></td></tr></table></figure></li>
<li><p>将 <code>$HADOOP_HOME/etc/hadoop/</code> 目录下的 <code>core-site.xml</code> 配置文件复制到 <code>myconfig</code> 目录下</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ <span class="built_in">cp</span> <span class="variable">$HADOOP_HOME</span>/etc/hadoop/core-site.xml myconfig/</span><br></pre></td></tr></table></figure></li>
<li><p>重新执行文件上传命令</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop --config myconfig fs -put testUpload /</span><br><span class="line">put: /testUpload._COPYING_ (权限不够)</span><br></pre></td></tr></table></figure></li>
<li><p> 报错信息分析：因为当前使用的是本地模式，也就是说 HDFS 使用 Linux 的本地文件系统，而我们当前登录的用户对 HDFS 的根目录 <code>/</code> 是没有操作权限的，我们可以将该文件上传到一个当前登录用户拥有权限的目录下，该命令就可以正常执行了。</p>
</li>
<li><p>重新执行上传命令</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop --config myconfig fs -put testUpload /opt/module/</span><br><span class="line">[lvnengdong@hadoop102 hadoop]$ <span class="built_in">echo</span> $?</span><br><span class="line">0	<span class="comment"># 表示上一条命令执行成功</span></span><br></pre></td></tr></table></figure></li>
<li><p>执行结果验证</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ ll /opt/module/</span><br><span class="line">总用量 4</span><br><span class="line"><span class="comment"># ..</span></span><br><span class="line">-rw-r--r--. 1 lvnengdong lvnengdong  19 11月 21 20:47 testUpload</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 /opt/module/ 目录下多出了一个 testUpload，表示文件上传执行成功了</span></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h1 id="分布式-HDFS-使用测试"><a href="#分布式-HDFS-使用测试" class="headerlink" title="分布式 HDFS 使用测试"></a>分布式 HDFS 使用测试</h1><h2 id="分布式-HDFS-常用命令"><a href="#分布式-HDFS-常用命令" class="headerlink" title="分布式 HDFS 常用命令"></a>分布式 HDFS 常用命令</h2><p>在分布式 HDFS 中，需要额外管理 NameNode、DataNode 等，所以针对 NameNode、DataNode 等有额外的操作命令。</p>
<h3 id="启动与停止"><a href="#启动与停止" class="headerlink" title="启动与停止"></a>启动与停止</h3><p><strong>NameNode</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动Namenode	</span></span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止Namenode	</span></span><br><span class="line">hadoop-daemon.sh stop namenode</span><br></pre></td></tr></table></figure>



<p><strong>DataNode</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动datanode	</span></span><br><span class="line">hadoop-daemon.sh start datanode</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止datanode</span></span><br><span class="line">hadoop-daemon.sh stop datanode</span><br></pre></td></tr></table></figure>

<ul>
<li>  上述四条命令都是 sbin 目录中的脚本【sbin 已经被添加到环境变量中了】</li>
</ul>
<hr>
<h3 id="查看-NameNode-DataNode-进程状态"><a href="#查看-NameNode-DataNode-进程状态" class="headerlink" title="查看 NameNode/DataNode 进程状态"></a>查看 NameNode/DataNode 进程状态</h3><ol>
<li><p><strong>jps命令</strong>    </p>
<ul>
<li>  因为 Hadoop 是用 Java 实现的，所以可以直接用 <code>jps</code> 命令查看 NameNode 和 DataNode 进程的运行状态。</li>
</ul>
</li>
<li><p>通过<strong>浏览器访问</strong></p>
<ul>
<li>  Hadoop 给我们提供了可视化的管理界面，可以访问该页面查看 HDFS 中相关进程（包括 NameNode 和 DataNode）的运行状态。</li>
</ul>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 访问地址</span></span><br><span class="line">http://&#123;NameNode所在主机的IP地址&#125;:50070</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过浏览器访问是通过 http 协议访问的，而 NameNode 的 http 协议访问端口号是 50070</span></span><br><span class="line"><span class="comment"># 当前还可以通过 RPC 协议来访问 NameNode，RPC 访问端口号是我们在配置文件中自定义的，一般都设置为 9000</span></span><br><span class="line"><span class="comment"># DataNode 被 NameNode 管理，所以在 NameNode 的可视化管理界面可以查看 DataNode 线程的信息</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h3 id="格式化-Namenode"><a href="#格式化-Namenode" class="headerlink" title="格式化 Namenode"></a>格式化 Namenode</h3><p><strong>命令：</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure>

<p><strong>注意：格式化 Namenode 只需要执行一次</strong></p>
<p><strong>作用：</strong>格式化时，首先会根据配置文件 <code>core-default.xml</code> 或 <code>core-site.xml</code> 中的配置信息生成用于保存 NameNode 和 DataNode 数据的目录；并在这个目录中生成一些版本验证文件。</p>
<p><strong>Demo：</strong></p>
<ol>
<li><p> 使用 <code>hadoop namenode -format</code> 命令自动创建的保存 NameNode 和 DataNode 数据的目录；</p>
</li>
<li><p> 并使用 <code>tree</code> 命令查看这个命令的目录结构。</p>
</li>
<li><p>我们发现在这个目录下会多出一个 <strong>dfs</strong> 目录，<strong>name</strong> 目录用于保存 NameNode 节点的数据，并且继续深入还会有一些目录和文件，这些目录和文件用于进行版本验证，表明当前目录就是一个 NameNode 保存运行时数据的目录。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ tree /opt/module/hadoop-2.7.2/data/</span><br><span class="line">/opt/module/hadoop-2.7.2/data/</span><br><span class="line">└── dfs</span><br><span class="line">    └── name	<span class="comment"># name 目录用于保存 NameNode 节点的数据文件</span></span><br><span class="line">        ├── current</span><br><span class="line">        │   ├── edits_inprogress_0000000000000000001</span><br><span class="line">        │   ├── fsimage_0000000000000000000</span><br><span class="line">        │   ├── fsimage_0000000000000000000.md5</span><br><span class="line">        │   ├── seen_txid</span><br><span class="line">        │   └── VERSION</span><br><span class="line">        └── in_use.lock</span><br></pre></td></tr></table></figure></li>
<li><p>由于当前我们还没有启动 DataNode 节点，当启动 DataNode 节点并向该节点上传数据后，该目录下会新出现一个 <code>./dfs/data/...</code> 目录用于存储 DataNode 运行时产生的数据。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ tree /opt/module/hadoop-2.7.2/data/</span><br><span class="line">/opt/module/hadoop-2.7.2/data/</span><br><span class="line">└── dfs</span><br><span class="line">    ├── data	<span class="comment"># data 目录用于保存 DataNode 节点的数据文件</span></span><br><span class="line">    │   ├── current</span><br><span class="line">    │   │   ├── BP-401557870-192.168.1.102-1637506411781</span><br><span class="line">    │   │   │   ├── current</span><br><span class="line">    │   │   │   │   ├── finalized</span><br><span class="line">    │   │   │   │   │   └── subdir0</span><br><span class="line">    │   │   │   │   │       └── subdir0</span><br><span class="line">    │   │   │   │   │           ├── blk_1073741825	<span class="comment"># block块的id，与浏览器中展示的块的id一致</span></span><br><span class="line">    │   │   │   │   │           └── blk_1073741825_1001.meta</span><br><span class="line">    │   │   │   │   ├── rbw</span><br><span class="line">    │   │   │   │   └── VERSION</span><br><span class="line">    │   │   │   ├── scanner.cursor</span><br><span class="line">    │   │   │   └── tmp</span><br><span class="line">    │   │   └── VERSION</span><br><span class="line">    │   └── in_use.lock</span><br><span class="line">    └── name	<span class="comment"># name 目录用于保存 NameNode 节点的数据文件</span></span><br><span class="line">        ├── current</span><br><span class="line">        │   ├── edits_inprogress_0000000000000000001</span><br><span class="line">        │   ├── fsimage_0000000000000000000</span><br><span class="line">        │   ├── fsimage_0000000000000000000.md5</span><br><span class="line">        │   ├── seen_txid</span><br><span class="line">        │   └── VERSION</span><br><span class="line">        └── in_use.lock</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="NameNode-通信地址原理分析"><a href="#NameNode-通信地址原理分析" class="headerlink" title="NameNode 通信地址原理分析"></a>NameNode 通信地址原理分析</h2><p>HDFS（Hadoop分布式文件系统）就是一个服务器集群，HDFS 集群是由一个 NameNode 和多个 DataNode 组成的，其中 NameNode 负责保存元数据信息、<strong>与 DataNode 通信</strong>以及<strong>与客户端通信</strong>。既然要通信，那么就必须明确两点：</p>
<ol>
<li> <strong>通信协议：</strong>在 Hadoop 中，客户端与 NameNode（服务器端）， NameNode 和 DataNode 之间通信采用的都是 RPC 协议，并且如果使用 HDFS 文件传输的话，还需要满足 <code>hdfs://</code> 文件传输协议。</li>
<li> <strong>通信地址：</strong>NameNode 在这三者中处于通信的核心地位，既要和 DataNode 通信，又要和客户端通信，所以 NameNode 必须对外暴露自己的通信地址，让客户端和 NameNode 可以找到自己。又因为 NameNode 是一个线程，唯一定位一个线程需要 IP 地址和端口号两个因素。</li>
</ol>
<p>在 <code>core-site.xml</code> 配置文件中，<code>fs.defaultFS</code> 属性配置的其实就是 NameNode 的通信地址，用于让客户端和 DataNode 能够找到自己，NameNode 进程的通信地址由 IP 地址和端口号组成。如果不配置端口号的话每次启动线程时 NameNode 所在的服务器都会随机分配一个端口号给 NameNode 进程，不方便客户端和 DataNode 查找到 NameNode，所以我们一般会将 NameNode 的端口号写死，这样每次启动 NameNode 时使用的就都是同一端口号了。</p>
<hr>
<h2 id="NameNode-生成保存数据的目录原理分析"><a href="#NameNode-生成保存数据的目录原理分析" class="headerlink" title="NameNode 生成保存数据的目录原理分析"></a>NameNode 生成保存数据的目录原理分析</h2><p><strong>手动指定保存数据的目录</strong></p>
<p>Hadoop 在运行期间，HDFS 中的 NameNode 节点中需要一定的磁盘空间来保存元数据，DataNode 节点需要一定的磁盘空间来保存真正的数据，在默认配置文件 <code>core-default.xml</code> 中，设置了将每个节点会数据都保存在当前服务器的 <code>/tmp/hadoop-用户名/dfs</code> 目录下，而我们当前的服务器上可能并没有这个目录，所以一定要事先将这个目录创建好。</p>
<p>由于 <code>/tmp</code> 目录是 Linux 系统中的一个临时目录，每 15 天会自动清空一次，所以为了防止数据丢失，我们一般会重新指定这个目录。这时就需要在自定义的配置文件 <code>hdfs-core.xml</code> 中重写配置信息覆盖 <strong>hadoop.tmp.dir</strong> 属性。</p>
<ul>
<li>  默认配置文件 <code>core-default.xml</code> 中的配置信息</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">&lt;!-- core-default.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/hadoop-$&#123;user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>A base for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>  自定义配置文件 <code>core-site.xml</code> 中的配置信息</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">     配置 HDFS 框架在运行时产生的数据的存储目录。</span></span><br><span class="line"><span class="comment">	（1）NameNode 上保存的元数据等信息,</span></span><br><span class="line"><span class="comment">	（2）DataNode 上保存的真实数据信息</span></span><br><span class="line"><span class="comment">  都保存在该目录下。</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p><strong>使用格式化指令创建保存数据的目录</strong></p>
<p>在上面的配置文件中，我们设置了 HDFS 中数据的保存目录。所以我们必须事先在 NameNode 节点的服务器上创建这个目录后才能成功启动 NameNode。<strong>但是</strong>这个文件并不是一个空目录，也就是说如果我们通过 <code>mkdir</code> 命令创建的空目录是无法识别为能够保存 HDFS 数据的目录，能够保存 HDFS 数据的目录中必须有一个版本验证文件，如果想要创建一个包含版本验证文件的目录，我们需要通过 Hadoop 提供的格式化工具<strong>（hadoop namenode -format）</strong>生成一个目录，并且会自动在这个目录下生成一个版本号。</p>
<hr>
<h2 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h2><ol>
<li> 如果想要使用分布式的 HDFS，首先需要在自定义配置文件 <code>core-site.xml</code> 中重写默认配置文件中的  <code>fs.defaultFS</code> 属性为 <code>hdfs://</code>。它的含义是设置文件传输协议为 hdfs，表示 FileSystem 为 hdfs 分布式文件系统。</li>
<li> 同时要设置 NameNode 的唯一通信地址为：<code>hadoop102:9000</code>（IP地址 + 端口号）</li>
<li> 二者结合一下就变成了 <code>hdfs://hadoop102:9000</code></li>
<li> 在 <code>core-site.xml</code> 中重写 NameNode 和 DataNode 保存数据的目录</li>
<li> 使用 Hadoop 提供的格式化命令 <code>hadoop namenode -format</code> 创建这个保存数据的目录。</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">    1、使用分布式文件存储系统</span></span><br><span class="line"><span class="comment">    2、通知DataNode和客户端NameNode的唯一地址</span></span><br><span class="line"><span class="comment">    [Hadoop 集群中的各个进程之间需要互相通信，通过 RPC 方式通信：</span></span><br><span class="line"><span class="comment">        （1）客户端与 NN 直接通信；</span></span><br><span class="line"><span class="comment">        （2）DN 与 NN 也要进行通信。</span></span><br><span class="line"><span class="comment">    所以我们必须在集群中每台机器的配置文件 core-site.xml 中配置 NN 的通信地址]</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="comment">&lt;!-- hadoop102:主机名到IP地址的映射</span></span><br><span class="line"><span class="comment">	 9000：NameNode进程在RPC协议下的端口号（随便取，只要不被占用即可）</span></span><br><span class="line"><span class="comment"> --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">     配置 HDFS 框架在运行时产生的数据的存储目录。</span></span><br><span class="line"><span class="comment">	（1）NameNode 上保存的元数据等信息,</span></span><br><span class="line"><span class="comment">	（2）DataNode 上保存的真实数据信息</span></span><br><span class="line"><span class="comment">  都保存在该目录下。</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>







<hr>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol>
<li><p>首先，修改上一章节使用过的自定义配置文件 <code>./myconfig/core-site.xml</code> 。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ vim myconfig/core-site.xml</span><br></pre></td></tr></table></figure></li>
<li><p>添加自定义的 NameNode 配置信息。</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>使用自定义的配置文件启动 NameNode</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop-daemon.sh --config /opt/module/hadoop-2.7.2/etc/hadoop/myconfig/ start namenode</span><br><span class="line"><span class="comment"># 启动后的输出的信息</span></span><br><span class="line">starting namenode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-lvnengdong-namenode-hadoop102.out</span><br><span class="line"><span class="comment"># 这条信息表示，启动 namenode 后，所有的日志信息都会保存在这个目录下，目录的组成信息为：</span></span><br><span class="line"><span class="comment"># $HADOOP_HOME/logs/hadoop-用户名-进程名-主机名.out</span></span><br><span class="line"><span class="comment"># out 表示日志输出时的配置文件，真正的日志其实保存在 logs 目录下的 `xxx.log` 文件中</span></span><br><span class="line">2021-11-21 21:56:35,921 INFO  [main] namenode.NameNode (LogAdapter.java:info(47)) - STARTUP_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">STARTUP_MSG: Starting NameNode</span><br><span class="line">STARTUP_MSG:   host = hadoop102/192.168.1.102</span><br><span class="line">STARTUP_MSG:   args = []</span><br><span class="line">STARTUP_MSG:   version = 2.7.2</span><br><span class="line">......</span><br></pre></td></tr></table></figure></li>
<li><p>查看进程是否启动成功。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ jps</span><br><span class="line"><span class="comment"># 发现未启动成功</span></span><br></pre></td></tr></table></figure></li>
<li><p>查看日志信息</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印日志文件的后100行信息</span></span><br><span class="line">[lvnengdong@hadoop102 logs]$ <span class="built_in">tail</span> -n 100 hadoop-lvnengdong-namenode-hadoop102.out</span><br></pre></td></tr></table></figure></li>
<li><p>storage directory does not exist or is not accessible ==&gt;存储目录不存在或不可访问。</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lvnengdong/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:<span class="number">327</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:<span class="number">215</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:<span class="number">975</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:<span class="number">681</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:<span class="number">584</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:<span class="number">644</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:<span class="number">811</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:<span class="number">795</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:<span class="number">1488</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:<span class="number">1554</span>)</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">异常分析：</span></span><br><span class="line"><span class="comment">	因为我们在自定义的配置文件中未重写 NameNode 和 DataNode 保存数据的目录，所以在项目启动时</span></span><br><span class="line"><span class="comment">	使用的仍然是默认配置文件中的配置，也就是说会把数据保存在 /tmp/hadoop-用户名/dfs/ 目录下，</span></span><br><span class="line"><span class="comment">	但是该目录在当前服务器上并不存在，所以才会报错“storage directory does not exist or is not accessible”</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></li>
<li><p>在自定义的配置文件 <code>hdfs-core.xml</code> 中添加配置信息覆盖 <strong>hadoop.tmp.dir</strong> 属性。</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>并创建 <code>/opt/module/hadoop-2.7.2/data</code> 目录</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 myconfig]$ <span class="built_in">mkdir</span> /opt/module/hadoop-2.7.2/data</span><br></pre></td></tr></table></figure></li>
<li><p>重新启动 NameNode</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 myconfig]$ hadoop-daemon.sh --config /opt/module/hadoop-2.7.2/etc/hadoop/myconfig/ start namenode</span><br></pre></td></tr></table></figure></li>
<li><p> 使用 jps 命令查看发现仍然没有 NameNode 进程；</p>
</li>
<li><p>查看日志：发现错误信息仍然显示保存数据的目录没有创建，但是我们确实创建了。</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /opt/<span class="keyword">module</span>/hadoop-<span class="number">2.7</span><span class="number">.2</span>/data/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:<span class="number">327</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:<span class="number">215</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:<span class="number">975</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:<span class="number">681</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:<span class="number">584</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:<span class="number">644</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:<span class="number">811</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:<span class="number">795</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:<span class="number">1488</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:<span class="number">1554</span>)</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">异常分析：</span></span><br><span class="line"><span class="comment">	这个错误是因为目前我们虽然创建了和配置文件中一致的目录来保存NameNode和DataNode运行时产生的数据，</span></span><br><span class="line"><span class="comment">	但是由于这个目录是我们通过 mkdir 手动创建的，是一个空目录，该目录中并没有版本验证文件，所以无法</span></span><br><span class="line"><span class="comment">	证明这个目录就是用来保存 NameNode 和 DataNode 数据的目录。为了解决这个问题，我们还需要通过</span></span><br><span class="line"><span class="comment">	Hadoop提供的格式化工具来自动根据配置文件生成带版本号的可以保存NameNode生产数据的目录。</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></li>
<li><p>先删除我们自己创建的 <code>/opt/module/hadoop-2.7.2/data</code> 目录</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ <span class="built_in">rm</span> -rf data</span><br></pre></td></tr></table></figure></li>
<li><p>使用 <strong>hadoop namenode -format</strong> 格式化工具自动生成带版本号文件的目录</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop --config /opt/module/hadoop-2.7.2/etc/hadoop/myconfig/ namenode -format</span><br></pre></td></tr></table></figure></li>
<li><p>我们发现在执行完这条命令后会根据配置文件自动帮我们创建一个 <strong>data</strong> 目录，并在 <code>data</code> 目录下自动帮我们创建了一个 <strong>dfs</strong> 文件，这个 <code>dfs</code> 文件就是存储 NameNode 和 DataNode 运行时数据的目录。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ ll data/</span><br><span class="line">总用量 0</span><br><span class="line">drwxrwxr-x. 3 lvnengdong lvnengdong 18 11月 21 22:53 dfs</span><br></pre></td></tr></table></figure></li>
<li><p>重新启动 NameNode</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 myconfig]$ hadoop-daemon.sh --config /opt/module/hadoop-2.7.2/etc/hadoop/myconfig/ start namenode</span><br></pre></td></tr></table></figure></li>
<li><p>使用 jps 命令查看，发现此时 NameNode 进程已经成功启动了。</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ jps</span><br><span class="line">94994 Jps</span><br><span class="line">94543 NameNode</span><br></pre></td></tr></table></figure></li>
<li><p>还可以通过浏览器界面来访问 NameNode</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122102444283.png" alt="image-20211122102444283"></p>
</li>
<li><p>使用当前配置文件启动一个 NameNode</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 myconfig] $ hadoop-daemon.sh --config /opt/module/hadoop-2.7.2/etc/hadoop/myconfig/ start datanode</span><br></pre></td></tr></table></figure></li>
<li><p>查看 DataNode 是否成功启动</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ jps</span><br><span class="line">50678 DataNode</span><br><span class="line">51053 Jps</span><br><span class="line">94543 NameNode</span><br></pre></td></tr></table></figure></li>
<li><p>或者在浏览器中查看 DataNode 的启动情况</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122104518498.png" alt="image-20211122104518498"></p>
</li>
<li><p>当 NameNode 和 DataNode 都启动成功后，执行上传文件命令</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop --config /opt/module/hadoop-2.7.2/etc/hadoop/myconfig/ fs -put /opt/module/hadoop-2.7.2/etc/hadoop/testUpload /</span><br><span class="line"><span class="comment"># 表示加载 myconfig 目录下的配置文件，完成将 testUpload 文件上传到 HDFS 的根目录（/）下</span></span><br></pre></td></tr></table></figure></li>
<li><p>上传完成后，通过浏览器查看是否上传成功</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122105055584.png" alt="image-20211122105055584"></p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122105640671.png" alt="image-20211122105640671"></p>
</li>
<li><p>也可以在 HDFS 指定的保存数据的目录下查看上传的文件信息</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ tree /opt/module/hadoop-2.7.2/data/</span><br><span class="line">/opt/module/hadoop-2.7.2/data/</span><br><span class="line">└── dfs</span><br><span class="line">    ├── data	<span class="comment"># data 目录用于保存 DataNode 节点的数据文件</span></span><br><span class="line">    │   ├── current</span><br><span class="line">    │   │   ├── BP-401557870-192.168.1.102-1637506411781</span><br><span class="line">    │   │   │   ├── current</span><br><span class="line">    │   │   │   │   ├── finalized</span><br><span class="line">    │   │   │   │   │   └── subdir0</span><br><span class="line">    │   │   │   │   │       └── subdir0</span><br><span class="line">    │   │   │   │   │           ├── blk_1073741825	<span class="comment"># block块的id，与浏览器中展示的块的id一致</span></span><br><span class="line">    │   │   │   │   │           └── blk_1073741825_1001.meta</span><br><span class="line">    │   │   │   │   ├── rbw</span><br><span class="line">    │   │   │   │   └── VERSION</span><br><span class="line">    │   │   │   ├── scanner.cursor</span><br><span class="line">    │   │   │   └── tmp</span><br><span class="line">    │   │   └── VERSION</span><br><span class="line">    │   └── in_use.lock</span><br><span class="line">    └── name	<span class="comment"># name 目录用于保存 NameNode 节点的数据文件</span></span><br><span class="line">        ├── current</span><br><span class="line">        │   ├── edits_inprogress_0000000000000000001</span><br><span class="line">        │   ├── fsimage_0000000000000000000</span><br><span class="line">        │   ├── fsimage_0000000000000000000.md5</span><br><span class="line">        │   ├── seen_txid</span><br><span class="line">        │   └── VERSION</span><br><span class="line">        └── in_use.lock</span><br></pre></td></tr></table></figure>

</li>
<li><p>继续查看 <code>blk_1073741825</code> 块文件的信息，发现内容与我们之前写的 testUpLoad 文件一致</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ <span class="built_in">cat</span> /opt/module/hadoop-2.7.2/data/dfs/data/current/BP-401557870-192.168.1.102-1637506411781/current/finalized/subdir0/subdir0/blk_1073741825</span><br><span class="line">测试上传文件</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h1 id="本地-MapReduce-测试"><a href="#本地-MapReduce-测试" class="headerlink" title="本地 MapReduce 测试"></a>本地 MapReduce 测试</h1><p>在 <code>$HADOOP_HOME/share/hadoop/mapreduce/</code> 目录下有一个自带的 MapReduce 测试案例 <code>hadoop-mapreduce-examples-2.7.2.jar</code>，我们可以直接调用这个 jar 包进行测试。</p>
<ol>
<li><p>进入到实例程序 jar 包所在的目录</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop-2.7.2]$ <span class="built_in">cd</span> share/hadoop/mapreduce/</span><br></pre></td></tr></table></figure></li>
<li><p>尝试执行这个 jar 包</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.2.jar </span><br><span class="line">An example program must be given as the first argument. <span class="comment"># 实例程序必须给一个参数，这个参数就是要执行该jar包下的哪个方法</span></span><br><span class="line">Valid program names are:</span><br><span class="line">  aggregatewordcount: An Aggregate based map/reduce program that counts the words <span class="keyword">in</span> the input files.</span><br><span class="line">  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words <span class="keyword">in</span> the input files.</span><br><span class="line">  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.</span><br><span class="line">  dbcount: An example job that count the pageview counts from a database.</span><br><span class="line">  distbbp: A map/reduce program that uses a BBP-<span class="built_in">type</span> formula to compute exact bits of Pi.</span><br><span class="line">  grep: A map/reduce program that counts the matches of a regex <span class="keyword">in</span> the input.</span><br><span class="line">  <span class="built_in">join</span>: A job that effects a <span class="built_in">join</span> over sorted, equally partitioned datasets</span><br><span class="line">  multifilewc: A job that counts words from several files.</span><br><span class="line">  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.</span><br><span class="line">  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.</span><br><span class="line">  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.</span><br><span class="line">  randomwriter: A map/reduce program that writes 10GB of random data per node.</span><br><span class="line">  secondarysort: An example defining a secondary <span class="built_in">sort</span> to the reduce.</span><br><span class="line">  <span class="built_in">sort</span>: A map/reduce program that sorts the data written by the random writer.</span><br><span class="line">  sudoku: A sudoku solver.</span><br><span class="line">  teragen: Generate data <span class="keyword">for</span> the terasort</span><br><span class="line">  terasort: Run the terasort</span><br><span class="line">  teravalidate: Checking results of terasort</span><br><span class="line">  wordcount: A map/reduce program that counts the words <span class="keyword">in</span> the input files.	<span class="comment"># 单词统计方法</span></span><br><span class="line">  wordmean: A map/reduce program that counts the average length of the words <span class="keyword">in</span> the input files.</span><br><span class="line">  wordmedian: A map/reduce program that counts the median length of the words <span class="keyword">in</span> the input files.</span><br><span class="line">  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words <span class="keyword">in</span> the input files.</span><br></pre></td></tr></table></figure></li>
<li><p>尝试执行该 jar 包下的 wordcount 方法</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount</span><br><span class="line">Usage: wordcount &lt;<span class="keyword">in</span>&gt; [&lt;<span class="keyword">in</span>&gt;...] &lt;out&gt;</span><br><span class="line"><span class="comment"># 调用 wordcount 方法必须传递一个或多个源文件路径和一个统计结果保存的路径</span></span><br><span class="line"><span class="comment"># 源文件路径和统计结果保存的路径都是HDFS上的路径</span></span><br></pre></td></tr></table></figure></li>
<li><p>当前 HDFS 的 NameNode 的根目录下有一个 <code>testUpLoad</code> 文件，我们可以统计该文件中单词的个数</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount / /result</span><br><span class="line"><span class="comment"># 调用 wordcount 方法统计根目录 `/` 下的所有文件中的单词个数，并把结果输出到 `/result` 目录下 </span></span><br></pre></td></tr></table></figure></li>
<li><p>查看浏览器发现根目录下多了一个 <code>/result</code> 目录</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122173553541.png" alt="image-20211122173553541"></p>
</li>
<li><p><code>/result</code> 目录</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122173729381.png" alt="image-20211122173729381"></p>
</li>
<li><p><code>part-r-00000</code>，</p>
<p> 我们可以将这个文件下载到本地查看，发现该文件就是统计了 <code>/</code> 目录下每个单词出现的次数。</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122174007581.png" alt="image-20211122174007581"></p>
</li>
</ol>
<hr>
<h1 id="在-YARN-上运行-MapReduce"><a href="#在-YARN-上运行-MapReduce" class="headerlink" title="在 YARN 上运行 MapReduce"></a>在 YARN 上运行 MapReduce</h1><h2 id="1、修改-MapReduce-的运行模式"><a href="#1、修改-MapReduce-的运行模式" class="headerlink" title="1、修改 MapReduce 的运行模式"></a>1、修改 MapReduce 的运行模式</h2><ol>
<li><p>默认的 MapReduce 的运行模式是本地模式。我们可以通过 MapReduce 的默认配置文件 <code>mapred-default.xml</code> 来查看。</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>local<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The runtime framework for executing MapReduce jobs.</span><br><span class="line">  Can be one of local, classic or yarn. </span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">	运行模式可以是 local、classic、yarn 中的任意一个，默认选择的 local</span></span><br><span class="line"><span class="comment">		（1）local 是本地模式</span></span><br><span class="line"><span class="comment">		（2）classic 是Hadoop1.x 版本提供的一种运行模式</span></span><br><span class="line"><span class="comment">		（3）yarn 是将MR程序交给YARN去执行</span></span><br><span class="line"><span class="comment">	--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>如果我们想要通过 Yarn 来执行 MapReduce 程序，就需要在自定义的配置文件中重写这个配置信息。在 <code>$HADOOP_HOME/etc/hadoop/</code> 目录下创建并修改 <code>mapred-site.xml</code> 文件的信息，添加如下配置：</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">     将 MapReduce 从本地模式改为在 YARN 上运行</span></span><br><span class="line"><span class="comment"> --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2、启动-Yarn"><a href="#2、启动-Yarn" class="headerlink" title="2、启动 Yarn"></a>2、启动 Yarn</h2><p><strong>配置 ResourceManager 的通信地址</strong></p>
<p>Yarn 由 ResourceManager 和 NodeManager 线程组成。同理，Yarn 作为一个服务器集群，其中的 ResourceManager 既要负责和客户端通信，又要负责和 NodeManager 通信，所以 ResourceManager 必须对外暴露自己的服务地址。这一信息要在 <code>yarn-default.xml</code> 配置文件中进行修改。修改 <code>$HADOOP_HOME/etc/hadoop/yarn-site.xml</code> 文件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">     配置 Yarn 的 ResourceManager 的服务地址</span></span><br><span class="line"><span class="comment"> --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p><strong>配置 Reducer 获取数据的方式</strong></p>
<ul>
<li>  MapReduce 思想处理数据的方式是将任务拆分执行，最后将结果合并，这时候就可能出现多个 Map 间需要进行数据共享的问题，那么这多个 Map 之间数据传递的方式就叫做 shuffle，所以我们必须配置这一属性，否则在 Yarn 上运行 MR 程序时就会报错。</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">     配置 Reducer 获取数据的方式</span></span><br><span class="line"><span class="comment">     Map 阶段处理好的数据最后要传递到 Reducer 上使用。</span></span><br><span class="line"><span class="comment">     而传递数据的方式称为 shuffle ，所以 YARN 中必须配置shuffle 才能正常运行</span></span><br><span class="line"><span class="comment"> --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>





<p><strong>启动</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、启动 RM</span></span><br><span class="line">yarn-daemon.sh start resourcemanager</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、启动 NM</span></span><br><span class="line">yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure>



<p><strong>查看</strong></p>
<ol>
<li><p>通过 jps 命令查看</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ jps</span><br><span class="line">63025 NodeManager</span><br><span class="line">35193 NameNode</span><br><span class="line">36077 DataNode</span><br><span class="line">62367 ResourceManager</span><br><span class="line">64799 Jps</span><br></pre></td></tr></table></figure></li>
<li><p>通过浏览器查看</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">访问：http://&#123;ResourceManager线程所在机器的IP&#125;:8088</span><br><span class="line"><span class="comment"># ResourceManager 对外暴露的 http 端口号是 8088</span></span><br></pre></td></tr></table></figure>

<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122183127209.png" alt="image-20211122183127209"></p>
</li>
</ol>
<hr>
<h2 id="3、提交任务"><a href="#3、提交任务" class="headerlink" title="3、提交任务"></a>3、提交任务</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar  jar包  主类名 参数&#123;多个输入目录，一个输出目录&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li><p>在<code>/opt/module/hadoop-2.7.2/share/hadoop/mapreduce</code> 目录下创建一个文件<code>testMR</code></p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 mapreduce]$ vim testMR</span><br></pre></td></tr></table></figure></li>
<li><p>编辑该文件的内容为：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">test</span></span><br><span class="line">MR</span><br><span class="line">demo</span><br></pre></td></tr></table></figure></li>
<li><p>将该文件上传到 HDFS 的<code>/mr</code> 目录下</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 mapreduce]$ hadoop fs -put ./testMR /mr</span><br></pre></td></tr></table></figure></li>
<li><p>调用 wordcount 方法统计目录 <code>/mr</code> 下的所有文件中的单词个数，并把结果输出到 <code>/count</code> 目录下</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount /mr /count</span><br></pre></td></tr></table></figure></li>
<li><p>通过浏览器查看执行结果。</p>
<p> <img src="/2021/11/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20211122194846938.png" alt="image-20211122194846938"></p>
</li>
</ol>
<p><strong>注意事项：</strong></p>
<ol>
<li> 更改配置信息后需要重启 ResourceManager 和 NodeManager！</li>
<li> 输入目录中必须全部是文件，不能有目录。如果有目录的话会报错！</li>
<li> 输出目录必须不存在！</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/11/20/Linux%E2%80%94%E2%80%94%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%AE%A1%E7%90%86/" rel="prev" title="Linux——软件包管理">
                  <i class="fa fa-chevron-left"></i> Linux——软件包管理
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/11/22/Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/" rel="next" title="Hadoop完全分布式集群搭建">
                  Hadoop完全分布式集群搭建 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  





</body>
</html>
