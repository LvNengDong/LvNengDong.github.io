<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="第 1 章    Spark SQL 概述1.1    什么是 Spark SQLSpark SQL 是 Spark 用于处理结构化数据的模块。 与基础的 Spark RDD API 不同， Spark SQL 的抽象数据类型为 Spark 提供了关于数据结构和正在执行的计算的更多信息。 在内部， Spark SQL 使用这些额外的信息去做一些额外的优化。 Spark 提供了多种方式与 Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark_SQL">
<meta property="og:url" content="http://example.com/2021/12/11/Spark-SQL/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="第 1 章    Spark SQL 概述1.1    什么是 Spark SQLSpark SQL 是 Spark 用于处理结构化数据的模块。 与基础的 Spark RDD API 不同， Spark SQL 的抽象数据类型为 Spark 提供了关于数据结构和正在执行的计算的更多信息。 在内部， Spark SQL 使用这些额外的信息去做一些额外的优化。 Spark 提供了多种方式与 Spark">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/12/11/Spark-SQL/image-20211221125130508.png">
<meta property="og:image" content="http://example.com/2021/12/11/Spark-SQL/image-20211216221858916.png">
<meta property="og:image" content="http://example.com/2021/12/11/Spark-SQL/image-20211216222125022.png">
<meta property="og:image" content="http://example.com/2021/12/11/Spark-SQL/image-20211216222328574.png">
<meta property="og:image" content="http://example.com/2021/12/11/Spark-SQL/image-20211217222307261.png">
<meta property="og:image" content="http://example.com/2021/12/11/Spark-SQL/image-20211217095851402.png">
<meta property="og:image" content="http://example.com/2021/12/11/Spark-SQL/image-20211221135048075.png">
<meta property="og:image" content="http://example.com/2021/12/11/Spark-SQL/image-20211224130331108.png">
<meta property="og:image" content="http://example.com/2021/12/11/Spark-SQL/image-20211225162805804.png">
<meta property="og:image" content="http://example.com/2021/12/11/Spark-SQL/image-20211225195825371.png">
<meta property="article:published_time" content="2021-12-11T12:50:18.000Z">
<meta property="article:modified_time" content="2022-01-16T05:22:57.393Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/12/11/Spark-SQL/image-20211221125130508.png">


<link rel="canonical" href="http://example.com/2021/12/11/Spark-SQL/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2021/12/11/Spark-SQL/","path":"2021/12/11/Spark-SQL/","title":"Spark_SQL"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Spark_SQL | Hexo</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hexo</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC-1-%E7%AB%A0-Spark-SQL-%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">第 1 章    Spark SQL 概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E4%BB%80%E4%B9%88%E6%98%AF-Spark-SQL"><span class="nav-number">1.1.</span> <span class="nav-text">1.1    什么是 Spark SQL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-Spark-SQL-%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-number">1.2.</span> <span class="nav-text">1.2    Spark SQL 的特点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Integrated-%E6%98%93%E6%95%B4%E5%90%88"><span class="nav-number">1.2.1.</span> <span class="nav-text">1. Integrated(易整合)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Uniform-Data-Access-%E7%BB%9F%E4%B8%80%E7%9A%84%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE%E6%96%B9%E5%BC%8F"><span class="nav-number">1.2.2.</span> <span class="nav-text">2. Uniform Data Access(统一的数据访问方式)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Hive-Integration%EF%BC%88%E9%9B%86%E6%88%90-Hive%EF%BC%89"><span class="nav-number">1.2.3.</span> <span class="nav-text">3. Hive Integration（集成 Hive）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Standard-Connectivity%EF%BC%88%E6%A0%87%E5%87%86%E7%9A%84%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F%EF%BC%89"><span class="nav-number">1.2.4.</span> <span class="nav-text">4. Standard Connectivity（标准的连接方式）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-%E4%BB%80%E4%B9%88%E6%98%AF-DataFrame"><span class="nav-number">1.3.</span> <span class="nav-text">1.3    什么是 DataFrame</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-%E4%BB%80%E4%B9%88%E6%98%AF-DataSet"><span class="nav-number">1.4.</span> <span class="nav-text">1.4    什么是 DataSet</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC-2-%E7%AB%A0-Spark-SQL-%E7%BC%96%E7%A8%8B"><span class="nav-number">2.</span> <span class="nav-text">第 2 章 Spark SQL 编程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-SparkSession"><span class="nav-number">2.1.</span> <span class="nav-text">2.1    SparkSession</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E4%BD%BF%E7%94%A8-DataFrame-%E8%BF%9B%E8%A1%8C%E7%BC%96%E7%A8%8B"><span class="nav-number">2.2.</span> <span class="nav-text">2.2    使用 DataFrame 进行编程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-%E5%88%9B%E5%BB%BA-DataFrame"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1    创建 DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-1-%E9%80%9A%E8%BF%87%E6%95%B0%E6%8D%AE%E6%BA%90%E5%88%9B%E5%BB%BA-DataFrame"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">2.2.1.1    通过数据源创建 DataFrame</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-2-%E9%80%9A%E8%BF%87-RDD-%E8%BF%9B%E8%A1%8C%E8%BD%AC%E6%8D%A2"><span class="nav-number">2.2.1.2.</span> <span class="nav-text">2.2.1.2    通过 RDD 进行转换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-3-%E9%80%9A%E8%BF%87%E6%9F%A5%E8%AF%A2-Hive-%E8%A1%A8%E5%88%9B%E5%BB%BA"><span class="nav-number">2.2.1.3.</span> <span class="nav-text">2.2.1.3    通过查询 Hive 表创建</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-DataFrame-%E8%AF%AD%E6%B3%95%E9%A3%8E%E6%A0%BC"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2    DataFrame 语法风格</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-1-SQL-%E8%AF%AD%E6%B3%95%E9%A3%8E%E6%A0%BC%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">2.2.2.1    SQL 语法风格（重要）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-2-DSL-%E8%AF%AD%E6%B3%95%E9%A3%8E%E6%A0%BC%EF%BC%88%E4%BA%86%E8%A7%A3%EF%BC%89"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">2.2.2.2    DSL 语法风格（了解）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E6%9F%A5%E7%9C%8B-Schema-%E4%BF%A1%E6%81%AF"><span class="nav-number">2.2.2.2.1.</span> <span class="nav-text">1    查看 Schema 信息</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E4%BD%BF%E7%94%A8-DSL-%E6%9F%A5%E8%AF%A2"><span class="nav-number">2.2.2.2.2.</span> <span class="nav-text">2    使用 DSL 查询</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-RDD-%E5%92%8C-DataFrame-%E7%9A%84%E4%BA%A4%E4%BA%92"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.2.3    RDD 和 DataFrame 的交互</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%E3%80%81RDD-gt-DataFrame"><span class="nav-number">2.2.3.1.</span> <span class="nav-text">1、RDD &#x3D;&gt; DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%89%8B%E5%8A%A8%E8%BD%AC%E6%8D%A2"><span class="nav-number">2.2.3.1.1.</span> <span class="nav-text">手动转换</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%80%9A%E8%BF%87%E6%A0%B7%E4%BE%8B%E7%B1%BB%E7%9A%84%E5%8F%8D%E5%B0%84%E6%9C%BA%E5%88%B6%E8%BD%AC%E6%8D%A2%EF%BC%88%E5%B8%B8%E7%94%A8%EF%BC%89"><span class="nav-number">2.2.3.1.2.</span> <span class="nav-text">通过样例类的反射机制转换（常用）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%80%9A%E8%BF%87-API-%E7%9A%84%E6%96%B9%E5%BC%8F%E8%BD%AC%E6%8D%A2%EF%BC%88%E4%BA%86%E8%A7%A3%EF%BC%89"><span class="nav-number">2.2.3.1.3.</span> <span class="nav-text">通过 API 的方式转换（了解）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2%E3%80%81%E4%BB%8E-DataFrame-%E5%88%B0-RDD"><span class="nav-number">2.2.3.2.</span> <span class="nav-text">2、从 DataFrame 到 RDD</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-%E4%BD%BF%E7%94%A8-DataSet-%E8%BF%9B%E8%A1%8C%E7%BC%96%E7%A8%8B"><span class="nav-number">2.3.</span> <span class="nav-text">2.3    使用 DataSet 进行编程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1-%E5%88%9B%E5%BB%BA-DataSet"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.1    创建 DataSet</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%E3%80%81%E9%80%9A%E8%BF%87-Scala-%E5%BA%8F%E5%88%97%EF%BC%88%E9%9B%86%E5%90%88%EF%BC%89%E5%BE%97%E5%88%B0"><span class="nav-number">2.3.1.1.</span> <span class="nav-text">1、通过 Scala 序列（集合）得到</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-RDD-%E5%92%8C-DataSet-%E7%9A%84%E4%BA%A4%E4%BA%92"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.2    RDD 和 DataSet 的交互</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%E3%80%81RDD-gt-DataSet"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">1、RDD &#x3D;&gt; DataSet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2%E3%80%81DataSet-gt-RDD"><span class="nav-number">2.3.2.2.</span> <span class="nav-text">2、DataSet &#x3D;&gt; RDD</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-DataFrame-%E5%92%8C-DataSet-%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BA%A4%E4%BA%92"><span class="nav-number">2.4.</span> <span class="nav-text">2.4    DataFrame 和 DataSet 之间的交互</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-1-DataFrame-gt-DataSet"><span class="nav-number">2.4.1.</span> <span class="nav-text">2.4.1    DataFrame &#x3D;&gt; DataSet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-2-DataSet-gt-DataFrame"><span class="nav-number">2.4.2.</span> <span class="nav-text">2.4.2    DataSet &#x3D;&gt; DataFrame</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-RDD-DataFrame-%E5%92%8C-DataSet-%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-number">2.5.</span> <span class="nav-text">2.5    RDD, DataFrame 和 DataSet 之间的关系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-1-%E4%B8%89%E8%80%85%E7%9A%84%E5%85%B1%E6%80%A7"><span class="nav-number">2.5.1.</span> <span class="nav-text">2.5.1    三者的共性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-2-%E4%B8%89%E8%80%85%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">2.5.2.</span> <span class="nav-text">2.5.2    三者的区别</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-2-1-RDD"><span class="nav-number">2.5.2.1.</span> <span class="nav-text">2.5.2.1    RDD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-2-2-DataFrame"><span class="nav-number">2.5.2.2.</span> <span class="nav-text">2.5.2.2    DataFrame</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-2-3-DataSet"><span class="nav-number">2.5.2.3.</span> <span class="nav-text">2.5.2.3    DataSet</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-3-%E4%B8%89%E8%80%85%E7%9A%84%E4%BA%92%E7%9B%B8%E8%BD%AC%E6%8D%A2"><span class="nav-number">2.5.3.</span> <span class="nav-text">2.5.3    三者的互相转换</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-%E4%BD%BF%E7%94%A8-IDEA-%E5%88%9B%E5%BB%BA-SparkSQL-%E7%A8%8B%E5%BA%8F"><span class="nav-number">2.6.</span> <span class="nav-text">2.6    使用 IDEA 创建 SparkSQL 程序</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A41%EF%BC%9A%E6%B7%BB%E5%8A%A0-SparkSQL-%E4%BE%9D%E8%B5%96"><span class="nav-number">2.6.1.</span> <span class="nav-text">步骤1：添加 SparkSQL 依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A42%EF%BC%9A%E5%85%B7%E4%BD%93%E4%BB%A3%E7%A0%81"><span class="nav-number">2.6.2.</span> <span class="nav-text">步骤2：具体代码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%E3%80%81%E5%88%9B%E5%BB%BA-DF-%E5%AF%B9%E8%B1%A1"><span class="nav-number">2.6.2.1.</span> <span class="nav-text">1、创建 DF 对象</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2%E3%80%81DF-gt-RDD"><span class="nav-number">2.6.2.2.</span> <span class="nav-text">2、DF &#x3D;&gt; RDD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3%E3%80%81RDD-gt-DF"><span class="nav-number">2.6.2.3.</span> <span class="nav-text">3、RDD &#x3D;&gt; DF</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-7-%E8%87%AA%E5%AE%9A%E4%B9%89-SparkSQL-%E5%87%BD%E6%95%B0"><span class="nav-number">2.7.</span> <span class="nav-text">2.7    自定义 SparkSQL 函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-1-%E8%87%AA%E5%AE%9A%E4%B9%89-UDF-%E5%87%BD%E6%95%B0"><span class="nav-number">2.7.1.</span> <span class="nav-text">2.7.1    自定义 UDF 函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-2-%E7%94%A8%E6%88%B7%E8%87%AA%E5%AE%9A%E4%B9%89%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0-UDAF"><span class="nav-number">2.7.2.</span> <span class="nav-text">2.7.2    用户自定义聚合函数(UDAF)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC-3-%E7%AB%A0-SparkSQL-%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="nav-number">3.</span> <span class="nav-text">第 3 章    SparkSQL 数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E9%80%9A%E7%94%A8%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98%E5%87%BD%E6%95%B0"><span class="nav-number">3.1.</span> <span class="nav-text">3.1    通用加载和保存函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-%E6%89%8B%E5%8A%A8%E6%8C%87%E5%AE%9A%E9%80%89%E9%A1%B9"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1    手动指定选项</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-%E5%9C%A8%E6%96%87%E4%BB%B6%E4%B8%8A%E7%9B%B4%E6%8E%A5%E8%BF%90%E8%A1%8C-SQL"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2    在文件上直接运行 SQL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-%E6%96%87%E4%BB%B6%E4%BF%9D%E5%AD%98%E9%80%89%E9%A1%B9-SaveMode"><span class="nav-number">3.1.3.</span> <span class="nav-text">3.1.3    文件保存选项(SaveMode)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E5%8A%A0%E8%BD%BD-JSON-%E6%96%87%E4%BB%B6"><span class="nav-number">3.2.</span> <span class="nav-text">3.2    加载 JSON 文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-%E8%AF%BB%E5%8F%96-Parquet-%E6%96%87%E4%BB%B6"><span class="nav-number">3.3.</span> <span class="nav-text">3.3    读取 Parquet 文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-JDBC"><span class="nav-number">3.4.</span> <span class="nav-text">3.4    JDBC</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-1-%E4%BB%8E-JDBC-%E8%AF%BB%E6%95%B0%E6%8D%AE"><span class="nav-number">3.4.1.</span> <span class="nav-text">3.4.1    从 JDBC 读数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-2-%E5%90%91-JDBC-%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="nav-number">3.4.2.</span> <span class="nav-text">3.4.2    向 JDBC 写入数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-Hive-%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89"><span class="nav-number">3.5.</span> <span class="nav-text">3.5    Hive 数据库（重要）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-1-%E4%BD%BF%E7%94%A8%E5%86%85%E5%B5%8C%E7%9A%84-Hive"><span class="nav-number">3.5.1.</span> <span class="nav-text">3.5.1    使用内嵌的 Hive</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-2-Spark-%E9%9B%86%E6%88%90%E5%A4%96%E7%BD%AE-Hive"><span class="nav-number">3.5.2.</span> <span class="nav-text">3.5.2    Spark 集成外置 Hive</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-2-1-%E9%9B%86%E6%88%90%E6%AD%A5%E9%AA%A4"><span class="nav-number">3.5.2.1.</span> <span class="nav-text">3.5.2.1    集成步骤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-2-2-%E5%90%AF%E5%8A%A8-spark-shell"><span class="nav-number">3.5.2.2.</span> <span class="nav-text">3.5.2.2    启动 spark-shell</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-2-3-%E4%BD%BF%E7%94%A8-spark-sql-%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="nav-number">3.5.2.3.</span> <span class="nav-text">3.5.2.3    使用 spark-sql 客户端</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-2-4-%E4%BD%BF%E7%94%A8-hiveserver2-beeline"><span class="nav-number">3.5.2.4.</span> <span class="nav-text">3.5.2.4    使用 hiveserver2 + beeline</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-3-%E5%9C%A8%E4%BB%A3%E7%A0%81%E4%B8%AD%E8%AE%BF%E9%97%AE-Hive"><span class="nav-number">3.5.3.</span> <span class="nav-text">3.5.3    在代码中访问 Hive</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A41-%E6%8B%B7%E8%B4%9D-hive-site-xml-%E5%88%B0-resources-%E7%9B%AE%E5%BD%95%E4%B8%8B"><span class="nav-number">3.5.3.1.</span> <span class="nav-text">步骤1: 拷贝 hive-site.xml 到 resources 目录下</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A42-%E6%B7%BB%E5%8A%A0%E4%BE%9D%E8%B5%96"><span class="nav-number">3.5.3.2.</span> <span class="nav-text">步骤2: 添加依赖</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A43-%E4%BB%A3%E7%A0%81"><span class="nav-number">3.5.3.3.</span> <span class="nav-text">步骤3: 代码</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%8E-Hive-%E4%B8%AD%E8%AF%BB%E6%95%B0%E6%8D%AE"><span class="nav-number">3.5.3.3.1.</span> <span class="nav-text">从 Hive 中读数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%91-Hive-%E4%B8%AD%E5%86%99%E6%95%B0%E6%8D%AE"><span class="nav-number">3.5.3.3.2.</span> <span class="nav-text">向 Hive 中写数据</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-number">3.5.3.3.2.1.</span> <span class="nav-text">创建数据库</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">224</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/11/Spark-SQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Spark_SQL | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark_SQL
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-11 20:50:18" itemprop="dateCreated datePublished" datetime="2021-12-11T20:50:18+08:00">2021-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-01-16 13:22:57" itemprop="dateModified" datetime="2022-01-16T13:22:57+08:00">2022-01-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="第-1-章-Spark-SQL-概述"><a href="#第-1-章-Spark-SQL-概述" class="headerlink" title="第 1 章    Spark SQL 概述"></a>第 1 章    Spark SQL 概述</h1><h2 id="1-1-什么是-Spark-SQL"><a href="#1-1-什么是-Spark-SQL" class="headerlink" title="1.1    什么是 Spark SQL"></a>1.1    什么是 Spark SQL</h2><p><code>Spark SQL</code> 是 <code>Spark</code> 用于处理结构化数据的模块。</p>
<p>与基础的 <code>Spark RDD API</code> 不同， <code>Spark SQL</code> 的抽象数据类型为 <code>Spark</code> 提供了关于数据结构和正在执行的计算的更多信息。</p>
<p>在内部， <code>Spark SQL</code> 使用这些额外的信息去做一些额外的优化。</p>
<p><code>Spark</code> 提供了多种方式与 <code>Spark SQL</code> 进行交互，比如： <code>SQL</code> 和 <code>Dataset API</code>。当计算结果的时候，使用的是相同的执行引擎，不依赖你正在使用哪种 <code>API</code> 或者语言。这种统一也就意味着开发者可以很容易在不同的 <code>API</code> 之间进行切换，这些 <code>API</code> 提供了最自然的方式来表达给定的转换。</p>
<p>我们已经学习了 <code>Hive</code>，它是将 <code>Hive SQL</code> 转换成 <code>MapReduce</code> 然后提交到集群上执行，大大简化了编写 <code>MapReduce</code> 程序的复杂性，由于 <code>MapReduce</code> 这种计算模型执行效率比较慢，所以 <code>Spark SQL</code> 的应运而生，它是将 <code>Spark SQL</code> 转换成 <code>RDD</code>，然后提交到集群执行，执行效率非常快。</p>
<p><code>Spark SQL</code> 提供了 2 个编程抽象，类似 <code>Spark Core</code> 中的 <code>RDD</code>。它们分别是：</p>
<ul>
<li>  <strong>DataFrame</strong></li>
<li>  <strong>DataSet</strong></li>
</ul>
<hr>
<h2 id="1-2-Spark-SQL-的特点"><a href="#1-2-Spark-SQL-的特点" class="headerlink" title="1.2    Spark SQL 的特点"></a>1.2    Spark SQL 的特点</h2><h3 id="1-Integrated-易整合"><a href="#1-Integrated-易整合" class="headerlink" title="1. Integrated(易整合)"></a>1. Integrated(易整合)</h3><p>无缝的整合了 <code>SQL</code> 查询和 <code>Spark</code> 编程。</p>
<h3 id="2-Uniform-Data-Access-统一的数据访问方式"><a href="#2-Uniform-Data-Access-统一的数据访问方式" class="headerlink" title="2. Uniform Data Access(统一的数据访问方式)"></a>2. Uniform Data Access(统一的数据访问方式)</h3><p>使用相同的方式连接不同的数据源。</p>
<h3 id="3-Hive-Integration（集成-Hive）"><a href="#3-Hive-Integration（集成-Hive）" class="headerlink" title="3. Hive Integration（集成 Hive）"></a>3. Hive Integration（集成 Hive）</h3><p>在已有的仓库上直接运行 <code>SQL</code> 或者 <code>HQL</code></p>
<p><img src="/2021/12/11/Spark-SQL/image-20211221125130508.png" alt="image-20211221125130508"></p>
<h3 id="4-Standard-Connectivity（标准的连接方式）"><a href="#4-Standard-Connectivity（标准的连接方式）" class="headerlink" title="4. Standard Connectivity（标准的连接方式）"></a>4. Standard Connectivity（标准的连接方式）</h3><p>通过 <code>JDBC</code> 或者 <code>ODBC</code> 来连接</p>
<hr>
<h2 id="1-3-什么是-DataFrame"><a href="#1-3-什么是-DataFrame" class="headerlink" title="1.3    什么是 DataFrame"></a>1.3    什么是 DataFrame</h2><p>与 <code>RDD</code> 类似，<strong>DataFrame 也是一个分布式的数据容器</strong>。</p>
<p>然而 <code>DataFrame</code> 更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息（元数据信息），即 <code>schema</code>。</p>
<p>同时，与 <code>Hive</code> 类似，<code>DataFrame</code> 也支持嵌套数据类型（<code>struct</code>、<code>array</code> 和 <code>map</code>）。</p>
<p>从 <code>API</code> 易用性的角度上看， <code>DataFrame API</code> 提供了一套更高层的关系操作，比函数式的 <code>RDD API</code> 要更加友好，门槛更低。</p>
<p><img src="/2021/12/11/Spark-SQL/image-20211216221858916.png" alt="image-20211216221858916"></p>
<p>上图直观地体现了 <code>DataFrame</code> 和 <code>RDD</code> 的区别：</p>
<ul>
<li>  左侧的 <code>RDD[Person]</code> 虽然以 <code>Person</code> 为类型参数，但 <code>Spark</code> 框架本身不了解 <code>Person</code> 类的内部结构。</li>
<li>  而右侧的 <code>DataFrame</code> 却提供了详细的结构信息，通过 <code>Spark SQL</code> 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。</li>
</ul>
<p><code>DataFrame</code> 为数据提供了 <code>Schema（元数据）</code> 的视图，可以把它当做数据库中的一张表来对待。</p>
<p><code>DataFrame</code> 也是懒执行的。</p>
<p><code>DataFrame</code> 性能上比 <code>RDD</code> 要高，原因是 <code>Spark</code> 底层会通过 <code>Spark catalyst optimiser</code> 对这种类 <code>SQL</code> 的语句进行优化。比如下面一个例子：</p>
<p> <img src="/2021/12/11/Spark-SQL/image-20211216222125022.png" alt="image-20211216222125022"></p>
<p>为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个 <code>DataFrame</code>，将它们 <code>join</code> 之后又做了一次 <code>filter</code> 操作。</p>
<p>如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为 <code>join</code> 是一个代价较大的操作，也可能会产生一个较大的数据集。</p>
<p>如果我们能将 <code>filter</code> 下推到 <code>join</code> 下方，先对 <code>DataFrame</code> 进行过滤，再 <code>join</code> 过滤后的较小的结果集，便可以有效缩短执行时间。</p>
<p>而 <code>Spark SQL</code> 的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。</p>
<p><img src="/2021/12/11/Spark-SQL/image-20211216222328574.png" alt="image-20211216222328574"></p>
<hr>
<h2 id="1-4-什么是-DataSet"><a href="#1-4-什么是-DataSet" class="headerlink" title="1.4    什么是 DataSet"></a>1.4    什么是 DataSet</h2><ol>
<li>   是 <code>DataFrame API</code> 的一个扩展，是 <code>SparkSQL</code> 最新的数据抽象（1.6 新增）。</li>
<li>   用户友好的 <code>API</code> 风格，既具有类型安全检查也具有 <code>DataFrame</code> 的查询优化特性。</li>
<li>   <code>Dataset</code> 支持编/解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。</li>
<li>   样例类被用来在 <code>DataSet</code> 中定义数据的结构信息，样例类中每个属性的名称直接映射到 <code>DataSet</code> 中的字段名称。</li>
<li>   <code>DataFrame</code> 是 <code>DataSet</code> 的特例，<code>DataFrame=DataSet[Row]</code>，所以可以通过 <code>as</code> 方法将 <code>DataFrame</code> 转换为 <code>DataSet</code>。 <code>Row</code> 是一个类型，跟 <code>Car</code>、 <code>Person</code> 这些类型一样，所有的表结构信息都可以用 <code>Row</code> 来表示。</li>
<li>   <code>DataSet</code> 是强类型的。比如可以有 <code>DataSet[Car]</code>，<code>DataSet[Person]</code> 等。</li>
<li>   <code>DataFrame</code> 只是知道字段，但是不知道字段的类型，所以是没办法在编译的时候检查是否类型失败的，比如你可以对一个 <code>String</code> 进行减法操作，在执行的时候才报错，而 <code>DataSet</code> 不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟 <code>JSON</code> 对象和类对象之间的类比。</li>
</ol>
<hr>
<h1 id="第-2-章-Spark-SQL-编程"><a href="#第-2-章-Spark-SQL-编程" class="headerlink" title="第 2 章 Spark SQL 编程"></a>第 2 章 Spark SQL 编程</h1><p>本章重点学习如何使用 <code>DataFrame</code> 和 <code>DataSet</code> 进行编程，已以及它们之间的关系和转换。</p>
<hr>
<h2 id="2-1-SparkSession"><a href="#2-1-SparkSession" class="headerlink" title="2.1    SparkSession"></a>2.1    SparkSession</h2><p>在老的版本中， <code>SparkSQL</code> 提供两种 <code>SQL</code> 查询起始点：一个叫 <code>SQLContext</code>，用于 <code>Spark</code> 自己提供的 <code>SQL</code> 查询；一个叫 <code>HiveContext</code>，用于连接 <code>Hive</code> 的查询。</p>
<p>从 2.0 开始，<code>SparkSession</code> 是 <code>Spark</code> 最新的 <code>SQL</code> 查询起始点，实质上是 <code>SQLContext</code> 和 <code>HiveContext</code> 的组合，所以在 <code>SQLContext</code> 和<code>HiveContext</code> 上可用的 <code>API</code> 在 <code>SparkSession</code> 上同样是可以使用的。</p>
<p><code>SparkSession</code> 内部封装了 <code>SparkContext</code>，所以计算实际上是由 <code>SparkContext</code> 完成的。</p>
<p>当我们使用 <code>spark-shell</code> 客户端的时候， <code>Spark</code> 会自动的创建一个叫做 <code>spark</code> 的 <code>SparkSession</code> 对象，就像我们以前可以自动获取到一个 <code>sc</code> 来表示 <code>SparkContext</code> 对象。</p>
<p><img src="/2021/12/11/Spark-SQL/image-20211217222307261.png" alt="image-20211217222307261"></p>
<hr>
<h2 id="2-2-使用-DataFrame-进行编程"><a href="#2-2-使用-DataFrame-进行编程" class="headerlink" title="2.2    使用 DataFrame 进行编程"></a>2.2    使用 DataFrame 进行编程</h2><p>首先学习 <code>DataFrame</code> 相关的知识。</p>
<p><code>Spark SQL</code> 的 <code>DataFrame API</code> 允许我们使用 <code>DataFrame</code> 而不用必须去注册临时表或者生成 <code>SQL</code> 表达式。<br><code>DataFrame API</code> 既有 <code>transformation</code> 操作也有 <code>action</code> 操作。<code>DataFrame</code> 的转换从本质上来说更具有关系，而 <code>DataSet API</code> 提供了更加函数式的 <code>API</code>。</p>
<hr>
<h3 id="2-2-1-创建-DataFrame"><a href="#2-2-1-创建-DataFrame" class="headerlink" title="2.2.1    创建 DataFrame"></a>2.2.1    创建 DataFrame</h3><p>在得到了 <code>SparkSession</code> 对象后，通过 <code>SparkSession</code> 有 2 种方式来创建 <code>DataFrame</code>：</p>
<ol>
<li> 加载数据源创建 <code>DataFrame</code>。常见的数据源有：<code>JDBC、json、scala集合、Hive 等等</code></li>
<li> 通过已知的 <code>RDD</code> 得到 <code>DataFrame</code></li>
</ol>
<hr>
<h4 id="2-2-1-1-通过数据源创建-DataFrame"><a href="#2-2-1-1-通过数据源创建-DataFrame" class="headerlink" title="2.2.1.1    通过数据源创建 DataFrame"></a>2.2.1.1    通过数据源创建 DataFrame</h4><p><code>Spark</code> 支持的数据源：</p>
<p><img src="/2021/12/11/Spark-SQL/image-20211217095851402.png" alt="image-20211217095851402"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取 json 文件</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/employees.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, salary: bigint]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 展示结果</span></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+-------+------+</span><br><span class="line">|   name|salary|</span><br><span class="line">+-------+------+</span><br><span class="line">|<span class="type">Michael</span>|  <span class="number">3000</span>|</span><br><span class="line">|   <span class="type">Andy</span>|  <span class="number">4500</span>|</span><br><span class="line">| <span class="type">Justin</span>|  <span class="number">3500</span>|</span><br><span class="line">|  <span class="type">Berta</span>|  <span class="number">4000</span>|</span><br><span class="line">+-------+------+</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="2-2-1-2-通过-RDD-进行转换"><a href="#2-2-1-2-通过-RDD-进行转换" class="headerlink" title="2.2.1.2    通过 RDD 进行转换"></a>2.2.1.2    通过 RDD 进行转换</h4><p>后面章节专门讨论</p>
<hr>
<h4 id="2-2-1-3-通过查询-Hive-表创建"><a href="#2-2-1-3-通过查询-Hive-表创建" class="headerlink" title="2.2.1.3    通过查询 Hive 表创建"></a>2.2.1.3    通过查询 Hive 表创建</h4><p>后面章节专门讨论</p>
<hr>
<h3 id="2-2-2-DataFrame-语法风格"><a href="#2-2-2-DataFrame-语法风格" class="headerlink" title="2.2.2    DataFrame 语法风格"></a>2.2.2    DataFrame 语法风格</h3><h4 id="2-2-2-1-SQL-语法风格（重要）"><a href="#2-2-2-1-SQL-语法风格（重要）" class="headerlink" title="2.2.2.1    SQL 语法风格（重要）"></a>2.2.2.1    SQL 语法风格（重要）</h4><p><code>SQL</code> 语法风格是指我们查询数据的时候使用 <code>SQL</code> 语句来查询。</p>
<ul>
<li>  <strong>这种风格的查询必须要有临时视图或者全局视图来辅助</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载指定数据源，创建 DataFrom 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个临时视图（临时表）</span></span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)	</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行查询</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from people&quot;</span>).show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<ul>
<li>  临时视图只能在当前 <code>Session</code> 有效，在新的 <code>Session</code> 中无效。</li>
<li>  可以创建全局视图。访问全局视图需要全路径，如：<code>global_temp.xxx</code></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载指定数据源，创建 DataFrom 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建全局视图</span></span><br><span class="line">scala&gt; df.createGlobalTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行查询</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from global_temp.people&quot;</span>)</span><br><span class="line">res31: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; res31.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line"><span class="comment">// 全局视图在新的 session 中同样生效</span></span><br><span class="line">scala&gt; spark.newSession.sql(<span class="string">&quot;select * from global_temp.people&quot;</span>)</span><br><span class="line">res33: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; res33.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure>





<hr>
<h4 id="2-2-2-2-DSL-语法风格（了解）"><a href="#2-2-2-2-DSL-语法风格（了解）" class="headerlink" title="2.2.2.2    DSL 语法风格（了解）"></a>2.2.2.2    DSL 语法风格（了解）</h4><ul>
<li>  <code>DataFrame</code> 提供一个特定领域语言（<code>domain-specific language，DSL</code>）去管理结构化的数据。可以在 <code>Scala</code>、 <code>Java</code>、 <code>Python</code> 和 <code>R</code> 中使用 <code>DSL</code>。</li>
<li>  使用 <code>DSL</code> 语法风格就不必创建临时视图了。</li>
</ul>
<h5 id="1-查看-Schema-信息"><a href="#1-查看-Schema-信息" class="headerlink" title="1    查看 Schema 信息"></a>1    查看 Schema 信息</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载指定数据源，创建 DataFrom 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查看df数据集的元数据信息（字段信息）</span></span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line">|-- age: long (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- name: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>



<hr>
<h5 id="2-使用-DSL-查询"><a href="#2-使用-DSL-查询" class="headerlink" title="2    使用 DSL 查询"></a>2    使用 DSL 查询</h5><ol>
<li><p>只查询 <code>name</code> 列数据</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select($<span class="string">&quot;name&quot;</span>).show</span><br><span class="line">+-------+</span><br><span class="line">|   name|</span><br><span class="line">+-------+</span><br><span class="line">|<span class="type">Michael</span>|</span><br><span class="line">|   <span class="type">Andy</span>|</span><br><span class="line">| <span class="type">Justin</span>|</span><br><span class="line">+-------+</span><br><span class="line"></span><br><span class="line">scala&gt; df.select(<span class="string">&quot;name&quot;</span>).show</span><br><span class="line">+-------+</span><br><span class="line">|   name|</span><br><span class="line">+-------+</span><br><span class="line">|<span class="type">Michael</span>|</span><br><span class="line">|   <span class="type">Andy</span>|</span><br><span class="line">| <span class="type">Justin</span>|</span><br><span class="line">+-------+</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询 <code>name</code> 和 <code>age</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).show</span><br><span class="line">+-------+----+</span><br><span class="line">|   name| age|</span><br><span class="line">+-------+----+</span><br><span class="line">|<span class="type">Michael</span>|<span class="literal">null</span>|</span><br><span class="line">|   <span class="type">Andy</span>|  <span class="number">30</span>|</span><br><span class="line">| <span class="type">Justin</span>|  <span class="number">19</span>|</span><br><span class="line">+-------+----+</span><br></pre></td></tr></table></figure>

</li>
<li><p>查询 <code>name </code>和 <code>age + 1</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select($<span class="string">&quot;name&quot;</span>, $<span class="string">&quot;age&quot;</span> + <span class="number">1</span>).show</span><br><span class="line">+-------+---------+</span><br><span class="line">|   name|(age + <span class="number">1</span>)|</span><br><span class="line">+-------+---------+</span><br><span class="line">|<span class="type">Michael</span>|     <span class="literal">null</span>|</span><br><span class="line">|   <span class="type">Andy</span>|       <span class="number">31</span>|</span><br><span class="line">| <span class="type">Justin</span>|       <span class="number">20</span>|</span><br><span class="line">+-------+---------+</span><br></pre></td></tr></table></figure></li>
<li><p>   <strong>注意：涉及到运算的时候，每列都必须使用 <code>$</code>。</strong></p>
</li>
<li><p>查询 <code>age &gt; 20</code> 的数据</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.filter($<span class="string">&quot;age&quot;</span> &gt; <span class="number">21</span>).show</span><br><span class="line">+---+----+</span><br><span class="line">|age|name|</span><br><span class="line">+---+----+</span><br><span class="line">| <span class="number">30</span>|<span class="type">Andy</span>|</span><br><span class="line">+---+----+</span><br></pre></td></tr></table></figure></li>
<li><p>按照 <code>age</code> 分组，查看数据条数</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.groupBy(<span class="string">&quot;age&quot;</span>).count.show</span><br><span class="line">+----+-----+</span><br><span class="line">| age|count|</span><br><span class="line">+----+-----+</span><br><span class="line">|  <span class="number">19</span>|    <span class="number">1</span>|</span><br><span class="line">|<span class="literal">null</span>|    <span class="number">1</span>|</span><br><span class="line">|  <span class="number">30</span>|    <span class="number">1</span>|</span><br><span class="line">+----+-----+</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="2-2-3-RDD-和-DataFrame-的交互"><a href="#2-2-3-RDD-和-DataFrame-的交互" class="headerlink" title="2.2.3    RDD 和 DataFrame 的交互"></a>2.2.3    RDD 和 DataFrame 的交互</h3><h4 id="1、RDD-gt-DataFrame"><a href="#1、RDD-gt-DataFrame" class="headerlink" title="1、RDD =&gt; DataFrame"></a>1、<code>RDD =&gt; DataFrame</code></h4><p>涉及到 <code>RDD，DataFrame，DataSet</code> 之间的操作时，需要先导入 <code>import spark.implicits._</code>，这里的 <code>spark</code> 不是包名，而是表示 <code>SparkSession</code> 对象。所以必须先创建 <code>SparkSession</code> 对象再导入，<code>implicits</code> 是一个内部 <code>object</code>。</p>
<ol>
<li><p>首先创建一个 <code>RDD</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.txt&quot;</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = /opt/module/spark-local/examples/src/main/resources/people.txt <span class="type">MapPartitionsRDD</span>[<span class="number">10</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h5 id="手动转换"><a href="#手动转换" class="headerlink" title="手动转换"></a>手动转换</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = rdd1.map(line =&gt; &#123; <span class="keyword">val</span> paras = line.split(<span class="string">&quot;, &quot;</span>); (paras(<span class="number">0</span>), paras(<span class="number">1</span>).toInt)&#125;)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">11</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 RDD 转换为 DataFrame。转换的时候需要手动指定每个数据对应的字段名</span></span><br><span class="line">scala&gt; rdd2.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).show</span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|<span class="type">Michael</span>| <span class="number">29</span>|</span><br><span class="line">|   <span class="type">Andy</span>| <span class="number">30</span>|</span><br><span class="line">| <span class="type">Justin</span>| <span class="number">19</span>|</span><br><span class="line">+-------+---+</span><br></pre></td></tr></table></figure>





<h5 id="通过样例类的反射机制转换（常用）"><a href="#通过样例类的反射机制转换（常用）" class="headerlink" title="通过样例类的反射机制转换（常用）"></a>通过样例类的反射机制转换（常用）</h5><ol>
<li><p>创建样例类</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name :<span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">People</span></span></span><br></pre></td></tr></table></figure>

</li>
<li><p>使用样例把 <code>RDD</code> 转换成 <code>DataFrame</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = rdd1.map(line =&gt; &#123; <span class="keyword">val</span> paras = line.split(<span class="string">&quot;, &quot;</span>); <span class="type">People</span>(paras(<span class="number">0</span>), paras(<span class="number">1</span>).toInt) &#125;)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">People</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at map at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.toDF.show</span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|<span class="type">Michael</span>| <span class="number">29</span>|</span><br><span class="line">|   <span class="type">Andy</span>| <span class="number">30</span>|</span><br><span class="line">| <span class="type">Justin</span>| <span class="number">19</span>|</span><br><span class="line">+-------+---+</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h5 id="通过-API-的方式转换（了解）"><a href="#通过-API-的方式转换（了解）" class="headerlink" title="通过 API 的方式转换（了解）"></a>通过 API 的方式转换（了解）</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">IntegerType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataFrameDemo2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">        .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        .appName(<span class="string">&quot;RDD2DF&quot;</span>)</span><br><span class="line">        .getOrCreate()</span><br><span class="line">        <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;lisi&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;zs&quot;</span>, <span class="number">20</span>), (<span class="string">&quot;zhiling&quot;</span>, <span class="number">40</span>)))</span><br><span class="line">        <span class="comment">// 映射出来一个 RDD[Row], 因为 DataFrame其实就是 DataSet[Row]</span></span><br><span class="line">        <span class="keyword">val</span> rowRdd: <span class="type">RDD</span>[<span class="type">Row</span>] = rdd.map(x =&gt; <span class="type">Row</span>(x._1, x._2))</span><br><span class="line">        <span class="comment">// 创建 StructType 类型：用于指定 ROW 中每列的列名和列类型</span></span><br><span class="line">        <span class="keyword">val</span> types = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;name&quot;</span>, <span class="type">StringType</span>), <span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>, <span class="type">IntegerType</span>)))</span><br><span class="line">        <span class="comment">// 创建 DF 对象</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.createDataFrame(rowRdd, types)</span><br><span class="line">        df.show</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="2、从-DataFrame-到-RDD"><a href="#2、从-DataFrame-到-RDD" class="headerlink" title="2、从 DataFrame 到 RDD"></a>2、从 DataFrame 到 RDD</h4><p>从 <code>DataFrame</code> 到 <code>RDD</code>，直接调用 <code>DataFrame</code> 的 <code>rdd</code> 方法就可以完成转换。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1、加载数据源，创建 DataFrame 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2、DataFrame =&gt; RDD</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = df.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at rdd at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res0: <span class="type">Array</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">Array</span>([<span class="literal">null</span>,<span class="type">Michael</span>], [<span class="number">30</span>,<span class="type">Andy</span>], [<span class="number">19</span>,<span class="type">Justin</span>])</span><br></pre></td></tr></table></figure>

<ul>
<li>  <strong>说明：</strong>转换后得到的 <code>RDD</code> 对象中存储的数据类型是 <code>Row</code>。</li>
</ul>
<hr>
<h2 id="2-3-使用-DataSet-进行编程"><a href="#2-3-使用-DataSet-进行编程" class="headerlink" title="2.3    使用 DataSet 进行编程"></a>2.3    使用 DataSet 进行编程</h2><p><code>DataSet</code> 和 <code>RDD</code> 类似，但是 <code>DataSet</code> 没有使用 <code>Java</code> 序列化或者 <code>Kryo</code> 序列化，而是使用了一种专门的编码器去序列化对象，然后在网络上传输。</p>
<p>虽然编码器和标准序列化都负责将对象转换成字节，但编码器是动态生成的代码，使用的格式允许 <code>Spark</code> 执行许多操作，如过滤、排序和哈希，而无需将字节反序列化回对象。</p>
<p><strong><code>DataSet</code> 是一种强类型的数据集合，需要提供对应的类型信息</strong>。而 <code>DataFrame</code> 是一种弱类型的数据集合，<code>DF</code> 中只能存储 <code>ROW</code> 类型的数据，至于 <code>ROW</code> 的具体细节 <code>DF</code> 是不知道的。</p>
<hr>
<h3 id="2-3-1-创建-DataSet"><a href="#2-3-1-创建-DataSet" class="headerlink" title="2.3.1    创建 DataSet"></a>2.3.1    创建 DataSet</h3><blockquote>
<ol>
<li> 通过 Scala 序列（集合）得到</li>
<li> 通过 RDD 转换得到</li>
<li> 通过 DF 转换得到</li>
<li> 通过 DS 转换得到新的 DS</li>
</ol>
</blockquote>
<h4 id="1、通过-Scala-序列（集合）得到"><a href="#1、通过-Scala-序列（集合）得到" class="headerlink" title="1、通过 Scala 序列（集合）得到"></a>1、通过 Scala 序列（集合）得到</h4><p><strong>Demo01</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 13:26</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CreateDS</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 Scala 序列</span></span><br><span class="line">    <span class="keyword">val</span> mylist: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 转换成 DS</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">Int</span>] = mylist.toDS()</span><br><span class="line">    <span class="comment">// 转换成 DF</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = mylist.toDF()</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 显然，将一个集合转换成 DS 后，我们可以通过解析样例类得到其数据的具体类型；</span></span><br><span class="line"><span class="comment">    * 而将集合转换为 DF 后，虽然我们同样可以得到这份数据，但是无法识别数据每个字段的真实类型，</span></span><br><span class="line"><span class="comment">    * 所有的数据都是 ROW 类型的。</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ds.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">+-----+</span></span><br><span class="line"><span class="comment">|value|</span></span><br><span class="line"><span class="comment">+-----+</span></span><br><span class="line"><span class="comment">|   10|</span></span><br><span class="line"><span class="comment">|   20|</span></span><br><span class="line"><span class="comment">|   30|</span></span><br><span class="line"><span class="comment">+-----+</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">我们可以发现，我们没有指定列名，Spark 自动用 value 作为列名了。</span></span><br><span class="line"><span class="comment">如果我们想要使用自己的列名，则可以使用样例类</span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br></pre></td></tr></table></figure>



<p><strong>Demo02</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 13:26</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CreateDS2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 Scala 序列</span></span><br><span class="line">    <span class="keyword">val</span> persons: <span class="type">List</span>[<span class="type">Person</span>] = <span class="type">List</span>(<span class="type">Person</span>(<span class="string">&quot;张三&quot;</span>, <span class="number">20</span>), <span class="type">Person</span>(<span class="string">&quot;李四&quot;</span>, <span class="number">30</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 转换成 DS</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">Person</span>] = persons.toDS()</span><br><span class="line">    <span class="comment">// 转换成 DF</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = persons.toDF()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ds.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">+----+---+</span></span><br><span class="line"><span class="comment">|name|age|</span></span><br><span class="line"><span class="comment">+----+---+</span></span><br><span class="line"><span class="comment">|  张三| 20|</span></span><br><span class="line"><span class="comment">|  李四| 30|</span></span><br><span class="line"><span class="comment">+----+---+</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">临时表的列名使用的是样例类中的属性</span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br></pre></td></tr></table></figure>



<ol>
<li><p>使用样例类的序列（集合）得到 <code>DataSet</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个样例类</span></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 为样例类创建一个编码器</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;lisi&quot;</span>, <span class="number">20</span>), <span class="type">Person</span>(<span class="string">&quot;zs&quot;</span>, <span class="number">21</span>)).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: int]</span><br><span class="line"></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">scala&gt; ds.show</span><br><span class="line">+----+---+</span><br><span class="line">|name|age|</span><br><span class="line">+----+---+</span><br><span class="line">|lisi| <span class="number">20</span>|</span><br><span class="line">| zs| <span class="number">21</span>|</span><br><span class="line">+----+---+</span><br></pre></td></tr></table></figure>

</li>
<li><p>使用基本类型的序列（集合）得到 <code>DataSet</code></p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 基本类型的编码被自动创建. importing spark.implicits._</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Int</span>] = [value: int]</span><br><span class="line"></span><br><span class="line"><span class="comment">// </span></span><br><span class="line">scala&gt; ds.show</span><br><span class="line">+-----+</span><br><span class="line">|value|</span><br><span class="line">+-----+</span><br><span class="line">| <span class="number">1</span>|</span><br><span class="line">| <span class="number">2</span>|</span><br><span class="line">| <span class="number">3</span>|</span><br><span class="line">| <span class="number">4</span>|</span><br><span class="line">| <span class="number">5</span>|</span><br><span class="line">| <span class="number">6</span>|</span><br><span class="line">+-----+</span><br></pre></td></tr></table></figure>

<p>   说明：在实际使用的时候，很少用到把序列转换成 <code>DataSet</code>，更多的是通过 <code>RDD</code> 转换成 <code>DataSet</code>。</p>
</li>
</ol>
<hr>
<h3 id="2-3-2-RDD-和-DataSet-的交互"><a href="#2-3-2-RDD-和-DataSet-的交互" class="headerlink" title="2.3.2    RDD 和 DataSet 的交互"></a>2.3.2    RDD 和 DataSet 的交互</h3><h4 id="1、RDD-gt-DataSet"><a href="#1、RDD-gt-DataSet" class="headerlink" title="1、RDD =&gt; DataSet"></a>1、<code>RDD =&gt; DataSet</code></h4><p>使用反射来推断包含特定类型对象的 RDD 的 <code>schema</code>。</p>
<p>这种基于反射的方法可以生成更简洁的代码，并且当您在编写Spark应用程序时已经知道模式时，这种方法可以很好地工作。</p>
<p>为 <code>Spark SQL</code> 设计的 <code>Scala API</code> 可以自动的把包含样例类的 RDD 转换成 <code>DataSet</code>。</p>
<p>样例类定义了 <code>schema(表结构)</code>：样例类参数名通过反射被读到，然后成为列名。</p>
<p>样例类可以被嵌套，也可以包含复杂类型：像 <code>Seq</code> 或者 <code>Array</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> peopleRDD = sc.textFile(<span class="string">&quot;examples/src/main/resources/people.txt&quot;</span>)</span><br><span class="line">peopleRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = examples/src/main/resources/people.txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; peopleRDD.map(line =&gt; &#123;<span class="keyword">val</span> para = line.split(<span class="string">&quot;,&quot;</span>);<span class="type">Person</span>(para(<span class="number">0</span>),para(<span class="number">1</span>).trim.toInt)&#125;).toDS</span><br><span class="line">res0: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint]</span><br></pre></td></tr></table></figure>



<h4 id="2、DataSet-gt-RDD"><a href="#2、DataSet-gt-RDD" class="headerlink" title="2、DataSet =&gt; RDD"></a>2、<code>DataSet =&gt; RDD</code></h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">调用rdd方法即可</span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;lisi&quot;</span>, <span class="number">40</span>), <span class="type">Person</span>(<span class="string">&quot;zs&quot;</span>, <span class="number">20</span>)).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 把 ds 转换成 rdd</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = ds.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Person</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">8</span>] at rdd at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res5: <span class="type">Array</span>[<span class="type">Person</span>] = <span class="type">Array</span>(<span class="type">Person</span>(lisi,<span class="number">40</span>), <span class="type">Person</span>(zs,<span class="number">20</span>))</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="2-4-DataFrame-和-DataSet-之间的交互"><a href="#2-4-DataFrame-和-DataSet-之间的交互" class="headerlink" title="2.4    DataFrame 和 DataSet 之间的交互"></a>2.4    DataFrame 和 DataSet 之间的交互</h2><h3 id="2-4-1-DataFrame-gt-DataSet"><a href="#2-4-1-DataFrame-gt-DataSet" class="headerlink" title="2.4.1    DataFrame =&gt; DataSet"></a>2.4.1    <code>DataFrame =&gt; DataSet</code></h3><p><strong>spark-shell</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">People</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrame 转换成 DataSet</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = df.as[<span class="type">People</span>]</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">People</span>] = [age: bigint, name: string]</span><br></pre></td></tr></table></figure>

<p><strong>IDEA</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 13:49</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DF2DS</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、通过 SparkSession 创建 DataFrame 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 我们知道，相比于 DF 而言，DS 可以查看其内部保存数据的具体类型，</span></span><br><span class="line"><span class="comment">    * 而 DF 则是将所有的类型都用 ROW 类型来处理。</span></span><br><span class="line"><span class="comment">    * 所以如果想要实现 DF =&gt; DS 的转换，一般需要提供一个样例类用于将 ROW 类型的数据映射到样例类上</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="comment">// 5、DF =&gt; DS</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User</span>] = df.as[<span class="type">User</span>]</span><br><span class="line"></span><br><span class="line">    ds.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6、关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4、样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">  +---+----+</span></span><br><span class="line"><span class="comment">  |age|name|</span></span><br><span class="line"><span class="comment">  +---+----+</span></span><br><span class="line"><span class="comment">  | 15|  ls|</span></span><br><span class="line"><span class="comment">  | 21|  ww|</span></span><br><span class="line"><span class="comment">  | 22|  zs|</span></span><br><span class="line"><span class="comment">  | 23|  zl|</span></span><br><span class="line"><span class="comment">  +---+----+</span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br></pre></td></tr></table></figure>





<h3 id="2-4-2-DataSet-gt-DataFrame"><a href="#2-4-2-DataSet-gt-DataFrame" class="headerlink" title="2.4.2    DataSet =&gt; DataFrame"></a>2.4.2    <code>DataSet =&gt; DataFrame</code></h3><p><strong>DS =&gt; DF</strong></p>
<ol>
<li> 不需要隐式类型转换</li>
<li> 方法：<code>ds.toDF</code></li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;Andy&quot;</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = ds.toDF</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+----+---+</span><br><span class="line">|name|age|</span><br><span class="line">+----+---+</span><br><span class="line">|<span class="type">Andy</span>| <span class="number">32</span>|</span><br><span class="line">+----+---+</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="2-5-RDD-DataFrame-和-DataSet-之间的关系"><a href="#2-5-RDD-DataFrame-和-DataSet-之间的关系" class="headerlink" title="2.5    RDD, DataFrame 和 DataSet 之间的关系"></a>2.5    RDD, DataFrame 和 DataSet 之间的关系</h2><p>在 <code>SparkSQL</code> 为我们提供了两个新的抽象，分别是 <code>DataFrame</code> 和 <code>DataSet</code>。它们和 <code>RDD</code> 有什么区别呢？首先从版本的产生上来看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)</span><br></pre></td></tr></table></figure>

<p>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。</p>
<p>在后期的 <code>Spark</code> 版本中， <code>DataSet</code> 会逐步取代 <code>RDD</code> 和 <code>DataFrame</code> 成为唯一的 <code>API</code> 接口。</p>
<h3 id="2-5-1-三者的共性"><a href="#2-5-1-三者的共性" class="headerlink" title="2.5.1    三者的共性"></a>2.5.1    三者的共性</h3><ol>
<li>   <code>RDD</code>、 <code>DataFrame</code>、 <code>Dataset</code> 全都是 <code>Spark</code> 平台下的分布式弹性数据集，为处理超大型数据提供便利</li>
<li>   三者都有惰性机制，在进行创建、转换，如 <code>map</code> 方法时，不会立即执行，只有在遇到 <code>action</code> 时，三者才会开始遍历运算。</li>
<li>   三者都会根据 <code>Spark</code> 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</li>
<li>   三者都有 <code>partition</code> 的概念</li>
<li>   三者有许多共同的函数，如 <code>map</code>， <code>filter</code>，排序等</li>
<li>   在对 <code>DataFrame</code> 和 <code>Dataset</code> 进行操作许多操作都需要这个包进行支持 <code>import [spark].implicits._</code></li>
<li>   <code>DataFrame</code> 和 <code>Dataset</code> 均可使用模式匹配获取各个字段的值和类型</li>
</ol>
<h3 id="2-5-2-三者的区别"><a href="#2-5-2-三者的区别" class="headerlink" title="2.5.2    三者的区别"></a>2.5.2    三者的区别</h3><h4 id="2-5-2-1-RDD"><a href="#2-5-2-1-RDD" class="headerlink" title="2.5.2.1    RDD"></a>2.5.2.1    RDD</h4><ol>
<li>   <code>RDD</code> 一般和 <code>spark mllib</code> 同时使用</li>
<li>   <code>RDD</code> 不支持 <code>spark sql</code> 操作</li>
</ol>
<h4 id="2-5-2-2-DataFrame"><a href="#2-5-2-2-DataFrame" class="headerlink" title="2.5.2.2    DataFrame"></a>2.5.2.2    DataFrame</h4><ol>
<li>   与 <code>RDD</code> 和 <code>Dataset</code> 不同，<code>DataFrame</code>每一行的类型固定为 <code>Row</code>，每一列的值没法直接访问，只有通过解析才能获取各个字段的值</li>
<li>   <code>DataFrame</code> 与 <code>DataSet</code> 一般不与 <code>spark mlib</code> 同时使用</li>
<li>   <code>DataFrame</code> 与 <code>DataSet</code> 均支持 <code>SparkSQL</code> 的操作，比如 <code>select</code>， <code>groupby</code> 之类，还能注册临时表/视窗，进行 <code>sql</code> 语句操作</li>
<li>   <code>DataFrame</code> 与 <code>DataSet</code> 支持一些特别方便的保存方式，比如保存成 <code>csv</code>，可以带上表头，这样每一列的字段名一目了然（后面专门讲解）</li>
</ol>
<h4 id="2-5-2-3-DataSet"><a href="#2-5-2-3-DataSet" class="headerlink" title="2.5.2.3    DataSet"></a>2.5.2.3    DataSet</h4><ol>
<li>   <code>Dataset</code> 和 <code>DataFrame</code> 拥有完全相同的成员函数，区别只是每一行的数据类型不同。</li>
<li>   <code>DataFrame</code> 其实就是 <code>DataSet</code> 的一个特例</li>
<li>   <code>DataFrame</code> 也可以叫 <code>Dataset[Row]</code>，每一行的类型是 <code>Row</code>，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 <code>getAS</code> 方法或者共性中的第七条提到的模式匹配拿出特定字段。而 <code>Dataset</code> 中，每一行是什么类型是不一定的，在自定义了 <code>case class</code> 之后可以很自由的获得每一行的信息</li>
</ol>
<h3 id="2-5-3-三者的互相转换"><a href="#2-5-3-三者的互相转换" class="headerlink" title="2.5.3    三者的互相转换"></a>2.5.3    三者的互相转换</h3><ul>
<li><code>基础类型 =&gt; 高级类型</code>：<ol>
<li> 需要提供样例类</li>
<li> 需要导入隐式转换</li>
</ol>
</li>
<li><code>高级类型 =&gt; 基础类型</code>：<ol>
<li> 无需提供额外的信息，可以直接完成类型之间的转换</li>
</ol>
</li>
</ul>
<p><img src="/2021/12/11/Spark-SQL/image-20211221135048075.png" alt="image-20211221135048075"></p>
<hr>
<h2 id="2-6-使用-IDEA-创建-SparkSQL-程序"><a href="#2-6-使用-IDEA-创建-SparkSQL-程序" class="headerlink" title="2.6    使用 IDEA 创建 SparkSQL 程序"></a>2.6    使用 IDEA 创建 SparkSQL 程序</h2><h3 id="步骤1：添加-SparkSQL-依赖"><a href="#步骤1：添加-SparkSQL-依赖" class="headerlink" title="步骤1：添加 SparkSQL 依赖"></a>步骤1：添加 SparkSQL 依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="步骤2：具体代码"><a href="#步骤2：具体代码" class="headerlink" title="步骤2：具体代码"></a>步骤2：具体代码</h3><p><code>user.json</code></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;ls&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span>  <span class="number">15</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;ww&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span>  <span class="number">21</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;zs&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span>  <span class="number">22</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;zl&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span>  <span class="number">23</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>



<h4 id="1、创建-DF-对象"><a href="#1、创建-DF-对象" class="headerlink" title="1、创建 DF 对象"></a>1、创建 DF 对象</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/22 9:57</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 创建 DF 对象</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CreateDF</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 1、先创建一个 SparkSession 对象【构造器模式】</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;Word Count&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、通过 SparkSession 创建 DataFrame 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、对 DF 做各种操作</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建临时表</span></span><br><span class="line">    df.createTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">    <span class="comment">// 查询临时表</span></span><br><span class="line">    spark.sql(<span class="string">&quot;select * from user where age &gt; 19&quot;</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、关闭连接</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>Tip：</strong>在导入隐式转换时，使用到的 <code>spark</code> 关键字是由 <code>SparkSession</code> 的实例对象名来决定的，也就是说，如果我们创建的 <code>SparkSession</code> 实例名字叫 <code>hello</code>，那么在导入隐式转换时就应该导入 <code>import hello.implicits._</code></p>
<p><img src="/2021/12/11/Spark-SQL/image-20211224130331108.png" alt="image-20211224130331108"></p>
<h4 id="2、DF-gt-RDD"><a href="#2、DF-gt-RDD" class="headerlink" title="2、DF =&gt; RDD"></a>2、<code>DF =&gt; RDD</code></h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/22 10:23</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DF2RDD</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;Word Count&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个 sc 对象</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 得到一个 DF 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = (<span class="number">1</span> to <span class="number">10</span>).toDF(<span class="string">&quot;num&quot;</span>)</span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// df =&gt; rdd</span></span><br><span class="line">    <span class="comment">// 由 df 转换的来的 rdd 的泛型一定是 [Row]</span></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Row</span>] = df.rdd</span><br><span class="line">    rdd.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 取出 rdd 中每个 row 中的数据</span></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.map(row =&gt; row.getInt(<span class="number">0</span>))</span><br><span class="line">    rdd2.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h4 id="3、RDD-gt-DF"><a href="#3、RDD-gt-DF" class="headerlink" title="3、RDD =&gt; DF"></a>3、<code>RDD =&gt; DF</code></h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/22 10:08</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDD2DF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、先创建一个 SparkSession 对象【构造器模式】</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;Word Count&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、创建一个 sc 对象</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、导入用到的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、如果 RDD 中的数据保存在元组中，则在执行 toDF 方法时需要显式指定每列的名称</span></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.parallelize((<span class="string">&quot;ls&quot;</span>, <span class="number">10</span>) :: (<span class="string">&quot;zs&quot;</span>, <span class="number">20</span>) :: <span class="type">Nil</span>)</span><br><span class="line">    rdd.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5、如果 RDD 中保存的数据是一个样例类对象，则在执行 toDF 方法时无需手动指定每列的名称就可完成自动解析，</span></span><br><span class="line">    <span class="comment">// 因为样例中已经有了属性做列名</span></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">User</span>] = sc.parallelize(<span class="type">Array</span>(<span class="type">User</span>(<span class="string">&quot;路飞&quot;</span>, <span class="number">10</span>), <span class="type">User</span>(<span class="string">&quot;黄猿&quot;</span>, <span class="number">53</span>)))</span><br><span class="line">    rdd2.toDF().show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6、关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="2-7-自定义-SparkSQL-函数"><a href="#2-7-自定义-SparkSQL-函数" class="headerlink" title="2.7    自定义 SparkSQL 函数"></a>2.7    自定义 SparkSQL 函数</h2><p>在 <code>Shell</code> 窗口中可以通过 <code>spark.udf</code> 功能自定义函数。</p>
<hr>
<h3 id="2-7-1-自定义-UDF-函数"><a href="#2-7-1-自定义-UDF-函数" class="headerlink" title="2.7.1    自定义 UDF 函数"></a>2.7.1    自定义 UDF 函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个 udf 函数: toUpper是函数名, 第二个参数是函数的具体实现</span></span><br><span class="line">scala&gt; spark.udf.register(<span class="string">&quot;toUpper&quot;</span>, (s: <span class="type">String</span>) =&gt; s.toUpperCase)</span><br><span class="line">res1: org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span> = <span class="type">UserDefinedFunction</span>(&lt;function1&gt;,<span class="type">StringType</span>,<span class="type">Some</span>(<span class="type">List</span>(<span class="type">StringType</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 测试自定义的 UDF 函数</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select toUpper(name), age from people&quot;</span>).show</span><br><span class="line">+-----------------+----+</span><br><span class="line">|<span class="type">UDF</span>:toUpper(name)| age|</span><br><span class="line">+-----------------+----+</span><br><span class="line">|          <span class="type">MICHAEL</span>|<span class="literal">null</span>|</span><br><span class="line">|             <span class="type">ANDY</span>|  <span class="number">30</span>|</span><br><span class="line">|           <span class="type">JUSTIN</span>|  <span class="number">19</span>|</span><br><span class="line">+-----------------+----+</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="2-7-2-用户自定义聚合函数-UDAF"><a href="#2-7-2-用户自定义聚合函数-UDAF" class="headerlink" title="2.7.2    用户自定义聚合函数(UDAF)"></a>2.7.2    用户自定义聚合函数(UDAF)</h3><p>强类型的 <code>Dataset</code> 和弱类型的 <code>DataFrame</code> 都提供了相关的聚合函数，如 <code>count()</code>， <code>countDistinct()</code>， <code>avg()</code>， <code>max()</code>， <code>min()</code>。除此之外，用户还可以设定自己的自定义聚合函数。</p>
<p>自定义聚合函数需要继承 <code>UserDefinedAggregateFunction</code></p>
<p><strong>自定义聚合函数实现 sum</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataType</span>, <span class="type">DoubleType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 14:18</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *          自定义聚合函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UDAFDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 DF 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建临时视图</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册自定义函数</span></span><br><span class="line">    spark.udf.register(<span class="string">&quot;mySum&quot;</span>, <span class="keyword">new</span> <span class="type">CustomSum</span>())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用自定义的求和函数 mySum 执行聚合查询</span></span><br><span class="line">    spark.sql(<span class="string">&quot;select mySum(age) from user&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/* 执行以上代码，输出结果为：</span></span><br><span class="line"><span class="comment">--------------------------------------</span></span><br><span class="line"><span class="comment">  +------------------------------+</span></span><br><span class="line"><span class="comment">  |customsum(CAST(age AS DOUBLE))|</span></span><br><span class="line"><span class="comment">  +------------------------------+</span></span><br><span class="line"><span class="comment">  |                          81.0|</span></span><br><span class="line"><span class="comment">  +------------------------------+</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">----------------------------------- */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义 UDAF 函数</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 1、自定义 UDAF 函数需要继承 UserDefinedAggregateFunction 类</span></span><br><span class="line"><span class="comment"> * 2、CustomSum 函数的功能等价于 Sum</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomSum</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定输入的类型。</span></span><br><span class="line"><span class="comment">  * 比如，在 select mySum(age) from User 中，age 的类型就是输入类型</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="comment">// 在自定义函数 mySum(age) 中，只有一个参数，且指定类型为 Double</span></span><br><span class="line">    <span class="comment">// 如果有多个参数，则需要指定多个 StructField</span></span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定缓冲区数据的类型。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 在计算聚合函数时（以 sum 为例），需要先计算出前两数之和，将临时结果放入缓冲区，</span></span><br><span class="line"><span class="comment">  * 再使用缓冲区中的结果与下一个数进行求和。就类似于 flap</span></span><br><span class="line"><span class="comment">  * 缓冲区的类型也需要手动指定</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;tmp&quot;</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定聚合函数执行结束后，最终返回结果的类型</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = &#123;</span><br><span class="line">    <span class="type">DoubleType</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定查询结果是否具有幂等性</span></span><br><span class="line"><span class="comment">  * （即多次查询是否能得到统一结果）</span></span><br><span class="line"><span class="comment">  * 一般都设置为 true</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 初始化缓冲区</span></span><br><span class="line"><span class="comment">  *   对于求和函数而言，初始化值应该为 0.00</span></span><br><span class="line"><span class="comment">  *   并且由于缓冲区可能不只缓冲一个字段，所以缓冲区的数据结构为可变集合</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// buffer 就是缓冲区中数据的集合</span></span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>D</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 分区内聚合：</span></span><br><span class="line"><span class="comment">   * 由于 RDD 是分布式数据集，所以聚合又可分为分区内聚合和分区间聚合</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param buffer 缓冲区内的值</span></span><br><span class="line"><span class="comment">   * @param input 传入缓冲区中的值</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 分区内求和</span></span><br><span class="line">    input <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="comment">// 如果收到的参数是 Double 类型，则取出 input 对象中的值，通过模式匹配匹配到 age 中，执行累加</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(age: <span class="type">Double</span>) =&gt; buffer(<span class="number">0</span>) = buffer.getDouble(<span class="number">0</span>) + age</span><br><span class="line">      <span class="comment">// 其它情况</span></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 分区间聚合</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param buffer1 一个分区的聚合结果</span></span><br><span class="line"><span class="comment">   * @param buffer2 另一个分区的聚合结果</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 分区间求和</span></span><br><span class="line">    <span class="comment">// 把 buffer1 和 buffer2 临时结果聚合到一起，然后再把值写回到 buffer1</span></span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getDouble(<span class="number">0</span>) + buffer2.getDouble(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 返回最终的输出值</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer.getDouble(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>自定义聚合函数实现 avg</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> day01.udf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataType</span>, <span class="type">DoubleType</span>, <span class="type">LongType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/24 14:18</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *          自定义聚合函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UDAFDemo2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 DF 对象</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建临时视图</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册自定义函数</span></span><br><span class="line">    spark.udf.register(<span class="string">&quot;myAvg&quot;</span>, <span class="keyword">new</span> <span class="type">CustomAvg</span>())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用自定义的求和函数 mySum 执行聚合查询</span></span><br><span class="line">    spark.sql(<span class="string">&quot;select myAvg(age) from user&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomAvg</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定输入的类型。</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="comment">// 在自定义函数 myAvg(age) 中，只有一个参数，且指定类型为 Double</span></span><br><span class="line">    <span class="comment">// 如果有多个参数，则需要指定多个 StructField</span></span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定缓冲区数据的类型。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 在计算聚合函数时（以 sum 为例），需要先计算出前两数之和，将临时结果放入缓冲区，</span></span><br><span class="line"><span class="comment">  * 再使用缓冲区中的结果与下一个数进行求和。就类似于 flap</span></span><br><span class="line"><span class="comment">  * 缓冲区的类型也需要手动指定</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="comment">// count 用于计算 User 的个数</span></span><br><span class="line">    <span class="comment">// sum 用于计算 User 中 age 的累加和</span></span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>) :: <span class="type">StructField</span>(<span class="string">&quot;sum&quot;</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定聚合函数执行结束后，最终返回结果的类型</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = &#123;</span><br><span class="line">    <span class="type">DoubleType</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 用于指定查询结果是否具有幂等性</span></span><br><span class="line"><span class="comment">  * （即多次查询是否能得到统一结果）</span></span><br><span class="line"><span class="comment">  * 一般都设置为 true</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 初始化缓冲区</span></span><br><span class="line"><span class="comment">  *   对于求和函数而言，初始化值应该为 0.00</span></span><br><span class="line"><span class="comment">  *   并且由于缓冲区可能不只缓冲一个字段，所以缓冲区的数据结构为可变集合</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// buffer 就是缓冲区中数据的集合</span></span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L  <span class="comment">// 缓冲区中 count 的初始值</span></span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>D  <span class="comment">// 缓冲区中 sum 的初始值</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 分区内聚合：</span></span><br><span class="line"><span class="comment">   * 由于 RDD 是分布式数据集，所以聚合又可分为分区内聚合和分区间聚合</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param buffer 缓冲区内的值</span></span><br><span class="line"><span class="comment">   * @param input 传入缓冲区中的值</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 分区内求和</span></span><br><span class="line">    input <span class="keyword">match</span> &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(age: <span class="type">Double</span>) =&gt; &#123;</span><br><span class="line">        buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + <span class="number">1</span>L</span><br><span class="line">        buffer(<span class="number">1</span>) = buffer.getDouble(<span class="number">1</span>) + age</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 其它情况</span></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 分区间聚合</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param buffer1 一个分区的聚合结果</span></span><br><span class="line"><span class="comment">   * @param buffer2 另一个分区的聚合结果</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 分区间求和</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 把 buffer1 和 buffer2 临时结果聚合到一起，然后再把值写回到 buffer1</span></span><br><span class="line">    buffer2 <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(count : <span class="type">Long</span>, age: <span class="type">Double</span>) =&gt; &#123;</span><br><span class="line">        buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + count</span><br><span class="line">        buffer1(<span class="number">1</span>) = buffer1.getDouble(<span class="number">1</span>) + age</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 其它情况</span></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 返回最终的输出值</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer.getDouble(<span class="number">1</span>) / buffer.getLong(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="第-3-章-SparkSQL-数据源"><a href="#第-3-章-SparkSQL-数据源" class="headerlink" title="第 3 章    SparkSQL 数据源"></a>第 3 章    SparkSQL 数据源</h1><p>本章介绍 <code>SparkSQL</code> 支持的各种数据源<code>(Data Sources)</code>。</p>
<p><code>Spark SQL</code> 的 <code>DataFrame</code> 接口支持操作多种数据源。一个 <code>DataFrame</code> 类型的对象可以像 <code>RDD</code> 那样操作（比如各种转换），也可以用来创建临时表。把 <code>DataFrame</code> 注册为一个临时表之后，就可以在它的数据上面执行 <code>SQL</code> 查询。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">读</span><br><span class="line">	通用读：spark.read.format(&quot;文件格式&quot;).load(&quot;文件路径&quot;)</span><br><span class="line">	专用读：spark.read.文件格式(&quot;文件路径&quot;)</span><br><span class="line"></span><br><span class="line">写</span><br><span class="line">	通用写：df.write.format(&quot;文件格式&quot;).save(&quot;文件路径&quot;)</span><br><span class="line">	专用写：df.write.文件格式(&quot;文件路径&quot;)</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="3-1-通用加载和保存函数"><a href="#3-1-通用加载和保存函数" class="headerlink" title="3.1    通用加载和保存函数"></a>3.1    通用加载和保存函数</h2><p>默认数据源格式是 <code>parquet</code>，我们也可以通过使用 <code>spark.sql.sources.default</code> 这个属性来设置默认的数据源格式。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载数据源中的数据到 DF 中</span></span><br><span class="line"><span class="keyword">val</span> usersDF = spark.read.load(<span class="string">&quot;examples/src/main/resources/users.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DF 中的数据写出到目的地</span></span><br><span class="line">usersDF.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;favorite_color&quot;</span>).write.save(<span class="string">&quot;namesAndFavColors.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>说明：</strong></p>
<ol>
<li>   <code>spark.read.load</code> 是加载数据的通用方法。</li>
<li>   <code>df.write.save</code> 是保存数据的通用方法。</li>
</ol>
<h3 id="3-1-1-手动指定选项"><a href="#3-1-1-手动指定选项" class="headerlink" title="3.1.1    手动指定选项"></a>3.1.1    手动指定选项</h3><p>也可以手动给数据源指定一些额外的选项。数据源应该用全名称来指定，但是对一些内置的数据源也可以使用短名称：<code>json</code>，<code>parquet</code>，<code>jdbc</code>，<code>orc,</code>libsvm<code>,</code>csv<code>，text</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// </span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// </span></span><br><span class="line">peopleDF.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).write.format(<span class="string">&quot;parquet&quot;</span>).save(<span class="string">&quot;namesAndAges.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>





<h3 id="3-1-2-在文件上直接运行-SQL"><a href="#3-1-2-在文件上直接运行-SQL" class="headerlink" title="3.1.2    在文件上直接运行 SQL"></a>3.1.2    在文件上直接运行 SQL</h3><p>我们前面都是使用 <code>read API</code> 先把文件加载到 <code>DataFrame</code>，然后再查询。其实，我们也可以直接在文件上进行查询。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from json.examples/src/main/resources/people.json&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>说明：<code>json</code> 表示文件的格式。后面的文件具体路径需要用反引号括起来。</p>
<h3 id="3-1-3-文件保存选项-SaveMode"><a href="#3-1-3-文件保存选项-SaveMode" class="headerlink" title="3.1.3    文件保存选项(SaveMode)"></a>3.1.3    文件保存选项(SaveMode)</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 方式一：</span></span><br><span class="line">df.write.format(<span class="string">&quot;文件格式&quot;</span>).mode(<span class="string">&quot;模式名称&quot;</span>).save(<span class="string">&quot;文件路径&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方式二：</span></span><br><span class="line">df.write.mode(<span class="string">&quot;模式名称&quot;</span>).文件格式(<span class="string">&quot;文件路径&quot;</span>)</span><br></pre></td></tr></table></figure>



<p>保存操作可以使用 <code>SaveMode</code>，用来指明如何处理数据。使用 <code>mode()</code> 方法来设置。</p>
<p>有一点很重要：这些 <code>SaveMode</code> 都是没有加锁的，也不是原子操作。还有，如果你执行的是 <code>Overwrite</code> 操作，在写入新的数据之前会先删除旧的数据。</p>
<table>
<thead>
<tr>
<th align="left">Scala/Java</th>
<th>Any Language</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>SaveMode.ErrorIfExists(default)</code></td>
<td><code>&quot;error&quot;(default)</code></td>
<td>如果文件已经存在则抛出异常</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Append</code></td>
<td><code>&quot;append&quot;</code></td>
<td>如果文件已经存在则追加</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Overwrite</code></td>
<td><code>&quot;overwrite&quot;</code></td>
<td>如果文件已经存在则覆盖</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Ignore</code></td>
<td><code>&quot;ignore&quot;</code></td>
<td>如果文件已经存在则忽略</td>
</tr>
</tbody></table>
<hr>
<h2 id="3-2-加载-JSON-文件"><a href="#3-2-加载-JSON-文件" class="headerlink" title="3.2    加载 JSON 文件"></a>3.2    加载 JSON 文件</h2><p><code>Spark SQL</code> 能够自动推测 <code>JSON</code> 数据集的结构，并将它加载为一个 <code>Dataset[Row]</code>。</p>
<p>可以通过 <code>SparkSession.read.json()</code> 去加载一个 <code>JSON</code> 文件。也可以通过 <code>SparkSession.read.format(&quot;json&quot;).load()</code> 来加载。</p>
<p>注意：这个 <code>JSON</code> 文件不是一个传统的 <code>JSON</code> 文件，每一行都必须是一个完整的 <code>JSON</code> 串。</p>
<p><strong>json 文件</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lisi&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span> <span class="punctuation">:</span> <span class="number">20</span><span class="punctuation">,</span> <span class="attr">&quot;friends&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;lisi&quot;</span><span class="punctuation">,</span> <span class="string">&quot;zs&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;zs&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span> <span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span> <span class="attr">&quot;friends&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;lisi&quot;</span><span class="punctuation">,</span> <span class="string">&quot;zs&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wangwu&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;age&quot;</span> <span class="punctuation">:</span> <span class="number">15</span><span class="punctuation">,</span> <span class="attr">&quot;friends&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;lisi&quot;</span><span class="punctuation">,</span> <span class="string">&quot;zs&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p><strong>代码</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataSourceDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;target/classes/user.json&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User</span>] = df.as[<span class="type">User</span>]</span><br><span class="line">        ds.foreach(user =&gt; println(user.friends(<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age: <span class="type">Long</span>, friends: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span></span><br></pre></td></tr></table></figure>



<hr>
<h2 id="3-3-读取-Parquet-文件"><a href="#3-3-读取-Parquet-文件" class="headerlink" title="3.3    读取 Parquet 文件"></a>3.3    读取 Parquet 文件</h2><p><code>Parquet</code> 是一种流行的列式存储格式，可以高效地存储具有嵌套字段的记录。<code>Parquet</code> 格式经常在 <code>Hadoop</code> 生态圈中被使用，它也支持 <code>Spark SQL</code> 的全部数据类型。 <code>Spark SQL</code> 提供了直接读取和存储 <code>Parquet</code> 格式文件的方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataSourceDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> jsonDF: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;target/classes/user.json&quot;</span>)</span><br><span class="line">        jsonDF.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).parquet(<span class="string">&quot;target/classes/user.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> parDF: <span class="type">DataFrame</span> = spark.read.parquet(<span class="string">&quot;target/classes/user.parquet&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> userDS: <span class="type">Dataset</span>[<span class="type">User</span>] = parDF.as[<span class="type">User</span>]</span><br><span class="line">        userDS.map(user =&gt; &#123;user.name = <span class="string">&quot;zl&quot;</span>; user.friends(<span class="number">0</span>) = <span class="string">&quot;志玲&quot;</span>;user&#125;).show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">var name:<span class="type">String</span>, age: <span class="type">Long</span>, friends: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong> <code>Parquet</code> 格式的文件是 <code>Spark</code> 默认格式的数据源。所以，当使用通用的方式时可以直接保存和读取，而不需要使用 <code>format spark.sql.sources.default</code> 这个配置来修改默认数据源。</p>
<hr>
<h2 id="3-4-JDBC"><a href="#3-4-JDBC" class="headerlink" title="3.4    JDBC"></a>3.4    JDBC</h2><p><code>Spark SQL</code> 也支持使用 <code>JDBC</code> 从其它的数据库中读取数据。<code>JDBC</code> 数据源比使用 <code>JdbcRDD</code> 更爽一些，这是因为返回的结果直接就是一个 <code>DataFrame</code>，<code>DataFrame</code> 更加容易被处理或者与其它的数据源进行 <code>join</code>。</p>
<p><code>Spark SQL</code> 可以通过 <code>JDBC</code> 从关系型数据库中读取数据的方式创建 <code>DataFrame</code>，通过对 <code>DataFrame</code> 一系列的计算后，还可以将数据再写回关系型数据库中。</p>
<p>注意：如果想在 <code>spark-shell</code> 中操作 <code>jdbc</code>，需要把相关的 <code>jdbc</code> 驱动拷贝到 <code>jars</code> 目录下。</p>
<p><strong>导入依赖：</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.40<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="3-4-1-从-JDBC-读数据"><a href="#3-4-1-从-JDBC-读数据" class="headerlink" title="3.4.1    从 JDBC 读数据"></a>3.4.1    从 JDBC 读数据</h3><p>可以使用通用的 <code>load</code> 方法，也可以使用 <code>jdbc</code> 方法。</p>
<ol>
<li><p>使用通用的 <code>load</code> 方法加载</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 11:20</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">    <span class="keyword">val</span> user = <span class="string">&quot;root&quot;</span></span><br><span class="line">    <span class="keyword">val</span> password = <span class="string">&quot;123456&quot;</span></span><br><span class="line">    <span class="keyword">val</span> dbtable = <span class="string">&quot;users&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 通过 JDBC 读取数据到 DF 中</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">      .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;url&quot;</span>, url)</span><br><span class="line">      .option(<span class="string">&quot;user&quot;</span>, user)</span><br><span class="line">      .option(<span class="string">&quot;password&quot;</span>, password)</span><br><span class="line">      .option(<span class="string">&quot;dbtable&quot;</span>, dbtable)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
<li><p>专用方法加载数据</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 11:20</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCDemo2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">    <span class="keyword">val</span> user = <span class="string">&quot;root&quot;</span></span><br><span class="line">    <span class="keyword">val</span> password = <span class="string">&quot;123456&quot;</span></span><br><span class="line">    <span class="keyword">val</span> dbtable = <span class="string">&quot;users&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> properties: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.put(<span class="string">&quot;user&quot;</span>, user)</span><br><span class="line">    properties.put(<span class="string">&quot;password&quot;</span>, password)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过 JDBC 读取数据到 DF 中</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">      .jdbc(url, dbtable, properties)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="3-4-2-向-JDBC-写入数据"><a href="#3-4-2-向-JDBC-写入数据" class="headerlink" title="3.4.2    向 JDBC 写入数据"></a>3.4.2    向 JDBC 写入数据</h3><p>也分两种方法：通用 <code>write.save</code> 和 <code>write.jdbc</code></p>
<p><strong>通用写法</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 11:38</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCWriteDemo01</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">URL</span> = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">USER</span> = <span class="string">&quot;root&quot;</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">PASSWORD</span> = <span class="string">&quot;123456&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 1、先读取 json 文件中的数据</span></span><br><span class="line"><span class="comment">    * 2、再写入数据库中</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="comment">// 读</span></span><br><span class="line">    <span class="keyword">val</span> source: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 写。如果表不存在，会自动创建</span></span><br><span class="line">    source.write</span><br><span class="line">      .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;url&quot;</span>, <span class="type">URL</span>)</span><br><span class="line">      .option(<span class="string">&quot;user&quot;</span>, <span class="type">USER</span>)</span><br><span class="line">      .option(<span class="string">&quot;password&quot;</span>, <span class="type">PASSWORD</span>)</span><br><span class="line">      .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;new_users&quot;</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)	<span class="comment">// SaveMode 是一个枚举类</span></span><br><span class="line">      .save()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>  代码执行完毕后可在数据库中查询是否写数据成功</li>
</ul>
<p><strong>专用写法</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 11:38</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCWriteDemo02</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">URL</span> = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">USER</span> = <span class="string">&quot;root&quot;</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">PASSWORD</span> = <span class="string">&quot;123456&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 1、先读取 json 文件中的数据</span></span><br><span class="line"><span class="comment">    * 2、再写入数据库中</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="comment">// 读</span></span><br><span class="line">    <span class="keyword">val</span> source: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 写。如果表不存在，会自动创建</span></span><br><span class="line">    <span class="keyword">val</span> properties: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.put(<span class="string">&quot;user&quot;</span>, <span class="type">USER</span>)</span><br><span class="line">    properties.put(<span class="string">&quot;password&quot;</span>, <span class="type">PASSWORD</span>)</span><br><span class="line"></span><br><span class="line">    source.write</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">      .jdbc(<span class="type">URL</span>, table = <span class="string">&quot;new_users&quot;</span>, properties)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="3-5-Hive-数据库（重要）"><a href="#3-5-Hive-数据库（重要）" class="headerlink" title="3.5    Hive 数据库（重要）"></a>3.5    Hive 数据库（重要）</h2><p><code>Apache Hive</code> 是 <code>Hadoop</code> 上的 <code>SQL</code> 引擎，<code>Spark SQL</code> 编译时可以包含 <code>Hive</code> 支持，也可以不包含。</p>
<p>包含 <code>Hive</code> 支持的 <code>Spark SQL</code> 可以支持 <code>Hive</code> 表访问、 <code>UDF(用户自定义函数)</code> 以及 <code>Hive 查询语言(HiveQL/HQL)</code> 等。</p>
<p>需要强调的一点是，如果要在 <code>Spark SQL</code> 中包含 <code>Hive</code> 的库，并不需要事先安装 <code>Hive</code>。一般来说，最好还是在编译 <code>Spark SQL</code> 时引入 <code>Hive</code> 支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 <code>Spark</code>，它应该已经在编译时添加了 <code>Hive</code> 支持。</p>
<p>若要把 <code>Spark SQL</code> 连接到一个部署好的 <code>Hive</code> 上，你必须把 <code>hive-site.xml</code> 复制到 <code>Spark</code> 的配置文件目录中 <code>($SPARK_HOME/conf)</code>。即使没有部署好 <code>Hive</code>，<code>Spark SQL</code> 也可以运行。 </p>
<p>需要注意的是，如果你没有部署好 <code>Hive</code>，<code>Spark SQL</code> 会在当前的工作目录中创建出自己的 <code>Hive</code> 元数据仓库，叫作 <code>metastore_db</code>。此外，如果你尝试使用 <code>HiveQL</code> 中的 <code>CREATE TABLE</code>（并非 <code>CREATE EXTERNAL TABLE</code>）语句来创建表，这些表会被放在你默认的文件系统中的 <code>/user/hive/warehouse</code> 目录中（如果你的 <code>classpath</code> 中有配好的 <code>hdfs-site.xml</code>，默认的文件系统就是 <code>HDFS</code>，否则就是本地文件系统）。</p>
<hr>
<h3 id="3-5-1-使用内嵌的-Hive"><a href="#3-5-1-使用内嵌的-Hive" class="headerlink" title="3.5.1    使用内嵌的 Hive"></a>3.5.1    使用内嵌的 Hive</h3><p>如果使用 <code>Spark</code> 内嵌的 <code>Hive</code>，则什么都不用做，直接使用即可。</p>
<p><code>Hive</code> 的元数据存储在 <code>derby</code> 中，仓库地址：<code>$SPARK_HOME/spark-warehouse</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;create table aa(id int)&quot;</span>)</span><br><span class="line"><span class="number">19</span>/<span class="number">02</span>/<span class="number">09</span> <span class="number">18</span>:<span class="number">36</span>:<span class="number">10</span> <span class="type">WARN</span> <span class="type">HiveMetaStore</span>: <span class="type">Location</span>: file:/opt/module/spark-local/spark-warehouse/aa specified <span class="keyword">for</span> non-external table:aa</span><br><span class="line">res2: org.apache.spark.sql.<span class="type">DataFrame</span> = []</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| <span class="keyword">default</span>|       aa|      <span class="literal">false</span>|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line"><span class="comment">// 向表中加载本地数据数据</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;load data local inpath &#x27;./ids.txt&#x27; into table aa&quot;</span>)</span><br><span class="line">res8: org.apache.spark.sql.<span class="type">DataFrame</span> = []</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from aa&quot;</span>).show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|<span class="number">100</span>|</span><br><span class="line">|<span class="number">101</span>|</span><br><span class="line">|<span class="number">102</span>|</span><br><span class="line">|<span class="number">103</span>|</span><br><span class="line">|<span class="number">104</span>|</span><br><span class="line">|<span class="number">105</span>|</span><br><span class="line">|<span class="number">106</span>|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>

<p>然而在实际使用中，几乎没有任何人会使用内置的 <code>Hive</code>。</p>
<hr>
<h3 id="3-5-2-Spark-集成外置-Hive"><a href="#3-5-2-Spark-集成外置-Hive" class="headerlink" title="3.5.2    Spark 集成外置 Hive"></a>3.5.2    Spark 集成外置 Hive</h3><p><code>Spark</code> 和 <code>Hive</code> 集成的两种方式：</p>
<ol>
<li><code>Spark on Hive</code><ul>
<li>  只需要让 SparkSQL 找到 Hive 的元数据就 OK 了。</li>
</ul>
</li>
<li><code>Hive on Spark</code><ul>
<li>  了解</li>
</ul>
</li>
</ol>
<h4 id="3-5-2-1-集成步骤"><a href="#3-5-2-1-集成步骤" class="headerlink" title="3.5.2.1    集成步骤"></a>3.5.2.1    集成步骤</h4><ol>
<li>   <code>Spark</code> 要接管 <code>Hive</code> 需要把 <code>hive-site.xml</code> 拷贝到 Spark 的配置目录 <code>conf/</code> 下</li>
<li>   由于 <code>Hive</code> 的元数据保存在 <code>MySQL</code> 中，如果 <code>Spark</code> 想要访问元数据的话必须作为 <code>MySQL</code> 的客户端，所以还需要把 <code>Mysql</code> 的驱动拷贝到 Spark  的 <code>jars/</code> 目录下</li>
<li>   如果访问不到 <code>HDFS</code>，则还需要把 <code>core-site.xml</code> 和 <code>hdfs-site.xml</code> 拷贝到 <code>conf/</code> 目录下（可选）</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把 hive-site.xml 拷贝到 Spark 的配置目录 conf/ 下</span></span><br><span class="line">[lvnengdong@hadoop102 hive]$ <span class="built_in">cp</span> /opt/module/hive/conf/hive-site.xml /opt/module/spark-local/conf/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 Mysql 的驱动拷贝到 Spark  的 jars/ 目录下</span></span><br><span class="line">[lvnengdong@hadoop102 /]$ <span class="built_in">cp</span> /opt/software/mysql/mysql-connector-java-5.1.27/mysql-connector-java-5.1.27-bin.jar /opt/module/spark-local/jars</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/ 目录下（可选）</span></span><br></pre></td></tr></table></figure>



<h4 id="3-5-2-2-启动-spark-shell"><a href="#3-5-2-2-启动-spark-shell" class="headerlink" title="3.5.2.2    启动 spark-shell"></a>3.5.2.2    启动 spark-shell</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| <span class="keyword">default</span>|      emp|      <span class="literal">false</span>|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from emp&quot;</span>).show</span><br><span class="line"><span class="number">19</span>/<span class="number">02</span>/<span class="number">09</span> <span class="number">19</span>:<span class="number">40</span>:<span class="number">28</span> <span class="type">WARN</span> <span class="type">LazyStruct</span>: <span class="type">Extra</span> bytes detected at the end of the row! <span class="type">Ignoring</span> similar problems.</span><br><span class="line">+-----+-------+---------+----+----------+------+------+------+</span><br><span class="line">|empno|  ename|      job| mgr|  hiredate|   sal|  comm|deptno|</span><br><span class="line">+-----+-------+---------+----+----------+------+------+------+</span><br><span class="line">| <span class="number">7369</span>|  <span class="type">SMITH</span>|    <span class="type">CLERK</span>|<span class="number">7902</span>|<span class="number">1980</span><span class="number">-12</span><span class="number">-17</span>| <span class="number">800.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7499</span>|  <span class="type">ALLEN</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-2</span><span class="number">-20</span>|<span class="number">1600.0</span>| <span class="number">300.0</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7521</span>|   <span class="type">WARD</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-2</span><span class="number">-22</span>|<span class="number">1250.0</span>| <span class="number">500.0</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7566</span>|  <span class="type">JONES</span>|  <span class="type">MANAGER</span>|<span class="number">7839</span>|  <span class="number">1981</span><span class="number">-4</span><span class="number">-2</span>|<span class="number">2975.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7654</span>| <span class="type">MARTIN</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-9</span><span class="number">-28</span>|<span class="number">1250.0</span>|<span class="number">1400.0</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7698</span>|  <span class="type">BLAKE</span>|  <span class="type">MANAGER</span>|<span class="number">7839</span>|  <span class="number">1981</span><span class="number">-5</span><span class="number">-1</span>|<span class="number">2850.0</span>|  <span class="literal">null</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7782</span>|  <span class="type">CLARK</span>|  <span class="type">MANAGER</span>|<span class="number">7839</span>|  <span class="number">1981</span><span class="number">-6</span><span class="number">-9</span>|<span class="number">2450.0</span>|  <span class="literal">null</span>|    <span class="number">10</span>|</span><br><span class="line">| <span class="number">7788</span>|  <span class="type">SCOTT</span>|  <span class="type">ANALYST</span>|<span class="number">7566</span>| <span class="number">1987</span><span class="number">-4</span><span class="number">-19</span>|<span class="number">3000.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7839</span>|   <span class="type">KING</span>|<span class="type">PRESIDENT</span>|<span class="literal">null</span>|<span class="number">1981</span><span class="number">-11</span><span class="number">-17</span>|<span class="number">5000.0</span>|  <span class="literal">null</span>|    <span class="number">10</span>|</span><br><span class="line">| <span class="number">7844</span>| <span class="type">TURNER</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>|  <span class="number">1981</span><span class="number">-9</span><span class="number">-8</span>|<span class="number">1500.0</span>|   <span class="number">0.0</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7876</span>|  <span class="type">ADAMS</span>|    <span class="type">CLERK</span>|<span class="number">7788</span>| <span class="number">1987</span><span class="number">-5</span><span class="number">-23</span>|<span class="number">1100.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7900</span>|  <span class="type">JAMES</span>|    <span class="type">CLERK</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>| <span class="number">950.0</span>|  <span class="literal">null</span>|    <span class="number">30</span>|</span><br><span class="line">| <span class="number">7902</span>|   <span class="type">FORD</span>|  <span class="type">ANALYST</span>|<span class="number">7566</span>| <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>|<span class="number">3000.0</span>|  <span class="literal">null</span>|    <span class="number">20</span>|</span><br><span class="line">| <span class="number">7934</span>| <span class="type">MILLER</span>|    <span class="type">CLERK</span>|<span class="number">7782</span>| <span class="number">1982</span><span class="number">-1</span><span class="number">-23</span>|<span class="number">1300.0</span>|  <span class="literal">null</span>|    <span class="number">10</span>|</span><br><span class="line">| <span class="number">7944</span>|zhiling|    <span class="type">CLERK</span>|<span class="number">7782</span>| <span class="number">1982</span><span class="number">-1</span><span class="number">-23</span>|<span class="number">1300.0</span>|  <span class="literal">null</span>|    <span class="number">50</span>|</span><br><span class="line">+-----+-------+---------+----+----------+------+------+------+</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="3-5-2-3-使用-spark-sql-客户端"><a href="#3-5-2-3-使用-spark-sql-客户端" class="headerlink" title="3.5.2.3    使用 spark-sql 客户端"></a>3.5.2.3    使用 spark-sql 客户端</h4><p>在 <code>spark-shell</code> 中执行 <code>Hive</code> 方面的查询比较麻烦。格式为：<code>spark.sql(&quot;SQL语句&quot;).show</code></p>
<p><code>Spark</code> 专门给我们提供了书写 <code>HiveQL</code> 的工具： <code>spark-sql cli</code></p>
<ol>
<li><p>启动 <code>spark-sql</code> 客户端：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 spark-local]$ bin/spark-sql</span><br></pre></td></tr></table></figure></li>
<li><p>在 <code>spark-sql</code> 中执行 SQL 语句</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark<span class="operator">-</span><span class="keyword">sql</span> (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> movie_info;</span><br><span class="line">movie	category</span><br><span class="line">《疑犯追踪》	[&quot;悬疑&quot;,&quot;动作&quot;,&quot;科幻&quot;,&quot;剧情&quot;]</span><br><span class="line">《Lie <span class="keyword">to</span> me》	[&quot;悬疑&quot;,&quot;警匪&quot;,&quot;动作&quot;,&quot;心理&quot;,&quot;剧情&quot;]</span><br><span class="line">《战狼<span class="number">2</span>》	[&quot;战争&quot;,&quot;动作&quot;,&quot;灾难&quot;]</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.395</span> seconds, Fetched <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure></li>
<li><p>退出 <code>spark-sql</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-sql (default)&gt; quit;</span><br></pre></td></tr></table></figure>

</li>
<li><p> <code>spark-sql</code> 客户端一般用于测试，生产环境中不常用</p>
</li>
</ol>
<hr>
<h4 id="3-5-2-4-使用-hiveserver2-beeline"><a href="#3-5-2-4-使用-hiveserver2-beeline" class="headerlink" title="3.5.2.4    使用 hiveserver2 + beeline"></a>3.5.2.4    使用 <code>hiveserver2 + beeline</code></h4><p><code>spark-sql</code> 得到的结果不够友好，所以可以使用 <code>hiveserver2 + beeline</code></p>
<ol>
<li><p>启动 <code>thrift</code> 服务器</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以 local 模式启动 thriftserver 服务</span></span><br><span class="line">[lvnengdong@hadoop102 spark-local]$ sbin/start-thriftserver.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 yarn 模式启动 thriftserver 服务 </span></span><br><span class="line">sbin/start-thriftserver.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--hiveconf hive.server2.thrift.bind.host=hadoop102 \</span><br><span class="line">-–hiveconf hive.server2.thrift.port=10000 \</span><br></pre></td></tr></table></figure>

</li>
<li><p>启动 <code>beeline</code> 客户端</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">    [lvnengdong@hadoop102 spark-local]$ bin/beeline</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 然后输入</span></span><br><span class="line">    !connect jdbc:hive2://hadoop102:10000</span><br><span class="line">    <span class="comment"># 然后按照提示输入用户名和密码</span></span><br><span class="line">    admin</span><br><span class="line">123456</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看</p>
 <img src="/2021/12/11/Spark-SQL/image-20211225162805804.png" alt="image-20211225162805804" style="zoom: 67%;"></li>
</ol>
<hr>
<h3 id="3-5-3-在代码中访问-Hive"><a href="#3-5-3-在代码中访问-Hive" class="headerlink" title="3.5.3    在代码中访问 Hive"></a>3.5.3    在代码中访问 Hive</h3><h4 id="步骤1-拷贝-hive-site-xml-到-resources-目录下"><a href="#步骤1-拷贝-hive-site-xml-到-resources-目录下" class="headerlink" title="步骤1: 拷贝 hive-site.xml 到 resources 目录下"></a>步骤1: 拷贝 <code>hive-site.xml</code> 到 resources 目录下</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Hive连接哪个库来查找元数据--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 default 数据库的默认位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="步骤2-添加依赖"><a href="#步骤2-添加依赖" class="headerlink" title="步骤2: 添加依赖"></a>步骤2: 添加依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 添加 Spark 支持 Hive 的 jar 包--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="步骤3-代码"><a href="#步骤3-代码" class="headerlink" title="步骤3: 代码"></a>步骤3: 代码</h4><h5 id="从-Hive-中读数据"><a href="#从-Hive-中读数据" class="headerlink" title="从 Hive 中读数据"></a>从 Hive 中读数据</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 17:00</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HiveRead</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 如果 HDFS 提示权限不足，则可以修改文件属主</span></span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;lvnengdong&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .enableHiveSupport()  <span class="comment">// 添加支持外置 Hive，如果不设置默认使用 Spark 内置的 Hive</span></span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">import</span> spark.sql</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行 SQL</span></span><br><span class="line">    sql(<span class="string">&quot;show databases&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h5 id="向-Hive-中写数据"><a href="#向-Hive-中写数据" class="headerlink" title="向 Hive 中写数据"></a>向 Hive 中写数据</h5><ol>
<li> 使用 Hive 的 <code>insert</code> 语句</li>
<li> <code>df.saveAsTable(&quot;表名&quot;)</code>：将 DF 中的数据直接写出到 HIve 表中。使用列名分配 value</li>
<li> <code>df.write.insertInto(&quot;表名&quot;)</code>：基本等价于方式二的 <code>append</code> 模式，但是插入数据时要求对应的 Hive 表必须提前存在。使用位置分配 value</li>
</ol>
<h6 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h6><p><strong>注意：</strong></p>
<p>在 Spark 中创建数据库，默认情况下，元数据信息保存在 MySQL 中，而数据则会保存在本地。</p>
<p>如果想要创建的数据库数据保存在 HDFS 上，主要有两种解决方案：</p>
<ol>
<li><p>在创建数据库时，通过参数修改数据库仓库的地址</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://hadoop201:9000/user/hive/warehouse&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p> 不要使用 Spark 创建数据库，而是使用 Hive 去创建数据库</p>
</li>
</ol>
<p><strong>Demo</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 19:10</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HiveWrite</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      <span class="comment">// 显式配置数据仓库地址</span></span><br><span class="line">      .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://hadoop102:9000/spark_hive/warehouse&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换.</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、创建数据库</span></span><br><span class="line">    spark.sql(<span class="string">&quot;create database spark_hive01&quot;</span>).show()</span><br><span class="line">    <span class="comment">// 2、创建表</span></span><br><span class="line">    spark.sql(<span class="string">&quot;use spark_hive01&quot;</span>).show()</span><br><span class="line">    spark.sql(<span class="string">&quot;create table user_info(id int, name string)&quot;</span>).show()</span><br><span class="line">    <span class="comment">// 3、插入数据</span></span><br><span class="line">    spark.sql(<span class="string">&quot;insert into user_info values(10, &#x27;zs&#x27;)&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>  插入的数据保存在 HDFS 上</li>
</ul>
<p><img src="/2021/12/11/Spark-SQL/image-20211225195825371.png" alt="image-20211225195825371"></p>
<p><strong>Demo2</strong></p>
<p>把读取到的数据写入到 Hive 中，表可以存在也可以不存在</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/25 19:10</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HiveWrite2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 先创建一个 SparkSession 对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      <span class="comment">// 显式配置数据仓库地址，默认保存在本地文件系统上</span></span><br><span class="line">      .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://hadoop102:9000/spark_hive/warehouse&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 导入用到的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、读取数据</span></span><br><span class="line">    <span class="keyword">val</span> source: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;E:\\workspace_bigData\\spark\\datas\\user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、写出数据到Hive表中（若表不存在则自动创建）</span></span><br><span class="line">    spark.sql(<span class="string">&quot;use spark_hive01&quot;</span>)</span><br><span class="line">    source.write.mode(<span class="string">&quot;append&quot;</span>).saveAsTable(<span class="string">&quot;user_info2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 方式二：将 df 数据写出到 Hive 表中（若表不存在则抛出异常）</span></span><br><span class="line"><span class="comment">//    source.write.insertInto(&quot;user_info2&quot;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<ul>
<li><p>聚合后，默认分区数为 200。如果想要调整分区数，可以使用 <code>coalesce</code> 函数</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 调整分区数量</span></span><br><span class="line">df.coalesce(<span class="number">1</span>).write.saveAsTable(<span class="string">&quot;表名&quot;</span>)	</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/12/11/Spark/" rel="prev" title="Spark 基础">
                  <i class="fa fa-chevron-left"></i> Spark 基础
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/12/11/Spark-Streaming/" rel="next" title="Spark_Streaming">
                  Spark_Streaming <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  





</body>
</html>
