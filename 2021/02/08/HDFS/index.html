<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="HDFS 概述分布式文件管理系统随着数据量越来越大，在一个机器上存不下所有的数据，就需要把数据分配到更多的机器中，但是这样又不方便管理和维护，所以我们迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS 是分布式文件管理系统中的一种落地实现。  HDFS 定义HDFS（Hadoop Distribute FileSystem）    Hadoop 分布式文件系统，本质上还是一">
<meta property="og:type" content="article">
<meta property="og:title" content="HDFS">
<meta property="og:url" content="http://example.com/2021/02/08/HDFS/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="HDFS 概述分布式文件管理系统随着数据量越来越大，在一个机器上存不下所有的数据，就需要把数据分配到更多的机器中，但是这样又不方便管理和维护，所以我们迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS 是分布式文件管理系统中的一种落地实现。  HDFS 定义HDFS（Hadoop Distribute FileSystem）    Hadoop 分布式文件系统，本质上还是一">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20210219125213030.png">
<meta property="og:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20210219125238273.png">
<meta property="og:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20210225211305080.png">
<meta property="og:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20210219131335414.png?lastModify=1637670391">
<meta property="og:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20210219130216472.png">
<meta property="og:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20211124095328168.png">
<meta property="og:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20211124095406232.png">
<meta property="og:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20211124173109470.png">
<meta property="og:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20211124184155174.png">
<meta property="og:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20211124210327466.png">
<meta property="og:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20211125112723407.png">
<meta property="og:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20211125115010253.png">
<meta property="og:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20211126180438535.png">
<meta property="article:published_time" content="2021-02-08T11:48:38.000Z">
<meta property="article:modified_time" content="2021-12-14T09:17:41.660Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/02/08/HDFS/HDFS/image-20210219125213030.png">


<link rel="canonical" href="http://example.com/2021/02/08/HDFS/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2021/02/08/HDFS/","path":"2021/02/08/HDFS/","title":"HDFS"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>HDFS | Hexo</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hexo</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#HDFS-%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">HDFS 概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F"><span class="nav-number">1.1.</span> <span class="nav-text">分布式文件管理系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS-%E5%AE%9A%E4%B9%89"><span class="nav-number">1.2.</span> <span class="nav-text">HDFS 定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS-%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">1.3.</span> <span class="nav-text">HDFS 的使用场景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS-%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="nav-number">1.4.</span> <span class="nav-text">HDFS 的优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E7%82%B9"><span class="nav-number">1.4.1.</span> <span class="nav-text">优点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9"><span class="nav-number">1.4.2.</span> <span class="nav-text">缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%E3%80%81HDFS-%E4%B8%8D%E6%94%AF%E6%8C%81%E5%AF%B9%E6%96%87%E4%BB%B6%E7%9A%84%E9%9A%8F%E6%9C%BA%E5%86%99"><span class="nav-number">1.4.2.1.</span> <span class="nav-text">1、HDFS 不支持对文件的随机写</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2%E3%80%81%E4%B8%8D%E9%80%82%E5%90%88%E4%BD%8E%E5%BB%B6%E6%97%B6%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AE%BF%E9%97%AE"><span class="nav-number">1.4.2.2.</span> <span class="nav-text">2、不适合低延时数据的访问</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3%E3%80%81%E4%B8%8D%E9%80%82%E5%90%88%E5%AD%98%E5%82%A8%E5%A4%A7%E9%87%8F%E7%9A%84%E5%B0%8F%E6%96%87%E4%BB%B6%E3%80%82"><span class="nav-number">1.4.2.3.</span> <span class="nav-text">3、不适合存储大量的小文件。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4%E3%80%81%E4%B8%8D%E6%94%AF%E6%8C%81%E5%B9%B6%E5%8F%91%E5%86%99%E5%85%A5%E3%80%81%E6%96%87%E4%BB%B6%E9%9A%8F%E6%9C%BA%E4%BF%AE%E6%94%B9"><span class="nav-number">1.4.2.4.</span> <span class="nav-text">4、不支持并发写入、文件随机修改</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS-%E5%9D%97%E5%A4%A7%E5%B0%8F"><span class="nav-number">1.5.</span> <span class="nav-text">HDFS 块大小</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E5%AE%83"><span class="nav-number">1.6.</span> <span class="nav-text">其它</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#HDFS-%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84"><span class="nav-number">2.</span> <span class="nav-text">HDFS 体系结构</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#HDFS-%E7%9A%84-Shell-%E6%93%8D%E4%BD%9C"><span class="nav-number">3.</span> <span class="nav-text">HDFS 的 Shell 操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#hadoop-%E5%91%BD%E4%BB%A4"><span class="nav-number">3.1.</span> <span class="nav-text">hadoop 命令</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hadoop-fs-%E5%91%BD%E4%BB%A4"><span class="nav-number">3.2.</span> <span class="nav-text">hadoop fs 命令</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hadoop-fs-%E5%92%8C-hdfs-dfs"><span class="nav-number">3.3.</span> <span class="nav-text">hadoop fs 和 hdfs dfs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="nav-number">3.4.</span> <span class="nav-text">常用命令</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%91%BD%E4%BB%A4%E5%88%86%E7%B1%BB"><span class="nav-number">3.4.1.</span> <span class="nav-text">命令分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AE%9E%E6%93%8D"><span class="nav-number">3.4.2.</span> <span class="nav-text">常用命令实操</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%E3%80%81%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="nav-number">3.4.2.1.</span> <span class="nav-text">1、准备工作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2%E3%80%81%E4%B8%8A%E4%BC%A0"><span class="nav-number">3.4.2.2.</span> <span class="nav-text">2、上传</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-3-%E4%B8%8B%E8%BD%BD"><span class="nav-number">3.4.3.</span> <span class="nav-text">2.3.3 下载</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#HDFS-%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C"><span class="nav-number">4.</span> <span class="nav-text">HDFS 客户端操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-number">4.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Java-%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="nav-number">4.2.</span> <span class="nav-text">Java 客户端环境准备</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS-%E7%9A%84-API-%E6%93%8D%E4%BD%9C"><span class="nav-number">4.3.</span> <span class="nav-text">HDFS 的 API 操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FileSystem"><span class="nav-number">4.3.1.</span> <span class="nav-text">FileSystem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Configuration"><span class="nav-number">4.3.2.</span> <span class="nav-text">Configuration</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95"><span class="nav-number">4.3.2.1.</span> <span class="nav-text">测试</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#URI"><span class="nav-number">4.3.2.2.</span> <span class="nav-text">URI</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">4.3.2.3.</span> <span class="nav-text">配置文件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5"><span class="nav-number">4.3.3.</span> <span class="nav-text">增删改查</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%E3%80%81%E5%88%9B%E5%BB%BA%E7%9B%AE%E5%BD%95"><span class="nav-number">4.3.3.1.</span> <span class="nav-text">1、创建目录</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2%E3%80%81%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6"><span class="nav-number">4.3.3.2.</span> <span class="nav-text">2、上传文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3%E3%80%81%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6"><span class="nav-number">4.3.3.3.</span> <span class="nav-text">3、下载文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4%E3%80%81%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6"><span class="nav-number">4.3.3.4.</span> <span class="nav-text">4、删除文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5%E3%80%81%E9%87%8D%E5%91%BD%E5%90%8D"><span class="nav-number">4.3.3.5.</span> <span class="nav-text">5、重命名</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6%E3%80%81%E5%88%A4%E6%96%AD%E5%BD%93%E5%89%8D%E8%B7%AF%E5%BE%84%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8"><span class="nav-number">4.3.3.6.</span> <span class="nav-text">6、判断当前路径是否存在</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7%E3%80%81%E5%88%A4%E6%96%AD%E5%BD%93%E5%89%8D%E8%B7%AF%E5%BE%84%E6%98%AF%E7%9B%AE%E5%BD%95%E8%BF%98%E6%98%AF%E6%96%87%E4%BB%B6"><span class="nav-number">4.3.3.7.</span> <span class="nav-text">7、判断当前路径是目录还是文件</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%87%8D%E8%A6%81%EF%BC%9AFileStatus"><span class="nav-number">4.3.3.7.1.</span> <span class="nav-text">重要：FileStatus</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%87%8D%E8%A6%81%EF%BC%9AListStatus"><span class="nav-number">4.3.3.7.2.</span> <span class="nav-text">重要：ListStatus</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8%E3%80%81%E8%8E%B7%E5%8F%96%E6%96%87%E4%BB%B6%E7%9A%84%E5%9D%97%E4%BF%A1%E6%81%AF"><span class="nav-number">4.3.3.8.</span> <span class="nav-text">8、获取文件的块信息</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E4%B8%8A%E4%BC%A0%E5%92%8C%E4%B8%8B%E8%BD%BD"><span class="nav-number">4.4.</span> <span class="nav-text">自定义上传和下载</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#HDFS%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81"><span class="nav-number">5.</span> <span class="nav-text">HDFS的数据流</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS-%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="nav-number">5.1.</span> <span class="nav-text">HDFS 写数据流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8%E5%86%99%E6%B5%81%E7%A8%8B"><span class="nav-number">5.1.1.</span> <span class="nav-text">异常写流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS-%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="nav-number">5.2.</span> <span class="nav-text">HDFS 读数据流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="nav-number">5.3.</span> <span class="nav-text">网络拓扑-节点距离计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5"><span class="nav-number">5.4.</span> <span class="nav-text">机架感知</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NameNode-%E5%92%8C-SecondaryNameNode"><span class="nav-number">6.</span> <span class="nav-text">NameNode 和 SecondaryNameNode</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#NameNode-%E4%B8%AD%E5%85%83%E6%95%B0%E6%8D%AE%E7%9A%84%E7%BB%84%E6%88%90"><span class="nav-number">6.1.</span> <span class="nav-text">NameNode 中元数据的组成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NN-%E5%92%8C-2NN-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="nav-number">6.2.</span> <span class="nav-text">NN 和 2NN 工作机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3"><span class="nav-number">6.3.</span> <span class="nav-text">NN和2NN工作机制详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fsimage-%E5%92%8C-edits-%E8%A7%A3%E6%9E%90"><span class="nav-number">6.4.</span> <span class="nav-text">fsimage 和 edits 解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E7%82%B9%EF%BC%9A"><span class="nav-number">6.4.1.</span> <span class="nav-text">注意点：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CheckPoint-%E6%97%B6%E9%97%B4%E8%AE%BE%E7%BD%AE"><span class="nav-number">6.5.</span> <span class="nav-text">CheckPoint 时间设置</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DataNode"><span class="nav-number">7.</span> <span class="nav-text">DataNode</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#DataNode-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="nav-number">7.1.</span> <span class="nav-text">DataNode 工作机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7"><span class="nav-number">7.2.</span> <span class="nav-text">数据完整性</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">224</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/02/08/HDFS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="HDFS | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          HDFS
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-02-08 19:48:38" itemprop="dateCreated datePublished" datetime="2021-02-08T19:48:38+08:00">2021-02-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-12-14 17:17:41" itemprop="dateModified" datetime="2021-12-14T17:17:41+08:00">2021-12-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="HDFS-概述"><a href="#HDFS-概述" class="headerlink" title="HDFS 概述"></a>HDFS 概述</h1><h2 id="分布式文件管理系统"><a href="#分布式文件管理系统" class="headerlink" title="分布式文件管理系统"></a>分布式文件管理系统</h2><p>随着数据量越来越大，在一个机器上存不下所有的数据，就需要把数据分配到更多的机器中，但是这样又不方便管理和维护，所以我们迫切需要<strong>一种系统来管理多台机器上的文件</strong>，这就是分布式文件管理系统。HDFS 是分布式文件管理系统中的一种落地实现。</p>
<hr>
<h2 id="HDFS-定义"><a href="#HDFS-定义" class="headerlink" title="HDFS 定义"></a>HDFS 定义</h2><p><strong>HDFS（Hadoop Distribute FileSystem）</strong></p>
<ul>
<li>  Hadoop 分布式文件系统，本质上还是一个<strong>文件系统</strong>，即用来存储文件的系统。</li>
<li>  其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的每个服务器有各自的角色。</li>
<li>  HDFS 通过目录树来定位文件；</li>
</ul>
<hr>
<h2 id="HDFS-的使用场景"><a href="#HDFS-的使用场景" class="headerlink" title="HDFS 的使用场景"></a>HDFS 的使用场景</h2><p><strong>适合一次写入，多次读出的场景，且不支持文件的修改</strong>。适合用来做数据分析。</p>
<ul>
<li>  不支持对文件的随机写，即可以追加写，但是不能修改已写入的数据。</li>
</ul>
<hr>
<h2 id="HDFS-的优缺点"><a href="#HDFS-的优缺点" class="headerlink" title="HDFS 的优缺点"></a>HDFS 的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol>
<li><p><strong>高容错性</strong></p>
<ul>
<li>  （1）数据自动创建多个副本中。它通过增加副本的形式，提高容错性。</li>
</ul>
<p><img src="HDFS/image-20210219125213030.png" alt="image-20210219125213030"></p>
<ul>
<li>  （2）某一个副本丢失以后 ，它可以自动恢复，使副本数量维持预设置的数量。</li>
</ul>
<p><img src="HDFS/image-20210219125238273.png" alt="image-20210219125238273"></p>
<blockquote>
<p>  HDFS 的副本自动恢复机制：</p>
<ul>
<li>  如果 HDFS 集群中某个 DataNode 节点宕机，导致副本数据丢失，HDFS 会在集群中剩余的未保存该副本的机器中随机挑选一台机器保存该副本，维护副本数量与用户设定的副本数量一致或与节点数量一致。</li>
<li>  每个副本在每个节点上只能保存一份。如果用户预设置了保存10个副本，但是集群中只有 3 个 DataNode，那么当前集群中就只会有 3 个副本，但是当集群扩展到10台时，NameNode 就会自动在新的 DataNode 节点上创建新的副本，维护副本数量到10个。</li>
</ul>
</blockquote>
</li>
<li><p><strong>适合处理大数据</strong></p>
<p>（1）数据规模：能够处理的数据规模达到 GB、TB、甚至 PB 级别；</p>
<p>（2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。</p>
</li>
<li><p>可构建在廉价机器上，通过多副本机制，提高可靠性。</p>
</li>
</ol>
<hr>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><h4 id="1、HDFS-不支持对文件的随机写"><a href="#1、HDFS-不支持对文件的随机写" class="headerlink" title="1、HDFS 不支持对文件的随机写"></a>1、HDFS 不支持对文件的随机写</h4><p>HDFS 仅支持对数据的追加写（append），不支持文件的随机修改。</p>
<p><strong>原因</strong>：</p>
<ol>
<li><p> HDFS 没有提供对文件的在线寻址（在线打开）功能，无法在线直接修改。如果想要修改就必须将文件下载到本地修改，修改完毕之后再上传到 HDFS 上，IO 代价很大。</p>
</li>
<li><p>文件在 HDFS 上存储时，以 block 为基本单位存储，如果修改了一个块中的内容，就会影响当前 block 之后的所有 block，效率很低。</p>
 <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">假设有一文件 A.txt ，共 100W 行，200M。</span><br><span class="line">文件A在存储到 HDFS 上时按 block 分开存储，每块 128M 。</span><br><span class="line">200M 可分为2块存储，block1 存储0~128M，block2 存储128~200M。</span><br><span class="line">假设 block1 在 DataNode1 来存储，block2 在 DataNode2 存储，即这两块数据位于不同的磁盘上。</span><br><span class="line"></span><br><span class="line">现在有一需求需要修改第5W行的内容：</span><br><span class="line"><span class="code">	1、首先我们要找到第5W行在哪个位置，但是 HDFS 没有寻址的功能，HDFS 不支持在线打开，只支持</span></span><br><span class="line"><span class="code">	下载到本地之后再打开。所以压根就找不到第5W行在哪个位置。</span></span><br><span class="line"><span class="code">	2、即使 HDFS 能找到第5W行在哪个位置。 假设第5W行在第60M的位置，我们据此找到了 block1，现</span></span><br><span class="line"><span class="code">	在需求是删除4W~5W行内容，并在后面追加20W行数据。这时我们就会发现 block1 中的数据已经超过了</span></span><br><span class="line"><span class="code">	128M，就需要将block1中超过128M的内容顺延存储到 block2 中，那么 block2 中数据的存储位置需要</span></span><br><span class="line"><span class="code">	全部重写一遍。而且本例中只使用了2个block，如果涉及到了更多的block，那么当前block之后的所有block</span></span><br><span class="line"><span class="code">	都有可能需要重写，这一代价是非常高的。</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="quote">&gt; 所以 HDFS 在设计之初就不支持对文件的随机写。</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="2、不适合低延时数据的访问"><a href="#2、不适合低延时数据的访问" class="headerlink" title="2、不适合低延时数据的访问"></a>2、不适合低延时数据的访问</h4><p>意思就是：如果我们想要能尽可能快的读写数据（即数据访问时延较低），使用 HDFS 是不合适的。比如毫秒级的读写数据，HDFS 是做不到的。一般处理少量的数据时，要求时延要尽可能地小，那么使用 HDFS 就是不适合的。</p>
<ul>
<li><p>比如你在本机打开一个文件，打开的速度是很快的；（不涉及到网络IO，只涉及到磁盘IO）</p>
</li>
<li><p>但是如果想要访问 HDFS 上存储的文件，它的速度是慢于本地打开的。（不仅涉及到网络IO，还涉及到磁盘IO）</p>
<blockquote>
<ul>
<li><p>  因为 HDFS 是分布式存储的，假设现在有一个 1G 的文件分为了8个block存储在 HDFS 上。</p>
</li>
<li><p>因为这 8 个 block 可能存储在集群的多个节点上（假设为[节点1：2个块]，[节点2：3个块]，[节点3：3个块]）：</p>
<p>  假设我们要下载这个文件，就需要请求这 3 台服务器将这 8 个 block 都下载下来。如果这三台机器所在的三个机房分为位于美国、上海、南极，那么客户端在请求这三台机器时首先就会涉及到网络 IO，客户端在打开时又要涉及到磁盘 IO。时效性较差，不适合低延时数据的访问。</p>
</li>
</ul>
</blockquote>
</li>
</ul>
<h4 id="3、不适合存储大量的小文件。"><a href="#3、不适合存储大量的小文件。" class="headerlink" title="3、不适合存储大量的小文件。"></a>3、不适合存储大量的小文件。</h4><p><strong>原因</strong>：</p>
<ol>
<li> 如果 HDFS 存储了大量的小文件，就会降低 NameNode 的服务能力！</li>
<li> 小文件存储的寻址时间会超过读取时间，它违反了 HDFS 的设计目标。</li>
</ol>
<p><strong>.eg</strong></p>
<p>NameNode 负责管理文件元数据信息，NameNode 在运行时，当有请求想要访问某个文件时，NameNode 必须将当前集群中存储的所有文件的元数据加载到内存进行检索，如果 NameNode 中存储了大量小文件的话，在加载时它会占用 NameNode 中部分内存来存储小文件的元数据信息。</p>
<p>例如：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">NameNode 的内存是有限的，假设 NameNode 内存中最大可以存储 1亿条元数据信息，</span><br><span class="line"><span class="code">	如果集群中全部存储 1k 的小文件的话，那么这个集群最大的存储能力是 1亿 * 1k ；</span></span><br><span class="line"><span class="code">	如果集群中全部存储 1G 的大文件的话，那么这个集群最大的存储能力是 1亿 * 1G ；</span></span><br><span class="line"><span class="code">	显然，存大文件时集群的存储能力呈指数级上升！</span></span><br></pre></td></tr></table></figure>





<h4 id="4、不支持并发写入、文件随机修改"><a href="#4、不支持并发写入、文件随机修改" class="headerlink" title="4、不支持并发写入、文件随机修改"></a>4、不支持并发写入、文件随机修改</h4><p>同一文件在同一时刻只能由一个线程写入，不允许多个线程同时写。假设线程 A 正在上传<code>ss.txt</code>文件，那么在线程 A 开始上传到结束之间，其它线程不能再上传<code>ss.txt</code>文件</p>
<p><img src="HDFS/image-20210225211305080.png" alt="image-20210225211305080"></p>
<hr>
<h2 id="HDFS-块大小"><a href="#HDFS-块大小" class="headerlink" title="HDFS 块大小"></a>HDFS 块大小</h2><p>HDFS 中的文件在物理上是分块存储（Block），块的大小由 <code>hdfs-default.xml </code>配置文件中的参数 <code>dfs.blocksize</code> 来决定。</p>
<ul>
<li>  在 Hadoop 2.x 中，默认值是<code>134217728</code>（128M）；</li>
<li>  在 Hadoop 1.x 中，默认值是 64M 。</li>
</ul>
<blockquote>
<ol>
<li> <strong>Hadoop 2.x 块大小默认为 128M 的原因</strong>：基于最佳传输损耗理论！</li>
</ol>
</blockquote>
<p><strong>最佳传输损耗理论</strong>：在一次传输中，寻址时间占总传输时间1%时，本次传输的损耗最小，为最佳性价比传输！</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> 不论对磁盘中的文件进行读还是写，首先要进行寻址！（寻找数据在磁盘中的哪个位置）</span><br><span class="line"><span class="bullet">-</span> 目前硬件的发展条件，普通磁盘写的速率大约为 100M/s，寻址时间大约为 10ms/次。</span><br><span class="line"><span class="code">    </span></span><br><span class="line"><span class="code">    根据 最佳传输损耗理论 ，总传输时间约为： 10ms / 1% = 1s</span></span><br><span class="line"><span class="code">    普通磁盘的传输容量大小约为： 1 * 100M = 100M</span></span><br><span class="line"><span class="code">    又因为</span></span><br><span class="line"><span class="code">        - 块的大小必须是 2^N （文件在传输过程中，每64K就需要校验一次，因此块的大小必须为 2^N）</span></span><br><span class="line"><span class="code">        - 最接近 100M 的就是 128M</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="bullet">-</span> 所以 Hadoop 2.x 规定默认块的大小为 128M 。</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> 如果公司使用的固态硬盘写的速度是300M/S，那么就可以将块大小调整到 256M</span><br><span class="line"><span class="bullet">-</span> 如果公司使用的固态硬盘写的速度是500M/S，那么就可以将块大小调整到 512M</span><br></pre></td></tr></table></figure>

<blockquote>
<ol start="2">
<li> <strong>为什么块的大小不能设置太小，也不能设置太大？</strong></li>
</ol>
</blockquote>
<p>块大小需要适当的调节，既不能太大，也不能太小！</p>
<ul>
<li><p>不能太大：</p>
  <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1.</span> 在一些分块读取的场景，不够灵活，会带来额外的网络消耗。</span><br><span class="line"><span class="code">    假设当前有一文件A，大小为1G。</span></span><br><span class="line"><span class="code">        如果块大小为128M，可分8块。    [取第一块]</span></span><br><span class="line"><span class="code">        如果块大小为1G，可分1块。      [取第一块]</span></span><br><span class="line"><span class="code">    需求：需要读取A文件0~128M部分的内容。</span></span><br><span class="line"><span class="code">    完成：两种分块方式在完成需求是同样只需要取第一块，将第一块下载到本地。但块太大的时候会带来额外的网络传输消耗和本地磁盘资源的消耗。</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="bullet">2.</span> 在上传文件时，一旦发生故障，会造成资源的浪费。</span><br><span class="line"><span class="code">    假设上文A文件，</span></span><br><span class="line"><span class="code">        按128M分块，在上传第5块时因网络原因传输中断了，在网络重新连接后，只需重新上传第5块即可。</span></span><br><span class="line"><span class="code">        但是如果按1G分块，在上传到999M时因网络原因传输中断了，在重新连接网络后还需要从头重写上传整个文件</span></span><br><span class="line"><span class="code">    </span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>​           </p>
<ul>
<li><p>不能太小：</p>
  <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1.</span> 块太小，同样大小的文件，会占用 NameNode 过多的的元数据空间。</span><br><span class="line"><span class="code">    假设 有一文件B，大小为128M。</span></span><br><span class="line"><span class="code">        若 块大小为1M ，则保存文件需128块，需要生成128个块的映射信息。这些信息都属于元数据，都要存到NameNode中。</span></span><br><span class="line"><span class="code">        若 块大小为128M ，则保存文件只需1块， 只需要存储一个块的映射信息。</span></span><br><span class="line"><span class="code">2. 块太小，在进行读写操作时，会增加寻址时间，程序一直在找块的开始位置；</span></span><br><span class="line"><span class="code">    下载B文件，</span></span><br><span class="line"><span class="code">        若 块大小为1M，共128块，需要下载全部的128个块，寻址128次，每次寻址耗时 10ms；</span></span><br><span class="line"><span class="code">        若 块大小为128M ，共1块，只需下载1块，寻址1次</span></span><br></pre></td></tr></table></figure></li>
</ul>
<blockquote>
<ol start="3">
<li> 图示</li>
</ol>
</blockquote>
<p><img src="HDFS/image-20210219131335414.png?lastModify=1637670391" alt="image-20210219131335414"></p>
<p><strong>总结：HDFS块的大小设置主要取决于磁盘传输速率。</strong></p>
<h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><blockquote>
<ol>
<li> 副本数量</li>
</ol>
</blockquote>
<p>副本数的概念指的是最大副本数！</p>
<p>具体存放几个副本需要参考 DataNode 节点的数量，每个 DataNode 节点最多只能存储一个副本！</p>
<blockquote>
<ol>
<li> 默认块大小</li>
</ol>
</blockquote>
<p>默认 block 大小为 128M ，128M 指的是单个 block 的最大存储空间，如果当前块存储的数据不满 128M ，存了多少数据就占用多少磁盘空间。它也是一个独立的块！</p>
<blockquote>
<ol start="3">
<li> <strong>一个 block 只属于一个文件</strong></li>
</ol>
</blockquote>
<p> 意思是即使当前 block 只占用了几 kb，还有很大的剩余内存，但是也不能存储其它文件的内容。</p>
<hr>
<h1 id="HDFS-体系结构"><a href="#HDFS-体系结构" class="headerlink" title="HDFS 体系结构"></a>HDFS 体系结构</h1><p><img src="HDFS/image-20210219130216472.png" alt="image-20210219130216472"></p>
<p><strong>一、NameNode</strong>：就是Master，它是一个主管、管理者。</p>
<ol>
<li> 管理HDFS的名称空间；</li>
<li> 配置副本策略；</li>
<li> 管理数据块（Block）映射信息；</li>
<li> 处理客户端读写请求</li>
</ol>
<p><strong>二、DataNode</strong>：就是Slave。NameNode下达命令，DataNode执行实际的操作</p>
<ol>
<li> 存储文件的真实数据，基本存储单位是 Block。</li>
<li> 执行数据块的读/写操作。</li>
</ol>
<p><strong>三、Client</strong>：就是客户端。</p>
<ol>
<li> 文件切分。文件上传到 HDFS 前，Client 会按计划将文件切分成一个一个的 Block，然后再进行上传；</li>
<li> 与NameNode交互，获取文件的位置信息；</li>
<li> 与DataNode交互，读取或者写入数据；</li>
<li> Client提供一些命令来管理HDFS，比如NameNode格式化；</li>
<li> Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作；</li>
</ol>
<p><strong>四、Secondary NameNode（2nn）</strong>：并非 NameNode 的热备。当 NameNode 挂掉的时候，它并不能马上替换NameNode并提供服务。</p>
<ol>
<li> 辅助 NameNode，分担其工作量，比如定期合并 Fsimage 和 Edits ，并推送给 NameNode；</li>
<li> 在紧急情况下，可辅助恢复 NameNode。</li>
</ol>
<hr>
<h1 id="HDFS-的-Shell-操作"><a href="#HDFS-的-Shell-操作" class="headerlink" title="HDFS 的 Shell 操作"></a>HDFS 的 Shell 操作</h1><h2 id="hadoop-命令"><a href="#hadoop-命令" class="headerlink" title="hadoop 命令"></a>hadoop 命令</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop</span><br><span class="line">Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]</span><br><span class="line">  CLASSNAME            run the class named CLASSNAME</span><br><span class="line"> or</span><br><span class="line">  <span class="built_in">where</span> COMMAND is one of:</span><br><span class="line">  fs                   run a generic filesystem user client	<span class="comment"># 运行一个通用的文件系统客户端</span></span><br><span class="line">  version              <span class="built_in">print</span> the version</span><br><span class="line">  jar &lt;jar&gt;            run a jar file	<span class="comment"># 运行一个jar包</span></span><br><span class="line">                       note: please use <span class="string">&quot;yarn jar&quot;</span> to launch</span><br><span class="line">                             YARN applications, not this <span class="built_in">command</span>.</span><br><span class="line">  checknative [-a|-h]  check native hadoop and compression libraries availability</span><br><span class="line">  distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively</span><br><span class="line">  archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive</span><br><span class="line">  classpath            prints the class path needed to get the</span><br><span class="line">  credential           interact with credential providers</span><br><span class="line">                       Hadoop jar and the required libraries</span><br><span class="line">  daemonlog            get/set the <span class="built_in">log</span> level <span class="keyword">for</span> each daemon</span><br><span class="line">  trace                view and modify Hadoop tracing settings</span><br><span class="line"></span><br><span class="line">Most commands <span class="built_in">print</span> <span class="built_in">help</span> when invoked w/o parameters.</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="hadoop-fs-命令"><a href="#hadoop-fs-命令" class="headerlink" title="hadoop fs 命令"></a>hadoop fs 命令</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop fs</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]	<span class="comment"># 追加</span></span><br><span class="line">	[-<span class="built_in">cat</span> [-ignoreCrc] &lt;src&gt; ...]	<span class="comment"># 查看文件内容</span></span><br><span class="line">	[-checksum &lt;src&gt; ...]	<span class="comment"># 检查校验和</span></span><br><span class="line">	[-<span class="built_in">chgrp</span> [-R] GROUP PATH...]	<span class="comment"># 改变所属组</span></span><br><span class="line">	[-<span class="built_in">chmod</span> [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]	<span class="comment"># 改变权限</span></span><br><span class="line">	[-<span class="built_in">chown</span> [-R] [OWNER][:[GROUP]] PATH...]	<span class="comment"># 改变所属主</span></span><br><span class="line">	[-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]	<span class="comment"># 上传文件（到HDFS）</span></span><br><span class="line">	[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]	<span class="comment"># （从HDFS）下载文件</span></span><br><span class="line">	[-count [-q] [-h] &lt;path&gt; ...]	</span><br><span class="line">	[-<span class="built_in">cp</span> [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]	<span class="comment"># 复制</span></span><br><span class="line">	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]	<span class="comment"># 创建快照</span></span><br><span class="line">	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]	<span class="comment"># 删除快照</span></span><br><span class="line">	[-<span class="built_in">df</span> [-h] [&lt;path&gt; ...]]	</span><br><span class="line">	[-<span class="built_in">du</span> [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">	[-expunge]	<span class="comment"># 清空回收站</span></span><br><span class="line">	[-find &lt;path&gt; ... &lt;expression&gt; ...]	<span class="comment"># 查找</span></span><br><span class="line">	[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]	<span class="comment"># 下载</span></span><br><span class="line">	[-getfacl [-R] &lt;path&gt;]	</span><br><span class="line">	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">	[-getmerge [-<span class="built_in">nl</span>] &lt;src&gt; &lt;localdst&gt;]	<span class="comment"># 下载并合并</span></span><br><span class="line">	[-<span class="built_in">help</span> [cmd ...]]	<span class="comment"># 查看命令的说明文档</span></span><br><span class="line">	[-<span class="built_in">ls</span> [-d] [-h] [-R] [&lt;path&gt; ...]]	<span class="comment"># 列出目录中的内容</span></span><br><span class="line">	[-<span class="built_in">mkdir</span> [-p] &lt;path&gt; ...]	<span class="comment"># 创建目录</span></span><br><span class="line">	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]	<span class="comment"># 从本地上传到HDFS（上传完毕后删除本地文件）</span></span><br><span class="line">	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]	<span class="comment"># 从HDFS下载到本地（下载完毕后删除HDFS中的文件）</span></span><br><span class="line">	[-<span class="built_in">mv</span> &lt;src&gt; ... &lt;dst&gt;]	<span class="comment"># 移动或重命名</span></span><br><span class="line">	[-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]	<span class="comment"># 上传</span></span><br><span class="line">	[-renameSnapshot &lt;snapshotDir&gt; &lt;olDataNodeame&gt; &lt;newName&gt;]	<span class="comment"># 重命名快照</span></span><br><span class="line">	[-<span class="built_in">rm</span> [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]	<span class="comment"># 删除</span></span><br><span class="line">	[-<span class="built_in">rmdir</span> [--ignore-fail-on-non-empty] &lt;<span class="built_in">dir</span>&gt; ...]	<span class="comment"># 删除目录</span></span><br><span class="line">	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--<span class="built_in">set</span> &lt;acl_spec&gt; &lt;path&gt;]]	<span class="comment"># 设置文件权限</span></span><br><span class="line">	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]	<span class="comment"># 设置文件属性</span></span><br><span class="line">	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]	<span class="comment"># 设置某个文件的副本数量</span></span><br><span class="line">	[-<span class="built_in">stat</span> [format] &lt;path&gt; ...]	<span class="comment"># </span></span><br><span class="line">	[-<span class="built_in">tail</span> [-f] &lt;file&gt;]</span><br><span class="line">	[-<span class="built_in">test</span> -[defsz] &lt;path&gt;]</span><br><span class="line">	[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-touchz &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">truncate</span> [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">	[-usage [cmd ...]]</span><br><span class="line"></span><br><span class="line">Generic options supported are</span><br><span class="line">-conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;            use value <span class="keyword">for</span> given property</span><br><span class="line">-fs &lt;<span class="built_in">local</span>|namenode:port&gt;      specify a namenode</span><br><span class="line">-jt &lt;<span class="built_in">local</span>|resourcemanager:port&gt;    specify a ResourceManager</span><br><span class="line">-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include <span class="keyword">in</span> the classpath.</span><br><span class="line">-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.</span><br><span class="line"></span><br><span class="line">The general <span class="built_in">command</span> line syntax is</span><br><span class="line">bin/hadoop <span class="built_in">command</span> [genericOptions] [commandOptions]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><strong>-help 命令测试</strong></p>
<ul>
<li><p>查看 <code>-count</code> 命令的说明文档</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hadoop fs -<span class="built_in">help</span> count</span><br><span class="line">-count [-q] [-h] &lt;path&gt; ... :</span><br><span class="line">	<span class="comment"># 统计目录中下的文件夹数量、文件数量、字节数</span></span><br><span class="line">  Count the number of directories, files and bytes under the paths</span><br><span class="line">  that match the specified file pattern.  The output columns are:</span><br><span class="line">  DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME or</span><br><span class="line">  QUOTA REMAINING_QUOTA SPACE_QUOTA REMAINING_SPACE_QUOTA </span><br><span class="line">        DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME</span><br><span class="line">  The -h option shows file sizes <span class="keyword">in</span> human readable format.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="hadoop-fs-和-hdfs-dfs"><a href="#hadoop-fs-和-hdfs-dfs" class="headerlink" title="hadoop fs 和 hdfs dfs"></a>hadoop fs 和 hdfs dfs</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs 具体命令 </span><br><span class="line"><span class="comment"># 和</span></span><br><span class="line">hdfs dfs 具体命令</span><br><span class="line"></span><br><span class="line"><span class="comment"># 的功能是类似的</span></span><br></pre></td></tr></table></figure>



<blockquote>
<p>Tip：</p>
</blockquote>
<p><code>hadoop fs</code> 命令既可以在本地模式在生效，也可以在分布式模式下生效；而 <code>hdfs dfs</code> 只能在分布式模式下生效！</p>
<p><strong>hdfs 命令</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 hadoop]$ hdfs</span><br><span class="line">Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND</span><br><span class="line">       <span class="built_in">where</span> COMMAND is one of:</span><br><span class="line">  dfs                  run a filesystem <span class="built_in">command</span> on the file systems supported <span class="keyword">in</span> Hadoop.	<span class="comment"># 也是运行一个</span></span><br><span class="line">  classpath            prints the classpath</span><br><span class="line">  namenode -format     format the DFS filesystem</span><br><span class="line">  secondarynamenode    run the DFS secondary namenode</span><br><span class="line">  namenode             run the DFS namenode</span><br><span class="line">  journalnode          run the DFS journalnode</span><br><span class="line">  zkfc                 run the ZK Failover Controller daemon</span><br><span class="line">  datanode             run a DFS datanode</span><br><span class="line">  dfsadmin             run a DFS admin client</span><br><span class="line">  haadmin              run a DFS HA admin client</span><br><span class="line">  fsck                 run a DFS filesystem checking utility</span><br><span class="line">  balancer             run a cluster balancing utility</span><br><span class="line">  jmxget               get JMX exported values from NameNode or DataNode.</span><br><span class="line">  mover                run a utility to move block replicas across</span><br><span class="line">                       storage types</span><br><span class="line">  oiv                  apply the offline fsimage viewer to an fsimage</span><br><span class="line">  oiv_legacy           apply the offline fsimage viewer to an legacy fsimage</span><br><span class="line">  oev                  apply the offline edits viewer to an edits file</span><br><span class="line">  fetchdt              fetch a delegation token from the NameNode</span><br><span class="line">  getconf              get config values from configuration</span><br><span class="line">  <span class="built_in">groups</span>               get the <span class="built_in">groups</span> <span class="built_in">which</span> <span class="built_in">users</span> belong to</span><br><span class="line">  snapshotDiff         diff two snapshots of a directory or diff the</span><br><span class="line">                       current directory contents with a snapshot</span><br><span class="line">  lsSnapshottableDir   list all snapshottable <span class="built_in">dirs</span> owned by the current user</span><br><span class="line">						Use -<span class="built_in">help</span> to see options</span><br><span class="line">  portmap              run a portmap service</span><br><span class="line">  nfs3                 run an NFS version 3 gateway</span><br><span class="line">  cacheadmin           configure the HDFS cache</span><br><span class="line">  crypto               configure HDFS encryption zones</span><br><span class="line">  storagepolicies      list/get/set block storage policies</span><br><span class="line">  version              <span class="built_in">print</span> the version</span><br><span class="line"></span><br><span class="line">Most commands <span class="built_in">print</span> <span class="built_in">help</span> when invoked w/o parameters.</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><h3 id="命令分类"><a href="#命令分类" class="headerlink" title="命令分类"></a>命令分类</h3><blockquote>
<p>本地 –&gt;&gt; HDFS（上传）</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-put			# 拷贝</span><br><span class="line">-copyFromLocal	 # 可以进行多线程拷贝</span><br><span class="line">-moveFromLocal	 # 移动 </span><br><span class="line">-appendToFile	 # 追加</span><br></pre></td></tr></table></figure>

<blockquote>
<p>HDFS –&gt;&gt; HDFS </p>
</blockquote>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-<span class="built_in">cd</span></span><br><span class="line">-<span class="built_in">mv</span></span><br><span class="line">-<span class="built_in">chown</span></span><br><span class="line">-<span class="built_in">chmod</span></span><br><span class="line">-<span class="built_in">du</span></span><br><span class="line">-<span class="built_in">df</span></span><br><span class="line">-<span class="built_in">cat</span></span><br><span class="line">-<span class="built_in">rm</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>HDFS –&gt;&gt; 本地（下载）</p>
</blockquote>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-get			<span class="comment"># 下载</span></span><br><span class="line">-copyToLocal	<span class="comment"># 下载（等价于 get）</span></span><br><span class="line">-getmerge		<span class="comment"># 合并下载。把 HDFS 上的一批可以用通配符匹配的文件先合并成一个文件，再下载到本地。合并的顺序是文件在 HDFS 上的排列顺序</span></span><br></pre></td></tr></table></figure>





<h3 id="常用命令实操"><a href="#常用命令实操" class="headerlink" title="常用命令实操"></a>常用命令实操</h3><h4 id="1、准备工作"><a href="#1、准备工作" class="headerlink" title="1、准备工作"></a>1、准备工作</h4><ol>
<li>启动Hadoop集群（方便后续的测试）</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>



<ol start="2">
<li>-help：输出这个命令参数</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -help rm</span><br></pre></td></tr></table></figure>

<h4 id="2、上传"><a href="#2、上传" class="headerlink" title="2、上传"></a>2、上传</h4><blockquote>
<ol>
<li><code>-moveFromLocal</code>：从本地剪切粘贴到HDFS</li>
</ol>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ touch kongming.txt</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo</span><br></pre></td></tr></table></figure>



<blockquote>
<ol start="2">
<li>-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</li>
</ol>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -copyFromLocal README.txt /</span><br></pre></td></tr></table></figure>



<blockquote>
<ol start="3">
<li>-appendToFile：追加一个文件到已经存在的文件末尾</li>
</ol>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ touch liubei.txt</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ vi liubei.txt</span><br><span class="line"></span><br><span class="line">输入</span><br><span class="line"></span><br><span class="line">san gu mao lu</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>





<blockquote>
<ol start="4">
<li>-put：等同于copyFromLocal</li>
</ol>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -put ./zaiyiqi.txt /user/atguigu/test/</span><br></pre></td></tr></table></figure>



<h3 id="2-3-3-下载"><a href="#2-3-3-下载" class="headerlink" title="2.3.3 下载"></a>2.3.3 下载</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">1）-copyToLocal：从HDFS拷贝到本地</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./</span><br><span class="line"></span><br><span class="line">2）-get：等同于copyToLocal，就是从HDFS下载文件到本地</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -get /sanguo/shuguo/kongming.txt ./</span><br><span class="line"></span><br><span class="line">3）-getmerge：合并下载多个文件，比如HDFS的目录 /user/atguigu/test下有多个文件:log.1, log.2,log.3,...</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -getmerge /user/atguigu/test/* ./zaiyiqi.txt</span><br><span class="line"></span><br><span class="line">### 2.3.4 HDFS直接操作</span><br><span class="line"></span><br><span class="line">1）-ls: 显示目录信息</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -ls /</span><br><span class="line"></span><br><span class="line">2）-mkdir：在HDFS上创建目录</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir -p /sanguo/shuguo</span><br><span class="line"></span><br><span class="line">3）-cat：显示文件内容</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -cat /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">4）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -chmod 666 /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -chown atguigu:atguigu  /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">5）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt</span><br><span class="line"></span><br><span class="line">6）-mv：在HDFS目录中移动文件</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mv /zhuge.txt /sanguo/shuguo/</span><br><span class="line"></span><br><span class="line">7）-tail：显示一个文件的末尾</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -tail /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">8）-rm：删除文件或文件夹</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -rm /user/atguigu/test/jinlian2.txt</span><br><span class="line"></span><br><span class="line">9）-rmdir：删除空目录</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /test</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -rmdir /test</span><br><span class="line"></span><br><span class="line">10）-du统计文件夹的大小信息</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -du -s -h /user/atguigu/test</span><br><span class="line"></span><br><span class="line">2.7 K /user/atguigu/test</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -du -h /user/atguigu/test</span><br><span class="line"></span><br><span class="line">1.3 K /user/atguigu/test/README.txt</span><br><span class="line"></span><br><span class="line">15   /user/atguigu/test/jinlian.txt</span><br><span class="line"></span><br><span class="line">1.4 K /user/atguigu/test/zaiyiqi.txt</span><br><span class="line"></span><br><span class="line">11）-setrep：设置HDFS中文件的副本数量</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -setrep 10 /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。</span><br></pre></td></tr></table></figure>







<hr>
<h1 id="HDFS-客户端操作"><a href="#HDFS-客户端操作" class="headerlink" title="HDFS 客户端操作"></a>HDFS 客户端操作</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在对 HDFS（Hadoop 的分布式文件系统）进行操作时，又可分为<strong>服务端</strong>操作和<strong>客户端</strong>操作两部分。</p>
<p><strong>服务端：</strong></p>
<ul>
<li>  启动 NameNode，DataNode，2NN 等；</li>
</ul>
<p><strong>客户端（可以有多种形式）：</strong></p>
<ul>
<li>使用 Shell 客户端。    使用 <code>hadoop fs -命令</code> 对 HDFS 进行操作。</li>
<li>使用 Java 客户端。    使用 JavaAPI 和方法对 HDFS 进行操作。 </li>
<li>使用 Python 客户端。    使用 PythonAPI 和方法对 HDFS 进行操作。</li>
<li>……</li>
</ul>
<hr>
<h2 id="Java-客户端环境准备"><a href="#Java-客户端环境准备" class="headerlink" title="Java 客户端环境准备"></a>Java 客户端环境准备</h2><ol>
<li><p> 在 Windows 上安装 Hadoop 客户端。</p>
</li>
<li><p>配置 HADOOP_HOME 环境变量</p>
<p> <img src="HDFS/image-20211124095328168.png" alt="image-20211124095328168"></p>
</li>
<li><p>配置 PATH 环境变量。然后重启电脑。</p>
<p> <img src="HDFS/image-20211124095406232.png" alt="image-20211124095406232"></p>
</li>
<li><p>测试</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\admin&gt;hadoop version</span><br><span class="line">Hadoop <span class="number">2.7</span><span class="number">.2</span></span><br><span class="line">Subversion https:<span class="comment">//git-wip-us.apache.org/repos/asf/hadoop.git -r b165c4fe8a74265c792ce23f546c64604acf0e41</span></span><br><span class="line">Compiled by jenkins on <span class="number">2016</span>-<span class="number">01</span>-26T00:08Z</span><br><span class="line">Compiled with protoc <span class="number">2.5</span><span class="number">.0</span></span><br><span class="line">From source with checksum d0fda26633fa762bff87ec759ebe689c</span><br><span class="line">This command was run using /D:/Software/Dev/hadoop-<span class="number">2.7</span><span class="number">.2</span>/share/hadoop/common/hadoop-common-<span class="number">2.7</span><span class="number">.2</span>.jar</span><br></pre></td></tr></table></figure></li>
<li><p>创建一个Maven工程，并导入相关依赖</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 使用 Hadoop中Common工具类所需的依赖--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- Hadoop 客户端依赖--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 使用 Hadoop hdfs 所需的依赖--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="HDFS-的-API-操作"><a href="#HDFS-的-API-操作" class="headerlink" title="HDFS 的 API 操作"></a>HDFS 的 API 操作</h2><h3 id="FileSystem"><a href="#FileSystem" class="headerlink" title="FileSystem"></a>FileSystem</h3><blockquote>
<p><code>org.apache.hadoop.fs.FileSystem;</code></p>
</blockquote>
<p><strong>FileSystem</strong> 是分布式文件系统的<strong>基类</strong>，它是一个抽象类，常见的有两种实现：</p>
<ol>
<li>本地文件系统：<code>LocalFileSystem</code>，相当于设置了 <code>fs.defaultFS=file:///</code></li>
<li>分布式文件系统：<code>DistributeFileSystem</code>，相当于设置了`fs.defaultFS=hdfs://IP地址:端口号``</li>
<li><code>FileSystem</code> 对象到底是本地文件系统对象还是分布式文件系统对象取决于配置文件中 <code>fs.defaultFS</code> 的值</li>
</ol>
<p><code>FileSystem</code>是一个抽象类，不能直接 new ，但是可以通过静态方法 <code>FileSystem.get()</code> 方法来创建该类的实现类对象。<code>get()</code> 有3个重载方法，分别是：</p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>get(Configuration conf)</code></td>
<td>创建一个FileSystem实现类对象，不指定使用的文件协议，以当前计算机的用户身份登录</td>
</tr>
<tr>
<td align="left"><code>get(URI uri, Configuration conf)</code></td>
<td>创建一个FileSystem对象，指定使用的文件协议，以当前计算机的用户身份登录</td>
</tr>
<tr>
<td align="left"><code>get(final URI uri, final Configuration conf, final String user)</code></td>
<td>创建一个FileSystem对象，指定使用的文件协议，并以指定身份登录</td>
</tr>
</tbody></table>
<ul>
<li>  可以看到 <code>get</code> 方法会依赖于一个 <strong>Configuration</strong> 对象，这个对象在下一章节就会讲到。</li>
</ul>
<hr>
<h3 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h3><blockquote>
<p><code>org.apache.hadoop.conf.Configuration</code></p>
</blockquote>
<p><strong>作用</strong>：读取配置文件中的参数。</p>
<p>Configuration 在读取配置文件时，会读取 4~8 个配置文件。</p>
<ol>
<li> Configuration 对象在加载时首先会读取 4 个默认配置文件 <code>xxx-default.xml</code>；</li>
<li> 如果类路径（classpath）下存在自定义配置文件 <code>xxx-site.xml</code>，Configuration 对象会继续读取这些自定义配置文件并使用自定义配置文件中的属性覆盖默认配置文件中的同名属性。 </li>
<li> 也就是说，Configuration 类一加载就会默认读取8个配置文件，将8个配置文件中所有属性读取到一个Map集合中；</li>
<li> 当然，该类也提供了 <code>set(name, value)</code> 方法，用于在代码中覆盖配置文件中的参数。</li>
</ol>
<p><strong>源码解读：</strong></p>
<p>Configuration 类的源码中有一段静态代码块。静态代码块中的代码在类加载到 JVM 中时就被执行。所以说该静态代码块中的内容一定是在创建 Configuration 对象前执行的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Configuration</span> <span class="keyword">implements</span> <span class="title class_">Iterable</span>&lt;Map.Entry&lt;String,String&gt;&gt;,</span><br><span class="line">Writable &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">static</span>&#123;</span><br><span class="line">        <span class="comment">//print deprecation warning if hadoop-site.xml is found in classpath</span></span><br><span class="line">        <span class="comment">// 1、获取类加载器</span></span><br><span class="line">        <span class="type">ClassLoader</span> <span class="variable">cL</span> <span class="operator">=</span> Thread.currentThread().getContextClassLoader();</span><br><span class="line">        <span class="keyword">if</span> (cL == <span class="literal">null</span>) &#123;</span><br><span class="line">            cL = Configuration.class.getClassLoader();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 2、如果在类路径下发现了 hadoop-site.xml 配置文件，打印警告</span></span><br><span class="line">        <span class="comment">//【因为 hadoop-site.xml 是 Hadoop 1.x 中使用的配置文件，在Hadoop2.x中已经过时了，</span></span><br><span class="line">        <span class="comment">// 它的作用是将所有配置信息配置到一个配置文件中】</span></span><br><span class="line">        <span class="keyword">if</span>(cL.getResource(<span class="string">&quot;hadoop-site.xml&quot;</span>)!=<span class="literal">null</span>) &#123;</span><br><span class="line">            LOG.warn(<span class="string">&quot;DEPRECATED: hadoop-site.xml found in the classpath. &quot;</span> +</span><br><span class="line">                     <span class="string">&quot;Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, &quot;</span>	</span><br><span class="line">                     + <span class="string">&quot;mapred-site.xml and hdfs-site.xml to override properties of &quot;</span> +</span><br><span class="line">                     <span class="string">&quot;core-default.xml, mapred-default.xml and hdfs-default.xml &quot;</span> +</span><br><span class="line">                     <span class="string">&quot;respectively&quot;</span>);</span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">        翻译：</span></span><br><span class="line"><span class="comment">        	hadoop-site.xml 已经被弃用了，应该用 core-site.xml, mapred-site.xml </span></span><br><span class="line"><span class="comment">        	和 hdfs-site.xml 去覆盖这个配置文件</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3、加载默认和自定义配置文件。</span></span><br><span class="line">        addDefaultResource(<span class="string">&quot;core-default.xml&quot;</span>);</span><br><span class="line">        addDefaultResource(<span class="string">&quot;core-site.xml&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>配置文件</strong></p>
<p>在我们引入 Hadoop 相关的 Maven 依赖时，这些依赖中就已经包含了 Hadoop 四个常用的配置文件，所以在创建 <code>new Configuration()</code> 对象时首先会加载这四个配置文件得到 <code>configuration</code> 对象。如果想要得到加载自定义配置文件的 <code>configuration</code> 对象，就需要在类路径下创建对应的 <code>xxx-site.xml</code> 配置文件来重写默认配置文件中的属性。</p>
<p><img src="HDFS/image-20211124173109470.png" alt="image-20211124173109470"></p>
<ul>
<li><p>举例：</p>
  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在 core-default.xml 配置文件中使用的是本地文件系统，即</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">如果我们想要使用 HDFS 的分布式文件系统，就需要在类路径下创建一个 core-site.xml 文件，覆盖原先的配置。</span><br><span class="line">这时我们再通过 `new Configuration()` 得到的对象就是一个分布式文件系统的配置对象。</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><ol>
<li><p>创建 HDFSClient </p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">HDFSClient 测试：</span></span><br><span class="line"><span class="comment">	1、本地模式的 HDFSClient</span></span><br><span class="line"><span class="comment">	2、分布式模式的 HDFSClient</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">mkdir</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="comment">// 1、创建 fileSystem 对象，用于连接 HDFS 集群</span></span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(conf);</span><br><span class="line">    <span class="comment">// 2、测试</span></span><br><span class="line">    System.out.println(fs);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">分析：</span></span><br><span class="line"><span class="comment">	1、当我们使用默认的配置文件创建 fs 对象时，得到的是一个`本地文件系统对象`，输出结果为：</span></span><br><span class="line"><span class="comment">		org.apache.hadoop.fs.LocalFileSystem@606e4010</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">	2、如果我们在类路径下自定义一个 `core-site.xml` 配置文件（配置文件见下文）并重写 </span></span><br><span class="line"><span class="comment">	fs.defaultFS=hdfs://hadoop102:9000，这时再创建 fs 对象，自定义的配置文件就会重写</span></span><br><span class="line"><span class="comment">	默认的配置文件中的同名属性，这时得到的 fs 对象就是一个`分布式文件系统对象`，输出的结果为：</span></span><br><span class="line"><span class="comment">		DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-53042264_1, ugi=admin (auth:SIMPLE)]]</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></li>
<li><p>调用 fs 对象的 <code>mkdirs</code>方法在 HDFS 文件系统的根目录下创建一个文件</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">mkdir</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">// 1、创建客户端对象，用于连接 HDFS 集群</span></span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line">    <span class="comment">// 2、调用 mkdirs() 方法在 HDFS 集群的根路径创建一个目录</span></span><br><span class="line">    fs.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/IDEA&quot;</span>));</span><br><span class="line">    <span class="comment">// 3、释放资源</span></span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">分析：</span></span><br><span class="line"><span class="comment">	程序运行失败，抛出异常。</span></span><br><span class="line"><span class="comment">	异常信息是： org.apache.hadoop.security.AccessControlException: Permission denied: user=admin, access=WRITE, inode=&quot;/&quot;:lvnengdong:supergroup:drwxr-xr-x</span></span><br><span class="line"><span class="comment">	我们可以看到这是一个访问控制异常，也就是说权限被拒绝。</span></span><br><span class="line"><span class="comment">	原因是：当前登录 HDFS 的用户是admin（这个是我Windows系统的用户名），而 Hadoop 集群所属的用户是 lvnengdong。</span></span><br><span class="line"><span class="comment">	根据权限控制信息 drwxr-xr-x 所示，非主非组用户所拥有的权限只有 r-x ，而没有写权限，所以 admin 用户不能在Hadoop集群上创建目录！</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">解决方法：</span></span><br><span class="line"><span class="comment">    1、修改 Hadoop 集群对游客的访问权限，赋予游客写的权限</span></span><br><span class="line"><span class="comment">    2、使用拥有写权限的用户（这里是 lvnengdong 用户）登录</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></li>
<li><p>修改代码（方式一）：以指定用户登录（这里是拥有写权限的 lvnengdong 用户）</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">mkdir</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">URI</span> <span class="variable">uri</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>);</span><br><span class="line">    <span class="keyword">final</span> <span class="type">String</span> <span class="variable">user</span> <span class="operator">=</span> <span class="string">&quot;lvnengdong&quot;</span>;</span><br><span class="line">    <span class="comment">// </span></span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(uri, configuration, user);</span><br><span class="line">    fs.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/IDEA &quot;</span>));</span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">分析：</span></span><br><span class="line"><span class="comment">	* 使用 fs 对象的 get 方法，可以指定访问 HDFS 系统的用户身份（这里指定为 lvnengdong）。</span></span><br><span class="line"><span class="comment">	* 由于 Hadoop 采用一种弱权限验证，所以无需验证密码，可直接以 lvnengdong 这个身份登录。</span></span><br><span class="line"><span class="comment">	* 重载方法中的另一个参数是 URI 对象，该对象的介绍见下文。</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>  <strong>弱权限验证</strong></p>
<ul>
<li>Hadoop 的权限验证是一种弱权限验证</li>
<li><strong>弱权限验证</strong>：你告诉 Hadoop 你是谁，它就认为你是谁，它并不会检查你是谁！</li>
</ul>
</blockquote>
</li>
<li><p>不用修改代码（方式二）</p>
<p> （1）服务端操作</p>
<ul>
<li>  <strong>原理：</strong>为了使用方便，我们还可以将 HDFS 根目录的权限完全放开！即允许任何用户都能在 HDFS 的目录下执行任何操作！</li>
<li>  <strong>操作：</strong>在 HDFS 服务器中的任一节点上执行 ： <code>hadoop fs -chmod -R 777 /</code> 命令，开放所有用户对该目录的操作权限！</li>
</ul>
<p> （2）客户端操作</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 这时我们再用 Windows 上的用户（或任意用户）在 HDFS 集群上进行创建目录操作时，也可以成功！</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">mkdir</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException &#123;</span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">URI</span> <span class="variable">uri</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:8020&quot;</span>);</span><br><span class="line">    <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(uri, conf);</span><br><span class="line">    fs.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/IDEA3 &quot;</span>));</span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h4 id="URI"><a href="#URI" class="headerlink" title="URI"></a>URI</h4><blockquote>
<p><code>java.net.URI</code></p>
</blockquote>
<ul>
<li><code>URI</code> 指的是如何连接到 HDFS 文件系统。（主要包括：1️⃣连接协议2️⃣连接地址）</li>
<li>可以是<strong>本地模式</strong>，使用 file文件系统协议：<code>file:/// </code></li>
<li>也可以是<strong>分布式模式</strong>，使用分布式文件系统协议：<code>hdfs://IP地址:端口号</code>（我的是    <code>hdfs://hadoop102:9000</code>）</li>
</ul>
<p><strong>Tip:</strong></p>
<ul>
<li>  如果使用 <code>FileSystem.get()</code> 的重载方法中包含 URI 这一参数，就相当于在代码中手动设置了 <code>fs.defaultFS=$uri</code>，这时就无需在自定义配置文件 <code>core-site.xml</code> 中重写 <code>fs.defaultFS</code>属性了，因为最终的 <code>fs.defaultFS</code> 的最终属性以代码中的 uri 为准。</li>
<li>  URI 这个参数对应的值实际就是<code>core-site.xml</code>配置文件中<code>fs.defaultFS</code> 的值，包含了连接 HDFS 使用的协议和 NameNode 的地址（如果是分布式文件系统）。</li>
</ul>
<hr>
<h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><ul>
<li>我们在搭建集群时会配置很多参数，</li>
<li>这些参数有些是由客户端来使用，有些是由服务端来使用，有些是客户端和服务端都来要使用。</li>
<li>对于仅在客户端使用的参数，在服务端可以无需配置，同理对于仅在服务端使用的参数，在客户端也可以不配置。而对于客户端和服务器端都需要使用的参数，则需要在客户端和服务器端都要配置。</li>
</ul>
<blockquote>
<p>  <strong>core-site.xml</strong></p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">        这个参数是 NameNode 的地址，客户端和服务端都需要使用！</span></span><br><span class="line"><span class="comment">        客户端需要知道 NameNode 的地址向 NameNode 发送请求。如上传文件时首先 NameNode 请求 分配资源；</span></span><br><span class="line"><span class="comment">        服务端上的 DataNode 节点也需要知道 NameNode 的地址与 NameNode 进行通信，如 DataNode 在启动时要加入到 Hadoop 集群中，也要知道 NameNode 的地址。</span></span><br><span class="line"><span class="comment">    --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- HDFS中NameNode的RPC地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 这个参数只在服务端使用，所以在客户端的配置文件中就可以省略！ --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定Hadoop运行时产生的文件保存的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>







<hr>
<h3 id="增删改查"><a href="#增删改查" class="headerlink" title="增删改查"></a>增删改查</h3><h4 id="1、创建目录"><a href="#1、创建目录" class="headerlink" title="1、创建目录"></a>1、创建目录</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HDFSClient</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 声明一个全局变量 fs</span></span><br><span class="line">    <span class="keyword">private</span> FileSystem fs;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">     	<span class="comment">// 1、创建 fs 对象</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">URI</span> <span class="variable">uri</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>);</span><br><span class="line">        <span class="type">String</span> <span class="variable">user</span> <span class="operator">=</span> <span class="string">&quot;lvnengdong&quot;</span>;</span><br><span class="line">        fs = FileSystem.get(uri,conf,user);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 3、释放资源</span></span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Shell命令：hadoop fs -mkdir /xxx</span></span><br><span class="line"><span class="comment">     * 分析：</span></span><br><span class="line"><span class="comment">     *  hadoop fs   运行一个客户端</span></span><br><span class="line"><span class="comment">     *  -mkdir  编译好的脚本</span></span><br><span class="line"><span class="comment">     *  /xxx    用户指定的参数</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     *  Java 命令（面向对象）</span></span><br><span class="line"><span class="comment">     *  1、创建一个客户端对象</span></span><br><span class="line"><span class="comment">     *  2、调用创建目录的方法</span></span><br><span class="line"><span class="comment">     *  3、路径作为方法的参数</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">mkdir</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 2、调用 mkdirs() 方法，创建一个文件夹</span></span><br><span class="line">        fs.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/IDEA3 &quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2、上传文件"><a href="#2、上传文件" class="headerlink" title="2、上传文件"></a>2、上传文件</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 上传文件</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test01</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">       <span class="type">Path</span> <span class="variable">src</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;C:\\Users\\admin\\Desktop\\image\\demo01.jpg&quot;</span>);</span><br><span class="line">       <span class="type">Path</span> <span class="variable">dst</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/&quot;</span>);</span><br><span class="line">       fs.copyFromLocalFile(src, dst);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>



<h4 id="3、下载文件"><a href="#3、下载文件" class="headerlink" title="3、下载文件"></a>3、下载文件</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 下载文件</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test02</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">       <span class="type">Path</span> <span class="variable">src</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/README.txt&quot;</span>);</span><br><span class="line">       <span class="type">Path</span> <span class="variable">dst</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;d:/&quot;</span>);</span><br><span class="line">       fs.copyToLocalFile(src, dst);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<h4 id="4、删除文件"><a href="#4、删除文件" class="headerlink" title="4、删除文件"></a>4、删除文件</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 删除文件</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">    */</span>	</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test03</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">       <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/IDEA3&quot;</span>);</span><br><span class="line">       <span class="comment">// 参数1：要删除文件的路径     参数2：是否递归删除</span></span><br><span class="line">       fs.delete(path, <span class="literal">true</span>);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<h4 id="5、重命名"><a href="#5、重命名" class="headerlink" title="5、重命名"></a>5、重命名</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 重命名</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test04</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">       <span class="type">Path</span> <span class="variable">src</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/IDEA2&quot;</span>);</span><br><span class="line">       <span class="type">Path</span> <span class="variable">dst</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/newIDEA2&quot;</span>);</span><br><span class="line">       fs.rename(src, dst);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<h4 id="6、判断当前路径是否存在"><a href="#6、判断当前路径是否存在" class="headerlink" title="6、判断当前路径是否存在"></a>6、判断当前路径是否存在</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 判断当前路径是否存在</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test05</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">       <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/IDEA&quot;</span>);</span><br><span class="line">       <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> fs.exists(path);</span><br><span class="line">       System.out.println(b);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<h4 id="7、判断当前路径是目录还是文件"><a href="#7、判断当前路径是目录还是文件" class="headerlink" title="7、判断当前路径是目录还是文件"></a>7、判断当前路径是目录还是文件</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 判断当前路径是目录还是文件</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test06</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/IDEA&quot;</span>);</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b1</span> <span class="operator">=</span> fs.isFile(path);</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b2</span> <span class="operator">=</span> fs.isDirectory(path);</span><br><span class="line">        System.out.println(b1);</span><br><span class="line">        System.out.println(b2);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">	但是不建议使用这两个方法，这两个方法已经过期。建议使用 FileStatus</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>



<h5 id="重要：FileStatus"><a href="#重要：FileStatus" class="headerlink" title="重要：FileStatus"></a>重要：FileStatus</h5><p>当前文件的相关信息(文件/文件夹的属性信息)</p>
<blockquote>
<p>源码（部分）：<code>FileStatus</code> 中保存的属性信息。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FileStatus</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Path path;	<span class="comment">// 完整的路径（包括文件名）</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> length;</span><br><span class="line">    <span class="keyword">private</span> Boolean isdir;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">short</span> block_replication;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> blocksize;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> modification_time;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> access_time;</span><br><span class="line">    <span class="keyword">private</span> FsPermission permission;</span><br><span class="line">    <span class="keyword">private</span> String owner;</span><br><span class="line">    <span class="keyword">private</span> String group;</span><br><span class="line">    <span class="keyword">private</span> Path symlink;</span><br><span class="line">    <span class="keyword">private</span> Set&lt;AttrFlags&gt; attr;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试1：判断当前路径是目录还是文件</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 判断当前路径是目录还是文件</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test07</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">       <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/IDEA&quot;</span>);</span><br><span class="line">       <span class="comment">// 1、得到 FileStatus 对象，保存了一个文件的属性信息</span></span><br><span class="line">       <span class="type">FileStatus</span> <span class="variable">info</span> <span class="operator">=</span> fs.getFileStatus(path);</span><br><span class="line">       <span class="comment">// 2、根据 status 对象判断当前路径是文件还是目录</span></span><br><span class="line">       <span class="type">boolean</span> <span class="variable">b1</span> <span class="operator">=</span> info.isDirectory();</span><br><span class="line">       <span class="type">boolean</span> <span class="variable">b2</span> <span class="operator">=</span> info.isFile();</span><br><span class="line">       System.out.println(b1);</span><br><span class="line">       System.out.println(b2);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>



<h5 id="重要：ListStatus"><a href="#重要：ListStatus" class="headerlink" title="重要：ListStatus"></a>重要：ListStatus</h5><p><code>ListStatus</code> 的作用与 <code>FileStatus</code> 相似，但是 <code>ListStatus</code> 可以列出当前路径及其子路径的所有文件/文件夹的属性信息。</p>
<blockquote>
<p>测试：</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test08</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">      <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/output2&quot;</span>);</span><br><span class="line">      <span class="comment">// 1、得到 FileStatus[] 对象，保存了当前路径及其子路径的属性信息</span></span><br><span class="line">      FileStatus[] listInfo = fs.listStatus(path);</span><br><span class="line">      <span class="keyword">for</span> (FileStatus info : listInfo) &#123;</span><br><span class="line">          <span class="comment">// 2、获取文件路径。Path 是完整的路径：协议+文件名</span></span><br><span class="line">          <span class="type">Path</span> <span class="variable">filePath</span> <span class="operator">=</span> info.getPath();</span><br><span class="line">          <span class="comment">// 3、如果只想获取文件名，可以使用</span></span><br><span class="line">          <span class="type">String</span> <span class="variable">fileName</span> <span class="operator">=</span> filePath.getName();</span><br><span class="line">          System.out.println(fileName + <span class="string">&quot;是否是目录：&quot;</span> + status.isDirectory());</span><br><span class="line">          System.out.println(fileName + <span class="string">&quot;是否是文件：&quot;</span> + status.isFile());</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="8、获取文件的块信息"><a href="#8、获取文件的块信息" class="headerlink" title="8、获取文件的块信息"></a>8、获取文件的块信息</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">offset和length</span><br><span class="line">	offset是偏移量： 指块在文件中的起始位置</span><br><span class="line">	length是长度，指块中文件的大小</span><br><span class="line"></span><br><span class="line">    sts.zip 390M</span><br><span class="line">    length    offset</span><br><span class="line">    blk1:   0-128M      128M		0</span><br><span class="line">    blk2:    128M-256M  128M        128M</span><br><span class="line">    ...</span><br><span class="line">    blk4:    384M-390M  6M          384M</span><br><span class="line"></span><br><span class="line">LocatedFileStatus</span><br><span class="line">	LocatedFileStatus是FileStatus的子类，除了文件的属性，还有块的位置信息！</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取到文件的块信息</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testGetBlockInfomation</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/sts.zip&quot;</span>);</span><br><span class="line"></span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; status = fs.listLocatedStatus(path);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(status.hasNext()) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">LocatedFileStatus</span> <span class="variable">locatedFileStatus</span> <span class="operator">=</span> status.next();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;Ownner:&quot;</span>+locatedFileStatus.getOwner());</span><br><span class="line">        System.out.println(<span class="string">&quot;Group:&quot;</span>+locatedFileStatus.getGroup());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//---------------块的位置信息--------------------</span></span><br><span class="line">        BlockLocation[] blockLocations = locatedFileStatus.getBlockLocations();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line"></span><br><span class="line">            System.out.println(blockLocation);</span><br><span class="line"></span><br><span class="line">            System.out.println(<span class="string">&quot;------------------------&quot;</span>);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="自定义上传和下载"><a href="#自定义上传和下载" class="headerlink" title="自定义上传和下载"></a>自定义上传和下载</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.junit.Assert.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.OutputStream;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.junit.After;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 1. 上传文件时，只上传这个文件的一部分</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 2. 下载文件时，如何只下载这个文件的某一个块？ </span></span><br><span class="line"><span class="comment"> * 			或只下载文件的某一部分？</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestCustomUploadAndDownload</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FileSystem fs;</span><br><span class="line">    <span class="keyword">private</span> FileSystem localFs;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> <span class="keyword">throws</span> IOException, URISyntaxException &#123;</span><br><span class="line">        <span class="comment">//创建一个客户端对象</span></span><br><span class="line">        fs=FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:9000&quot;</span>),conf);</span><br><span class="line">        localFs=FileSystem.get(<span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="keyword">if</span> (fs !=<span class="literal">null</span>) &#123;</span><br><span class="line">            fs.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 只上传文件的前10M</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">	 * 官方的实现</span></span><br><span class="line"><span class="comment">	 * InputStream in=null;</span></span><br><span class="line"><span class="comment">      OutputStream out = null;</span></span><br><span class="line"><span class="comment">      try &#123;</span></span><br><span class="line"><span class="comment">        in = srcFS.open(src);</span></span><br><span class="line"><span class="comment">        out = dstFS.create(dst, overwrite);</span></span><br><span class="line"><span class="comment">        IOUtils.copyBytes(in, out, conf, true);</span></span><br><span class="line"><span class="comment">      &#125; catch (IOException e) &#123;</span></span><br><span class="line"><span class="comment">        IOUtils.closeStream(out);</span></span><br><span class="line"><span class="comment">        IOUtils.closeStream(in);</span></span><br><span class="line"><span class="comment">        throw e;</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCustomUpload</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">//提供两个Path，和两个FileSystem</span></span><br><span class="line">        Path src=<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;e:/悲惨世界(英文版).txt&quot;</span>);</span><br><span class="line">        Path dest=<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/悲惨世界(英文版)10M.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用本地文件系统中获取的输入流读取本地文件</span></span><br><span class="line">        <span class="type">FSDataInputStream</span> <span class="variable">is</span> <span class="operator">=</span> localFs.open(src);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用HDFS的分布式文件系统中获取的输出流，向dest路径写入数据</span></span><br><span class="line">        <span class="type">FSDataOutputStream</span> <span class="variable">os</span> <span class="operator">=</span> fs.create(dest, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1k</span></span><br><span class="line">        <span class="type">byte</span> [] buffer=<span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">1024</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 流中数据的拷贝</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">1024</span> * <span class="number">10</span>; i++) &#123;</span><br><span class="line">            is.read(buffer);</span><br><span class="line">            os.write(buffer);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关流</span></span><br><span class="line">        IOUtils.closeStream(is);</span><br><span class="line">        IOUtils.closeStream(os);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testFirstBlock</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">//提供两个Path，和两个FileSystem</span></span><br><span class="line">        Path src=<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/sts.zip&quot;</span>);</span><br><span class="line">        Path dest=<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;e:/firstblock&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用HDFS的分布式文件系统中获取的输入流，读取HDFS上指定路径的数据</span></span><br><span class="line">        <span class="type">FSDataInputStream</span> <span class="variable">is</span> <span class="operator">=</span> fs.open(src);</span><br><span class="line">        <span class="comment">// 使用本地文件系统中获取的输出流写入本地文件</span></span><br><span class="line">        <span class="type">FSDataOutputStream</span> <span class="variable">os</span> <span class="operator">=</span> localFs.create(dest, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1k</span></span><br><span class="line">        <span class="type">byte</span> [] buffer=<span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">1024</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 流中数据的拷贝</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">1024</span> * <span class="number">128</span>; i++) &#123;</span><br><span class="line">            is.read(buffer);</span><br><span class="line">            os.write(buffer);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关流</span></span><br><span class="line">        IOUtils.closeStream(is);</span><br><span class="line">        IOUtils.closeStream(os);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testSecondBlock</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">//提供两个Path，和两个FileSystem</span></span><br><span class="line">        Path src=<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/sts.zip&quot;</span>);</span><br><span class="line">        <span class="comment">//Path dest=new Path(&quot;e:/secondblock&quot;);</span></span><br><span class="line">        Path dest=<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;e:/thirdblock&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用HDFS的分布式文件系统中获取的输入流，读取HDFS上指定路径的数据</span></span><br><span class="line">        <span class="type">FSDataInputStream</span> <span class="variable">is</span> <span class="operator">=</span> fs.open(src);</span><br><span class="line">        <span class="comment">// 使用本地文件系统中获取的输出流写入本地文件</span></span><br><span class="line">        <span class="type">FSDataOutputStream</span> <span class="variable">os</span> <span class="operator">=</span> localFs.create(dest, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定位到流的指定位置</span></span><br><span class="line">        is.seek(<span class="number">1024</span>*<span class="number">1024</span>*<span class="number">128</span>*<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1k</span></span><br><span class="line">        <span class="type">byte</span> [] buffer=<span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">1024</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 流中数据的拷贝</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">1024</span> * <span class="number">128</span>; i++) &#123;</span><br><span class="line">            is.read(buffer);</span><br><span class="line">            os.write(buffer);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关流</span></span><br><span class="line">        IOUtils.closeStream(is);</span><br><span class="line">        IOUtils.closeStream(os);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testFinalBlock</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">//提供两个Path，和两个FileSystem</span></span><br><span class="line">        Path src=<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/sts.zip&quot;</span>);</span><br><span class="line">        <span class="comment">//Path dest=new Path(&quot;e:/secondblock&quot;);</span></span><br><span class="line">        Path dest=<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;e:/fourthblock&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用HDFS的分布式文件系统中获取的输入流，读取HDFS上指定路径的数据</span></span><br><span class="line">        <span class="type">FSDataInputStream</span> <span class="variable">is</span> <span class="operator">=</span> fs.open(src);</span><br><span class="line">        <span class="comment">// 使用本地文件系统中获取的输出流写入本地文件</span></span><br><span class="line">        <span class="type">FSDataOutputStream</span> <span class="variable">os</span> <span class="operator">=</span> localFs.create(dest, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定位到流的指定位置</span></span><br><span class="line">        is.seek(<span class="number">1024</span>*<span class="number">1024</span>*<span class="number">128</span>*<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//buffSize 默认不能超过4096</span></span><br><span class="line">        IOUtils.copyBytes(is, os, <span class="number">4096</span>, <span class="literal">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h1 id="HDFS的数据流"><a href="#HDFS的数据流" class="headerlink" title="HDFS的数据流"></a>HDFS的数据流</h1><h2 id="HDFS-写数据流程"><a href="#HDFS-写数据流程" class="headerlink" title="HDFS 写数据流程"></a>HDFS 写数据流程</h2><p><img src="HDFS/image-20211124184155174.png" alt="image-20211124184155174"></p>
<ol>
<li><p> 服务端：启动 NameNode 和 DataNode；</p>
</li>
<li><p> 客户端：创建一个能够连接到 HDFS 的客户端对象 <code>DistributedFileSystem</code>。</p>
</li>
<li><p> 由客户端对象向 NameNode 发送上传文件请求；</p>
</li>
<li><p>NameNode 处理请求，校验客户端请求权限、路径等的合法性</p>
<ul>
<li>  权限校验：NameNode 会检查客户端用户是否有权限操作 HDFS 文件系统；</li>
<li>  路径校验：NameNode 会检查待上传的目标文件是否已存在，目标文件的父目录是否存在等；如果存在，通过 <code>overwrite=true/fasle</code> 属性来决定是否覆盖 HDFS 上现有的文件。</li>
</ul>
</li>
<li><p> 校验通过后，NameNode 响应客户端可以上传文件；</p>
</li>
<li><p>客户端根据 block 参数的大小，将文件切分为一个个 Block，并开始上传第一个块，默认第一块的范围是 0~128M；NameNode 根据客户端设置的文件副本数(默认为3)，通过机架感知策略选取 3 个 DataNode 节点返回，表示采用这三个节点存储数据；</p>
<ul>
<li>  根据机架感知策略，NameNode 返回的 DataNode 列表中，是按照网络拓扑距离离客户端从小到大的顺序排列的，也就是说，列表中的第一个 DataNode 节点距离客户端一定是最近的，之后的每个节点距离客户端越来越远。</li>
</ul>
</li>
<li><p>客户端根据返回的 DataNode 节点列表，请求建立传输通道。</p>
<ul>
<li>  客户端向最近（网络拓扑距离最近）的 DataNode 节点发起通道建立请求，再由这个 DataNode 节点依次向通道中的（距离当前DataNode 网络距离最近的）下一个节点发送建立通道请求，各个节点都响应成功后 ，通道建立成功。</li>
</ul>
</li>
<li><p>客户端每读取 64K 的数据，就会将数据封装成一个 <strong>packet</strong>（数据包，传输的基本单位），通过 <strong>FSDataOutputStream</strong> 对象将 packet 发送到通道的下一个节点，通道中的节点收到 packet 之后，首先落盘存储，然后再将 packet 发送到通道的下一个节点！</p>
<blockquote>
<p>  优点：如果由客户端一个人向所有的 DataNode 进行数据传输的话，有多少个副本，客户端就需要发送多少次数据，当副本数量很大时（比如达到了100），一个客户端就需要向100个客户端都发送数据，显然效率非常慢。在 HDFS 中采用人传人的方式传递数据，比如由客户端向一个 DataNode 发送数据，再由该 DataNode 向下一个 DataNode 继续发送数据，以此类推，达到负载均衡和加速数据传输的效果。</p>
</blockquote>
</li>
<li><p> 每个节点在收到 packet 后，都会向客户端发送 ack 确认消息。</p>
</li>
<li><p> 当一个 Block 传输完成之后（一个 Block 由多个 Packet 组成），Block 传输通道关闭，所有的 DataNode 都会向 NameNode 汇报当前 Block 已经传输完毕以及 Block 的其它相关信息，如 BlockId、offset 偏移量等等。</p>
</li>
<li><p> 第一个块传输完成后，第二个块开始传输，依次重复 3~9 步，直到最后一个块传输完成，NameNode 向客户端响应传输完成，客户端关闭输出流。</p>
</li>
</ol>
<hr>
<h3 id="异常写流程"><a href="#异常写流程" class="headerlink" title="异常写流程"></a>异常写流程</h3><ol>
<li> <code>1~6</code> 同上；</li>
<li> 客户端每读取 64K 的数据，封装成一个 packet，封装成功的 packet，放入到一个队列中，这个队列称为 <strong>dataQuene(待发送数据包队列)</strong></li>
<li>在发送时，会先将 dataQuene 中的 packet 按顺序发送，发送后再放入到**ackquene(已发送数据包等待ack确认消息队列)**。<ul>
<li>  每个 DataNode 节点在收到 packet 后，都会向客户端发送 ack 确认消息；</li>
<li>  如果一个 packet 在发送后，已经收到了所有 DataNode 返回的 ack 确认消息，这个 packet 会在 ackquene 中删除；</li>
<li>  假如一个 packet 在发送后，在等待 DataNode 的 ack 确认消息时超时，传输就会中止，ackquene 中的所有 packet 都会回滚到 dataQuene。</li>
</ul>
</li>
<li> 重新建立通道：剔除坏的 DataNode 节点，重新建立通道，建立完成之后，继续传输！</li>
<li> 只要有一个 DataNode 节点收到了数据，DataNode 上报 NameNode 已经收完此块，NameNode 就认为当前块已经传输成功了。至于其它的 DataNode 节点可能未收到完整数据，NameNode 会在之后空闲的时候自动维护副本数！</li>
</ol>
<hr>
<h2 id="HDFS-读数据流程"><a href="#HDFS-读数据流程" class="headerlink" title="HDFS 读数据流程"></a>HDFS 读数据流程</h2><p><img src="HDFS/image-20211124210327466.png" alt="image-20211124210327466"></p>
<ol>
<li> 服务端启动 HDFS 中的 NameNode 和 DataNode 进程；</li>
<li> 客户端创建一个分布式文件系统客户端 <code>DistributedFileSystem</code>，由客户端向 NameNode 发送请求，请求下载文件；</li>
<li>NameNode 处理请求，检查客户端是否有权限上传，路径是否合法等<ul>
<li>  权限校验：NameNode 会检查客户端用户是否有权限操作 HDFS 文件系统；</li>
<li>  路径校验：NameNode 会检查待上传的目标文件是否已存在，目标文件的父目录是否存在等；</li>
</ul>
</li>
<li>检查通过后，NameNode 通过查询元数据，找到文件块所在的 DataNode 的地址并响应给客户端。<ul>
<li>  根据机架感知策略，NameNode 返回的 DataNode 列表中，是按照网络拓扑距离离客户端从小到大的顺序排列的，也就是说，列表中的第一个 DataNode 节点距离客户端一定是最近的，之后的每个节点距离客户端越来越远。</li>
</ul>
</li>
<li> 客户端根据返回的 DataNode 节点，挑选一台最近的可以正常运行的 DataNode 服务器，建立连接，请求读取数据。</li>
<li> 客户端以 Packet 为单位接收数据，先将数据写在缓存中，缓存区满后再将数据写入目标文件。</li>
</ol>
<hr>
<h2 id="网络拓扑-节点距离计算"><a href="#网络拓扑-节点距离计算" class="headerlink" title="网络拓扑-节点距离计算"></a>网络拓扑-节点距离计算</h2><p>在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近的 DataNode （也就是距离发起上传的客户端最近的 DataNode）接收数据。那么这个最近距离怎么计算呢？</p>
<p><strong>节点距离：两个节点到达最近的共同祖先的距离总和。</strong>  </p>
<p><img src="HDFS/image-20211125112723407.png" alt="image-20211125112723407"></p>
<h2 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h2><p><strong>Hadoop 2.7.2 默认的机架感知策略：</strong></p>
<ol>
<li> 第一个副本放在本地机架的一个 DN 节点；</li>
<li>第二个副本放在本地机架的另一个 DN 节点；<ul>
<li>  本地机架的网络拓扑距离最多为2，速度快！</li>
</ul>
</li>
<li>第三个副本放在其他机架的一个 DN 节点<ul>
<li>  为了安全性，防止本地机架挂掉。</li>
</ul>
</li>
</ol>
<hr>
<h1 id="NameNode-和-SecondaryNameNode"><a href="#NameNode-和-SecondaryNameNode" class="headerlink" title="NameNode 和 SecondaryNameNode"></a>NameNode 和 SecondaryNameNode</h1><h2 id="NameNode-中元数据的组成"><a href="#NameNode-中元数据的组成" class="headerlink" title="NameNode 中元数据的组成"></a>NameNode 中元数据的组成</h2><ul>
<li>  NameNode 中完整的元数据信息是由<strong>日志文件edits_xxx</strong>和<strong>快照文件fsimage</strong>共同组成的。</li>
</ul>
<p>元数据信息保存在 NameNode 服务器上的指定目录下（这个目录是根据配置文件信息创建的），我自己指定的目录是<code>/opt/module/hadoop-2.7.2/data/dfs/name/</code>，可以看到元数据信息由两部分组成，分别是日志文件 <code>edits_xxx</code> 和快照文件<code>fsimage_xxx</code>。也就是说 NameNode 服务在运行时会将这两部分数据都加载到内存中形成完整的元数据信息。</p>
<p>当客户端向 NameNode 发送读数据请求时，NameNode 会先将磁盘上的元数据信息加载到内存中，在内存中进行检索找到保存真实数据的 DataNode 节点地址返回给客户端。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[lvnengdong@hadoop102 dfs]$ tree /opt/module/hadoop-2.7.2/data/dfs/name/</span><br><span class="line">/opt/module/hadoop-2.7.2/data/dfs/name/</span><br><span class="line">├── current</span><br><span class="line">│   ├── edits_0000000000000000001-0000000000000000007</span><br><span class="line">│   ├── edits_inprogress_0000000000000000008</span><br><span class="line">│   ├── fsimage_0000000000000000000</span><br><span class="line">│   ├── fsimage_0000000000000000000.md5</span><br><span class="line">│   ├── fsimage_0000000000000000007</span><br><span class="line">│   ├── fsimage_0000000000000000007.md5</span><br><span class="line">│   ├── seen_txid</span><br><span class="line">│   └── VERSION</span><br><span class="line">└── in_use.lock</span><br></pre></td></tr></table></figure>

<p>为什么磁盘中的元数据信息要分成两部分呢？这一点其实和 Redis 很相似，首先日志文件中保存了所有对元数据的写操作（行为数据），但是日志文件会随着 NameNode 服务运行时间的增加而不断膨胀，当日志文件的数量（或大小）到达了一个阈值后，会对这些日志文件进行一个压缩，压缩后的日志文件就变成了一个快照文件<code>fsimage</code>，<code>fsimage</code> 是一个当前时刻元数据的一致性快照（快照文件保存的是状态数据），当生成快照文件后，就可以将当前快照之前的所有日志文件清空，重新开始记录新的日志文件。所以在运行 NameNode 进程时，需要加载两种类型的文件才能在内存中拼接成完整的元数据信息：1️⃣某一时刻的快照文件；2️⃣该时刻之后新生成的所有日志文件。</p>
<hr>
<h2 id="NN-和-2NN-工作机制"><a href="#NN-和-2NN-工作机制" class="headerlink" title="NN 和 2NN 工作机制"></a>NN 和 2NN 工作机制</h2><p><img src="HDFS/image-20211125115010253.png" alt="image-20211125115010253"></p>
<p><strong>第一阶段：NameNode启动</strong></p>
<ol>
<li><p> 如果是第一次启动 NameNode 的话，需要执行 NameNode 格式化指令，创建 Fsimage 和 Edits 文件。如果不是第一次启动 NameNode，会直接加载已存在的日志文件和镜像文件到内存中组成完成的元数据信息。</p>
</li>
<li><p> 客户端提交对元数据进行增删改查的请求。</p>
</li>
<li><p> NameNode 会先将写操作记录到日志文件中，更新滚动日志。</p>
</li>
<li><p>NameNode 在内存中对元数据进行增删改。【注意是先将写操作同步到日志文件中，再执行真正的写操作】</p>
<blockquote>
<p>  首先对元数据的写操作肯定是先发生在内存中的，为了防止突然断电导致内存中的数据丢失，NameNode 采用的策略是先将对每个对元数据执行的写操作都同步到日志文件中，然后再去更新内存中的元数据信息，这样在进行断电后数据恢复时，就可以根据日志文件来将元数据到断电前的状态。</p>
</blockquote>
</li>
</ol>
<p><strong>第二阶段：Secondary NameNode工作</strong></p>
<ol>
<li> Secondary NameNode询问 NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。</li>
<li> Secondary NameNode请求执行CheckPoint。</li>
<li> NameNode滚动正在写的Edits日志。</li>
<li> 将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</li>
<li> Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</li>
<li> 生成新的镜像文件fsimage.chkpoint。</li>
<li> 拷贝fsimage.chkpoint到NameNode。</li>
<li> NameNode将fsimage.chkpoint重新命名成fsimage。</li>
</ol>
<hr>
<h2 id="NN和2NN工作机制详解"><a href="#NN和2NN工作机制详解" class="headerlink" title="NN和2NN工作机制详解"></a>NN和2NN工作机制详解</h2><ul>
<li>  Fsimage：NameNode 内存中元数据序列化后形成的文件。  </li>
<li>  Edits：记录客户端更新元数据信息的每一步操作。</li>
</ul>
<ol>
<li> NameNode 启动时，先滚动 Edits 并生成一个空的 <code>edits.inprogress</code>，然后加载 Edits 和 Fsimage 到内存中，此时 NameNode 内存就持有最新的元数据信息。</li>
<li> Client 开始对 NameNode 发送元数据的增删改的请求，这些请求的操作首先会被记录到 <code>edits.inprogress</code> 中（查询元数据的操作不会被记录在 Edits 中，因为查询操作不会更改元数据信息），如果此时 NameNode 挂掉，重启后会从 Edits 中读取元数据的信息。然后，NameNode 会在内存中执行元数据的增删改的操作。  </li>
<li> 由于 Edits 中记录的操作会越来越多，Edits 文件会越来越大，导致 NameNode 在启动加载 Edits 时会很慢，所以需要对 Edits 和Fsimage 进行合并（所谓合并，就是将 Edits 和 Fsimage 加载到内存中，照着 Edits 中的操作一步步执行，最终形成新 Fsimage）。</li>
<li> SecondaryNameNode 的作用就是帮助 NameNode 进行 Edits 和 Fsimage 的合并工作。SecondaryNameNode 首先会询 NameNode是否需要 CheckPoint（触发 CheckPoint 需要满足两个条件中的任意一个，定时时间到l了和 Edits 中数据写满了）。直接带 NameNode是否检查结果。</li>
<li> SecondaryNameNode 执行 CheckPoint 操作，首先会让 NameNode 滚动 Edits 并生成一个空的 <code>edits.inprogress</code>，【滚动 Edits 就是封闭当前的 <code>edits.inprogress</code>，重新生成一个新的  <code>edits.inprogress</code>】滚动 Edits 的目的是给 Edits 打个标记，以后所有新的操作都写入 <code>edits.inprogress</code>，其他未合并的 Edits 和 Fsimage 会拷贝到 SecondaryNameNode 的本地，然后将拷贝的 Edits 和 Fsimage 加载到内存中进行合并，生成 <code>fsimage.chkpoint</code>，然后将 <code>fsimage.chkpoint</code> 拷贝给 NameNode，重命名为 Fsimage 后替换掉原来的 Fsimage。NameNode 在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">启动加载就是在 fsimage 的基础上按照 edits 日志文件重新执行一遍写操作，就能将 NameNode 恢复到上一次停止时的状态。</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="fsimage-和-edits-解析"><a href="#fsimage-和-edits-解析" class="headerlink" title="fsimage 和 edits 解析"></a>fsimage 和 edits 解析</h2><p>第一次格式化NN时，此时会创建NN工作的目录，将在 <code>/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current</code> 目录中产生如下文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fsimage 0000000000000000000</span><br><span class="line">fsimage 0000000000000000000.md5</span><br><span class="line">seen_txid</span><br><span class="line">VERSION</span><br></pre></td></tr></table></figure>

<ol>
<li> <strong>fsimage</strong> 文件：HDFS 文件系统元数据的一个永久性的检查点，其中包含 HDFS 文件系统的所有目录和文件 <code>idnode</code> 的序列化信息。</li>
<li> edits文件：存放 HDFS 文件系统的所有更新操作的文件，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。</li>
<li> <strong>seen_txid</strong>文件：保存的是一个数字，就是最后一个edits_的数字</li>
<li> 每次 NameNode 启动的时候都会将 <code>fsimage</code> 文件读入内存，加载 <code>edits</code> 里面的写操作，保证内存中的元数据信息是最新的、同步的。</li>
</ol>
<p><strong>fsimage文件的产生</strong>：</p>
<pre><code>②当NN在启动时，NN会将所有的edits文件和fsiamge文件加载到内存合并得到最新的元数据，将元数据持久化到磁盘生成新的fsimage文件。
【NameNode在运行期间是不允许将edits文件和fsiamge文件合并成一个新的fsiamge文件的，因为NameNode在HDFS中处于一个非常重要的地位，承担的流量非常大，而合并edits文件和fsiamge文件又是一个非常耗费性能的操作，所以在NameNode运行期间不允许合并这两个文件。只有在每次NameNode刚启动时的阶段，处于一个安全模式，这个模式下不允许客户端对NameNode进行读写操作，整个流量处于一个比较低的阶段，这时候才会进行将edits文件和fsiamge文件合并成一个新的fsiamge文件操作】

③如果启用了2nn,2nn也会辅助NN合并元数据，会将合并后的元数据发送到NN
</code></pre>
<p>edits：<br>NN在启动之后，每次接受的写操作请求，都会将写命令记录到edits文件中，edits文件每间隔一定的时间和大小滚动！</p>
<p>为合并过的Edits中的元数据信息已经被记录在Fsimage中。  </p>
<h3 id="注意点："><a href="#注意点：" class="headerlink" title="注意点："></a>注意点：</h3><blockquote>
<p>  <strong>Q：fsimage 中没有记录 block 所对应 DataNode，为什么？</strong></p>
</blockquote>
<p>A：在集群启动后，要求 DataNode 上报数据块信息，并间隔一段时间后再次上报。</p>
<hr>
<h2 id="CheckPoint-时间设置"><a href="#CheckPoint-时间设置" class="headerlink" title="CheckPoint 时间设置"></a>CheckPoint 时间设置</h2><ol>
<li><p>默认情况下，SecondaryNameNode每隔一小时执行一次。</p>
<ul>
<li>  hdfs-default.xml</li>
</ul>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">&lt;/property &gt;</span><br></pre></td></tr></table></figure>

</li>
<li><p>一分钟检查一次操作次数，当操作次数达到 100 万时，SecondaryNameNode执行一次。</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">&lt;/property &gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h1 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h1><h2 id="DataNode-工作机制"><a href="#DataNode-工作机制" class="headerlink" title="DataNode 工作机制"></a>DataNode 工作机制</h2><p><img src="HDFS/image-20211126180438535.png" alt="image-20211126180438535"></p>
<ol>
<li> 一个数据块在 DataNode 上以文件形式存储在磁盘上，每个 Block 包括两个文件，一个是数据本身，一个是元数据（包括数据块的长度，块数据的校验和，以及时间戳等）。</li>
<li> DataNode 启动需要先将自身的信息注册到 NameNode 上，注册完成后，还需要周期性（1小时）的向 NameNode 上报最新的块信息。</li>
<li> 心跳是每3秒一次，心跳返回结果中还带有 NameNode 发给该 DataNode 的命令，如复制块数据到另一台机器，或删除某个数据块。如果 NameNode 超过 10 分钟没有收到某个 DataNode 的心跳，则认为该 DataNode 节点不可用。</li>
<li> 集群运行中可以安全加入和退出一些机器。</li>
</ol>
<h2 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h2><p>当 DataNode 向 NameNode 上报自身的信息时，会先对当前节点内的所有数据进行一次数据完整性校验。校验流程如下：</p>
<ol>
<li> 当 DataNode 读取 Block 的时候，它会计算 CheckSum。</li>
<li> 如果计算后的 CheckSum，与 Block 创建时值不一样，说明 Block 已经损坏。</li>
<li> Block 损坏后，DataNode 会删除该损坏的 Block，并向 NameNode 上报该信息，这样当 Client 再来读取该 Block 时，NameNode 就会分配 Client 读取其他 DataNode 上的 Block。</li>
<li> 并且 DataNode 在运行时会周期性的验证磁盘上所有块的 CheckSum。</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/02/07/%E5%8F%82%E8%80%83%E5%8D%9A%E5%AE%A2/" rel="prev" title="参考博客">
                  <i class="fa fa-chevron-left"></i> 参考博客
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/02/10/Hadoop/" rel="next" title="Hadoop理论基础">
                  Hadoop理论基础 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  





</body>
</html>
