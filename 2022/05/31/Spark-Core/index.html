<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="第 1 章 RDD 概述1.1 什么是 RDD   RDD；Resilient Distributed Dataset；弹性分布式数据集  数据集：表示 RDD 是一个保存数据的集合 分布式：RDD 中保存的数据可以位于多台不同的服务器上 弹性：可扩展，易转换 RDD 是 Spark 中最基本的数据抽象。 RDD 在代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark_Core">
<meta property="og:url" content="http://example.com/2022/05/31/Spark-Core/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="第 1 章 RDD 概述1.1 什么是 RDD   RDD；Resilient Distributed Dataset；弹性分布式数据集  数据集：表示 RDD 是一个保存数据的集合 分布式：RDD 中保存的数据可以位于多台不同的服务器上 弹性：可扩展，易转换 RDD 是 Spark 中最基本的数据抽象。 RDD 在代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2022/05/31/Spark-Core/Spark-Core/image-20211214122641729.png">
<meta property="og:image" content="http://example.com/2022/05/31/Spark-Core/Spark-Core/image-20211214122828163.png">
<meta property="og:image" content="http://example.com/2022/05/31/Spark-Core/Spark-Core/image-20211215213010832.png">
<meta property="og:image" content="http://example.com/2022/05/31/Spark-Core/Spark-Core/image-20211217102243803.png">
<meta property="og:image" content="http://example.com/2022/05/31/Spark-Core/Spark-Core/image-20211217102251217.png">
<meta property="og:image" content="http://example.com/2022/05/31/Spark-Core/Spark-Core/image-20211217102320709.png">
<meta property="og:image" content="http://example.com/2022/05/31/Spark-Core/Spark-Core/image-20211217104250450.png">
<meta property="og:image" content="http://example.com/2022/05/31/Spark-Core/Spark-Core/image-20211217104447823.png">
<meta property="og:image" content="http://example.com/2022/05/31/Spark-Core/Spark-Core/image-20211217121753477.png">
<meta property="og:image" content="http://example.com/2022/05/31/Spark-Core/Spark-Core/image-20211217124958363.png">
<meta property="og:image" content="http://example.com/2022/05/31/Spark-Core/Spark-Core/image-20211217125526721.png">
<meta property="og:image" content="http://example.com/2022/05/31/Spark-Core/Spark-Core/image-20211217170539985.png">
<meta property="article:published_time" content="2022-05-31T04:13:49.820Z">
<meta property="article:modified_time" content="2022-01-16T05:06:58.959Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/05/31/Spark-Core/Spark-Core/image-20211214122641729.png">


<link rel="canonical" href="http://example.com/2022/05/31/Spark-Core/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2022/05/31/Spark-Core/","path":"2022/05/31/Spark-Core/","title":"Spark_Core"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Spark_Core | Hexo</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hexo</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC-1-%E7%AB%A0-RDD-%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">第 1 章 RDD 概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E4%BB%80%E4%B9%88%E6%98%AF-RDD"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 什么是 RDD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-RDD-%E7%9A%84-5-%E4%B8%AA%E4%B8%BB%E8%A6%81%E5%B1%9E%E6%80%A7%EF%BC%88property%EF%BC%89"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 RDD 的 5 个主要属性（property）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-%E7%90%86%E8%A7%A3-RDD"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 理解 RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-1-RDD-%E7%89%B9%E7%82%B9"><span class="nav-number">1.3.1.</span> <span class="nav-text">1.3.1  RDD 特点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-1-%E5%BC%B9%E6%80%A7"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">1.3.1.1   弹性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-2-%E5%88%86%E5%8C%BA"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">1.3.1.2 分区</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-3-%E5%8F%AA%E8%AF%BB"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">1.3.1.3   只读</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-4-%E4%BE%9D%E8%B5%96%EF%BC%88%E8%A1%80%E7%BC%98%EF%BC%89"><span class="nav-number">1.3.1.4.</span> <span class="nav-text">1.3.1.4   依赖（血缘）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-5-%E7%BC%93%E5%AD%98"><span class="nav-number">1.3.1.5.</span> <span class="nav-text">1.3.1.5   缓存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-6-checkpoint"><span class="nav-number">1.3.1.6.</span> <span class="nav-text">1.3.1.6    checkpoint</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC-2-%E7%AB%A0-RDD-%E7%BC%96%E7%A8%8B"><span class="nav-number">2.</span> <span class="nav-text">第 2 章 RDD 编程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-RDD-%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 RDD 编程模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-RDD-%E7%9A%84%E5%88%9B%E5%BB%BA"><span class="nav-number">2.2.</span> <span class="nav-text">2.2    RDD 的创建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-%E4%BB%8E%E9%9B%86%E5%90%88%E4%B8%AD%E5%88%9B%E5%BB%BA-RDD"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1  从集合中创建 RDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-%E4%BB%8E%E5%A4%96%E9%83%A8%E5%AD%98%E5%82%A8%E5%88%9B%E5%BB%BA-RDD"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2  从外部存储创建 RDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-%E4%BB%8E%E5%85%B6%E5%AE%83-RDD-%E8%BD%AC%E6%8D%A2%E5%BE%97%E5%88%B0%E6%96%B0%E7%9A%84-RDD"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.2.3  从其它 RDD 转换得到新的 RDD</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-RDD-%E7%9A%84%E8%BD%AC%E6%8D%A2%EF%BC%88transformation%EF%BC%89"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 RDD 的转换（transformation）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1-Value-%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.1  Value 类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-map-func"><span class="nav-number">2.3.1.1.</span> <span class="nav-text">1    map(func)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-mapPartitions-func"><span class="nav-number">2.3.1.2.</span> <span class="nav-text">2   mapPartitions(func)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-map-%E5%92%8C-mapPartitions-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">2.3.1.3.</span> <span class="nav-text">3   map() 和 mapPartitions() 的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-mapPartitionsWithIndex-func"><span class="nav-number">2.3.1.4.</span> <span class="nav-text">4   mapPartitionsWithIndex(func)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-flatMap-func"><span class="nav-number">2.3.1.5.</span> <span class="nav-text">5   flatMap(func)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-glom"><span class="nav-number">2.3.1.6.</span> <span class="nav-text">6    glom()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-groupBy-func"><span class="nav-number">2.3.1.7.</span> <span class="nav-text">7   groupBy(func)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-filter-func"><span class="nav-number">2.3.1.8.</span> <span class="nav-text">8    filter(func)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-sample-withReplacement-fraction-seed"><span class="nav-number">2.3.1.9.</span> <span class="nav-text">9    sample(withReplacement, fraction, seed)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-distinct-numTasks"><span class="nav-number">2.3.1.10.</span> <span class="nav-text">10    distinct([numTasks]))</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-coalesce-numPartitions"><span class="nav-number">2.3.1.11.</span> <span class="nav-text">11    coalesce(numPartitions)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-repartition-numPartitions"><span class="nav-number">2.3.1.12.</span> <span class="nav-text">12  repartition(numPartitions)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-coalasce-%E5%92%8C-repartition-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">2.3.1.13.</span> <span class="nav-text">13  coalasce 和 repartition 的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-sortBy-func-ascending-%EF%BC%8C-numTasks"><span class="nav-number">2.3.1.14.</span> <span class="nav-text">14  sortBy(func,[ascending]，[numTasks])</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-pipe-command-envVars"><span class="nav-number">2.3.1.15.</span> <span class="nav-text">15  pipe(command, [envVars])</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-%E5%8F%8C-Value-%E7%B1%BB%E5%9E%8B%E4%BA%A4%E4%BA%92"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.2  双 Value 类型交互</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-union-otherDataset"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">1   union(otherDataset)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-subtract-otherDataset"><span class="nav-number">2.3.2.1.1.</span> <span class="nav-text">2    subtract (otherDataset)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-intersection-otherDataset"><span class="nav-number">2.3.2.2.</span> <span class="nav-text">3    intersection(otherDataset)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-cartesian-otherDataset"><span class="nav-number">2.3.2.2.1.</span> <span class="nav-text">4    cartesian(otherDataset)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-zip-otherDataset"><span class="nav-number">2.3.2.2.2.</span> <span class="nav-text">5    zip(otherDataset)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-3-Key-Value-%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.3.3.</span> <span class="nav-text">2.3.3    Key-Value 类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-partitionBy"><span class="nav-number">2.3.3.1.</span> <span class="nav-text">1    partitionBy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-groupByKey"><span class="nav-number">2.3.3.2.</span> <span class="nav-text">2    groupByKey()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-reduceByKey-func-numTasks"><span class="nav-number">2.3.3.3.</span> <span class="nav-text">3    reduceByKey(func, [numTasks])</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-reduceByKey-%E5%92%8C-groupByKey-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">2.3.3.4.</span> <span class="nav-text">4    reduceByKey 和 groupByKey 的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-foldByKey"><span class="nav-number">2.3.3.5.</span> <span class="nav-text">5    foldByKey</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-aggregateByKey-zeroValue-seqOp-combOp-numTasks"><span class="nav-number">2.3.3.6.</span> <span class="nav-text">6   aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-combineByKey-C"><span class="nav-number">2.3.3.7.</span> <span class="nav-text">7   combineByKey[C]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-sortByKey"><span class="nav-number">2.3.3.8.</span> <span class="nav-text">8   sortByKey</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-mapValues"><span class="nav-number">2.3.3.9.</span> <span class="nav-text">9   mapValues</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-join-otherDataset-numTasks"><span class="nav-number">2.3.3.10.</span> <span class="nav-text">10  join(otherDataset, [numTasks])</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-cogroup-otherDataset-numTasks"><span class="nav-number">2.3.3.11.</span> <span class="nav-text">11  cogroup(otherDataset, [numTasks])</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-4-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="nav-number">2.3.4.</span> <span class="nav-text">2.3.4  案例实操</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9C%80%E6%B1%82"><span class="nav-number">2.3.4.1.</span> <span class="nav-text">需求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.3.4.2.</span> <span class="nav-text">具体实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-RDD-%E7%9A%84-Action-%E6%93%8D%E4%BD%9C"><span class="nav-number">2.4.</span> <span class="nav-text">2.4    RDD 的 Action 操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-1-reduce-func"><span class="nav-number">2.4.1.</span> <span class="nav-text">2.4.1  reduce(func)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-2-collect"><span class="nav-number">2.4.2.</span> <span class="nav-text">2.4.2  collect</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-3-count"><span class="nav-number">2.4.3.</span> <span class="nav-text">2.4.3  count()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-4-take-n"><span class="nav-number">2.4.4.</span> <span class="nav-text">2.4.4  take(n)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-5-first"><span class="nav-number">2.4.5.</span> <span class="nav-text">2.4.5  first</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-6-takeOrdered-n-ordering"><span class="nav-number">2.4.6.</span> <span class="nav-text">2.4.6  takeOrdered(n, [ordering])</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-7-aggregate"><span class="nav-number">2.4.7.</span> <span class="nav-text">2.4.7    aggregate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-8-fold"><span class="nav-number">2.4.8.</span> <span class="nav-text">2.4.8  fold</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-9-saveAsTextFile-path"><span class="nav-number">2.4.9.</span> <span class="nav-text">2.4.9    saveAsTextFile(path)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-10-saveAsSequenceFile-path"><span class="nav-number">2.4.10.</span> <span class="nav-text">2.4.10    saveAsSequenceFile(path)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-11-saveAsObjectFile-path"><span class="nav-number">2.4.11.</span> <span class="nav-text">2.4.11    saveAsObjectFile(path)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-12-countByKey"><span class="nav-number">2.4.12.</span> <span class="nav-text">2.4.12    countByKey()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-13-foreach-func"><span class="nav-number">2.4.13.</span> <span class="nav-text">2.4.13    foreach(func)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-RDD-%E4%B8%AD%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%A0%E9%80%92"><span class="nav-number">2.5.</span> <span class="nav-text">2.5    RDD 中函数的传递</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-1-%E4%BC%A0%E9%80%92%E5%87%BD%E6%95%B0"><span class="nav-number">2.5.1.</span> <span class="nav-text">2.5.1  传递函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-2-%E4%BC%A0%E9%80%92%E5%8F%98%E9%87%8F"><span class="nav-number">2.5.2.</span> <span class="nav-text">2.5.2  传递变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-3-kryo-%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%86%E6%9E%B6"><span class="nav-number">2.5.3.</span> <span class="nav-text">2.5.3  kryo 序列化框架</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-RDD-%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="nav-number">2.6.</span> <span class="nav-text">2.6    RDD 的依赖关系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-1-%E6%9F%A5%E7%9C%8B-RDD-%E7%9A%84%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB"><span class="nav-number">2.6.1.</span> <span class="nav-text">2.6.1  查看 RDD 的血缘关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-2-%E6%9F%A5%E7%9C%8B-RDD-%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="nav-number">2.6.2.</span> <span class="nav-text">2.6.2  查看 RDD 的依赖关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-3-%E7%AA%84%E4%BE%9D%E8%B5%96"><span class="nav-number">2.6.3.</span> <span class="nav-text">2.6.3  窄依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-4-%E5%AE%BD%E4%BE%9D%E8%B5%96"><span class="nav-number">2.6.4.</span> <span class="nav-text">2.6.4  宽依赖</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-7-Spark-Job-%E7%9A%84%E5%88%92%E5%88%86"><span class="nav-number">2.7.</span> <span class="nav-text">2.7    Spark Job 的划分</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-1-DAG-Directed-Acyclic-Graph-%E6%9C%89%E5%90%91%E6%97%A0%E7%8E%AF%E5%9B%BE"><span class="nav-number">2.7.1.</span> <span class="nav-text">2.7.1    DAG(Directed Acyclic Graph) 有向无环图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-2-Jobs"><span class="nav-number">2.7.2.</span> <span class="nav-text">2.7.2    Jobs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-3-stages"><span class="nav-number">2.7.3.</span> <span class="nav-text">2.7.3    stages</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-4-Tasks"><span class="nav-number">2.7.4.</span> <span class="nav-text">2.7.4  Tasks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-8-RDD-%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">2.8.</span> <span class="nav-text">2.8    RDD 的持久化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-1-%E5%85%88%E7%9C%8B%E4%B8%80%E6%AE%B5%E4%BB%A3%E7%A0%81"><span class="nav-number">2.8.1.</span> <span class="nav-text">2.8.1  先看一段代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-2-RDD-%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">2.8.2.</span> <span class="nav-text">2.8.2    RDD 数据的持久化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-9-%E8%AE%BE%E7%BD%AE%E6%A3%80%E6%9F%A5%E7%82%B9"><span class="nav-number">2.9.</span> <span class="nav-text">2.9    设置检查点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%81%E4%B9%85%E5%8C%96%E5%92%8Ccheckpoint%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">2.9.1.</span> <span class="nav-text">持久化和checkpoint的区别</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC-3-%E7%AB%A0-Key-Value-%E7%B1%BB%E5%9E%8B-RDD-%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%86%E5%8C%BA%E5%99%A8"><span class="nav-number">3.</span> <span class="nav-text">第 3 章 Key-Value 类型 RDD 的数据分区器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E6%9F%A5%E7%9C%8B-RDD-%E7%9A%84%E5%88%86%E5%8C%BA"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 查看 RDD 的分区</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-value-RDD-%E7%9A%84%E5%88%86%E5%8C%BA%E5%99%A8"><span class="nav-number">3.1.1.</span> <span class="nav-text">1.   value RDD 的分区器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-key-value-RDD-%E7%9A%84%E5%88%86%E5%8C%BA%E5%99%A8"><span class="nav-number">3.1.2.</span> <span class="nav-text">2.   key-value RDD 的分区器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-HashPartitioner"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 HashPartitioner</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-RangePartitioner"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 RangePartitioner</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA%E5%99%A8"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 自定义分区器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MyPartitioner"><span class="nav-number">3.4.1.</span> <span class="nav-text">MyPartitioner</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC-4-%E7%AB%A0-%E6%96%87%E4%BB%B6%E4%B8%AD%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AF%BB%E5%8F%96%E5%92%8C%E4%BF%9D%E5%AD%98"><span class="nav-number">4.</span> <span class="nav-text">第 4 章    文件中数据的读取和保存</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-%E8%AF%BB%E5%86%99-Text-%E6%96%87%E4%BB%B6"><span class="nav-number">4.1.</span> <span class="nav-text">4.1    读写 Text 文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-%E8%AF%BB%E5%8F%96-Json-%E6%96%87%E4%BB%B6"><span class="nav-number">4.2.</span> <span class="nav-text">4.2    读取 Json 文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-%E8%AF%BB%E5%86%99-SequenceFile-%E6%96%87%E4%BB%B6"><span class="nav-number">4.3.</span> <span class="nav-text">4.3    读写 SequenceFile 文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-%E8%AF%BB%E5%86%99-objectFile-%E6%96%87%E4%BB%B6"><span class="nav-number">4.4.</span> <span class="nav-text">4.4    读写 objectFile 文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-5-%E4%BB%8E-HDFS-%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="nav-number">4.5.</span> <span class="nav-text">4.5    从 HDFS 读写文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-6-%E4%BB%8E-Mysql-%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="nav-number">4.6.</span> <span class="nav-text">4.6    从 Mysql 数据读写文件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E5%BC%95%E5%85%A5-MySQL-%E4%BE%9D%E8%B5%96"><span class="nav-number">4.6.1.</span> <span class="nav-text">1、引入 MySQL 依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E4%BB%8E-Mysql-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="nav-number">4.6.2.</span> <span class="nav-text">2、从 Mysql 读取数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E5%90%91-Mysql-%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="nav-number">4.6.3.</span> <span class="nav-text">3、向 Mysql 写入数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-7-%E4%BB%8E-Hbase-%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="nav-number">4.7.</span> <span class="nav-text">4.7    从 Hbase 读写文件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E5%AF%BC%E5%85%A5%E4%BE%9D%E8%B5%96"><span class="nav-number">4.7.1.</span> <span class="nav-text">1、导入依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E4%BB%8E-HBase-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="nav-number">4.7.2.</span> <span class="nav-text">2、从 HBase 读取数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E5%90%91-HBase-%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="nav-number">4.7.3.</span> <span class="nav-text">3、向 HBase 写入数据</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC-5-%E7%AB%A0-RDD-%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6"><span class="nav-number">5.</span> <span class="nav-text">第 5 章 RDD 编程进阶</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F%E9%97%AE%E9%A2%98"><span class="nav-number">5.1.</span> <span class="nav-text">5.1共享变量问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2%E7%B4%AF%E5%8A%A0%E5%99%A8-Accumulator"><span class="nav-number">5.1.1.</span> <span class="nav-text">5.2累加器(Accumulator)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E7%BD%AE%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">5.1.1.1.</span> <span class="nav-text">内置累加器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">5.1.1.2.</span> <span class="nav-text">自定义累加器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="nav-number">5.1.2.</span> <span class="nav-text">5.3广播变量</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">224</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/31/Spark-Core/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Spark_Core | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark_Core
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-31 12:13:49" itemprop="dateCreated datePublished" datetime="2022-05-31T12:13:49+08:00">2022-05-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-01-16 13:06:58" itemprop="dateModified" datetime="2022-01-16T13:06:58+08:00">2022-01-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="第-1-章-RDD-概述"><a href="#第-1-章-RDD-概述" class="headerlink" title="第 1 章 RDD 概述"></a>第 1 章 RDD 概述</h1><h2 id="1-1-什么是-RDD"><a href="#1-1-什么是-RDD" class="headerlink" title="1.1 什么是 RDD"></a>1.1 什么是 RDD</h2><blockquote>
<p>  RDD；Resilient Distributed Dataset；弹性分布式数据集</p>
</blockquote>
<p><strong>数据集</strong>：表示 RDD 是一个保存数据的集合</p>
<p><strong>分布式</strong>：RDD 中保存的数据可以位于多台不同的服务器上</p>
<p><strong>弹性：</strong>可扩展，易转换</p>
<p>RDD 是 Spark 中最基本的数据抽象。</p>
<p>RDD 在代码中是一个<strong>抽象类</strong>，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。</p>
<ul>
<li>  可并行计算：即可以使用多个线程处理同一个 RDD 中的数据</li>
</ul>
<hr>
<h2 id="1-2-RDD-的-5-个主要属性（property）"><a href="#1-2-RDD-的-5-个主要属性（property）" class="headerlink" title="1.2 RDD 的 5 个主要属性（property）"></a>1.2 RDD 的 5 个主要属性（property）</h2><ol>
<li><p><strong>A list of partitions</strong>：一个分区列表</p>
<p> RDD 是一个数据集，并且支持并行运算（多线程同时处理该数据集中的数据）。在并行运算时，需要对 RDD 数据集进行<strong>分区</strong>，每个线程负责执行一个分区的运算。记录每个分区元数据信息的列表就叫做<strong>分区列表</strong>。</p>
<ul>
<li>  一个 RDD 数据集在运行时会在逻辑上划分为多个分区。</li>
<li>  每个分区都会分配一个线程来执行数据的运算，分区数决定了并行数。</li>
<li>  用户可以在创建 RDD 对象时显式指定分区数目，如果未指定则采用默认值，默认值就是程序所分配到的 CPU Core 的数目。</li>
<li>  每个 <code>Partition</code> 存储的分配是由 <code>BlockManager</code> 决定的，每个分区都会被逻辑映射成 <code>BlockManager</code> 的一个 <code>Block</code>，而这个 <code>Block</code> 会被一个 <code>Task</code> 负责计算。</li>
</ul>
</li>
<li><p><strong>A function for computing each split</strong>：一个计算切片的函数</p>
<ul>
<li><p>  切片就是切分 RDD 数据集，切分后的数据保存在分区中。在 Spark 中，切片和分区可以视为等价的。</p>
</li>
<li><p>  切片函数就是定义切片规则的函数。</p>
</li>
<li><p>  Spark 中 RDD 的计算是以 <code>split</code> 为单位的，每个 RDD 都会实现 <code>compute()</code> 方法以达到这个目的。</p>
</li>
</ul>
</li>
</ol>
<p>​    </p>
<ol start="3">
<li><p><strong>A list of dependencies on other RDDs</strong>：保存当前 RDD 与其它 RDD 之间依赖关系的列表。</p>
<ul>
<li><p>  一个 RDD 可能会依赖多个 RDD，每个 RDD 都会维护一张保存自身所有依赖关系的列表。</p>
</li>
<li><p>RDD 是不可变的，每次转换都会生成一个新的 RDD。所以 RDD 之间会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时， Spark 可以通过这个依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算。</p>
</li>
</ul>
</li>
<li><p><strong>Optionally，a Partitioner for key-value RDDs</strong> （e.g. to say that the RDD is hash-partitioned）</p>
<ul>
<li>  可选的。只有对于 <code>key-value</code> 结构的 RDD，才需要设置分区器<code>(Partitioner)</code>，对于非 <code>key-value</code> 结构的 RDD，其 <code>Partitioner</code> 的值是 <code>None</code>。</li>
<li>  <code>Partitiner</code> 不但决定了 RDD 的本区数量，也决定了 <code>parent RDD Shuffle</code> 输出时的分区数量。</li>
</ul>
</li>
<li><p><strong>Optionally，a list of preferred locations to compute each split on （e.g. block locations for an HDFS file）</strong></p>
<ul>
<li>  可选的。存储每个切片优先（preferred location）位置的列表。比如对于一个 HDFS 文件来说，这个列表保存的就是每个 <code>Partition</code> 所在文件块的位置。按照“移动数据不如移动计算”的理念， Spark 在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="1-3-理解-RDD"><a href="#1-3-理解-RDD" class="headerlink" title="1.3 理解 RDD"></a>1.3 理解 RDD</h2><p>一个 RDD 对象可以简单的理解为一个存储分布式数据的集合。</p>
<p>RDD 是一个只读数据集，对 RDD 进行改动，只能通过 RDD 的转换操作，然后得到新的 RDD，并不会对原 RDD 有任何的影响。</p>
<p>在 Spark 中，所有的工作要么是创建 RDD，要么是转换已经存在 RDD 成为新的 RDD，要么在 RDD 上去执行一些操作来得到一些计算结果。</p>
<p>每个 RDD 被切分到多个 <code>partition</code> 中去，每个 <code>partition</code> 可能会在集群中不同的节点上进行计算。</p>
<h3 id="1-3-1-RDD-特点"><a href="#1-3-1-RDD-特点" class="headerlink" title="1.3.1  RDD 特点"></a>1.3.1  RDD 特点</h3><h4 id="1-3-1-1-弹性"><a href="#1-3-1-1-弹性" class="headerlink" title="1.3.1.1   弹性"></a>1.3.1.1   弹性</h4><ul>
<li><p>存储的弹性：内存与磁盘的自动切换；</p>
<p>  【在进行内存运算时如果内存不够用了，就会切换到磁盘运算。反之亦然】</p>
</li>
<li><p>容错的弹性：数据丢失可以自动恢复；</p>
</li>
<li><p>计算的弹性：计算出错重试机制；</p>
</li>
<li><p>分片的弹性：可根据需要重新分片。</p>
</li>
</ul>
<hr>
<h4 id="1-3-1-2-分区"><a href="#1-3-1-2-分区" class="headerlink" title="1.3.1.2 分区"></a>1.3.1.2 分区</h4><p>RDD 逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个 <code>compute()</code> 函数得到每个分区的数据。</p>
<p>如果 RDD 是通过已有的文件系统构建，则 <code>compute()</code> 函数是读取指定文件系统中的数据，如果 RDD 是通过其它 RDD 转换而来，则 <code>compute()</code> 函数是执行转换逻辑将其它 RDD 的数据进行转换。</p>
<hr>
<h4 id="1-3-1-3-只读"><a href="#1-3-1-3-只读" class="headerlink" title="1.3.1.3   只读"></a>1.3.1.3   只读</h4><p>RDD 是只读的，要想改变 RDD 中的数据，只能在现有 RDD 基础上创建新的 RDD。</p>
<p>由一个 RDD 转换到另一个 RDD，可以通过丰富的转换算子实现，不再像 <code>MapReduce</code> 那样只能写 <code>map</code> 和 <code>reduce</code> 了。</p>
<p>RDD 的操作算子包括两类，</p>
<ul>
<li>一类叫做 <code>transformation</code>，它是用来对 RDD 进行转化，构建 RDD 的血缘关系；</li>
<li>另一类叫做 <code>action</code>，它是用来触发 RDD 进行计算，得到 RDD 的相关计算结果或者保存 RDD 数据到文件系统中。</li>
</ul>
<hr>
<h4 id="1-3-1-4-依赖（血缘）"><a href="#1-3-1-4-依赖（血缘）" class="headerlink" title="1.3.1.4   依赖（血缘）"></a>1.3.1.4   依赖（血缘）</h4><p>RDDs 通过操作算子进行转换，转换得到的新 RDD 包含了从其它 RDDs 衍生所必需的信息，RDDs 之间维护着这种血缘关系，也称之为依赖。</p>
<p>如下图所示，依赖包括两种，</p>
<ul>
<li><strong>窄依赖</strong>：上游 RDD 的一个分区只与下游 RDD 的一个分区有关，是一对一或多对一关系。</li>
<li><strong>宽依赖</strong>：上游 RDD 的一个分区与下游 RDD 的多个分区都有关，是一对多的关系。</li>
</ul>
<img src="Spark-Core/image-20211214122641729.png" alt="image-20211214122641729" style="zoom:150%;" />





<hr>
<h4 id="1-3-1-5-缓存"><a href="#1-3-1-5-缓存" class="headerlink" title="1.3.1.5   缓存"></a>1.3.1.5   缓存</h4><p>如果在应用程序中多次使用到了同一个 RDD，可以将该 RDD 缓存起来，该 RDD 只有在第一次计算的时候会根据血缘关系依次计算得到分区的数据，在后续其它地方用到该 RDD 的时候，会直接从缓存中取而不用再根据血缘关系重头计算，这样可以加速后期的重用。</p>
<p>如下图所示，RDD-1 经过一系列的转换后得到 RDD-n 并保存到 HDFS，RDD-1 在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的 RDD-1 转换到 RDD-m 这一过程中，就不会计算其之前的 RDD-0 了。</p>
<img src="Spark-Core/image-20211214122828163.png" alt="image-20211214122828163" style="zoom：150%;" />



<hr>
<h4 id="1-3-1-6-checkpoint"><a href="#1-3-1-6-checkpoint" class="headerlink" title="1.3.1.6    checkpoint"></a>1.3.1.6    checkpoint</h4><p>虽然 RDD 的血缘关系天然地可以实现容错，当 RDD 的某个分区数据计算失败或丢失，可以通过血缘关系重建。</p>
<p>但是对于长时间迭代型应用来说，随着迭代的进行，RDDs 之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。</p>
<p>为此，RDD 支持 <code>checkpoint</code> 将数据持久化存储，这样就可以切断 <code>checkpoint</code> 点之前的血缘关系，在 <code>checkpoint</code> 后的 RDD 不需要知道它的父 RDDs 了，它可以直接从 <code>checkpoint</code> 处拿到数据。</p>
<hr>
<h1 id="第-2-章-RDD-编程"><a href="#第-2-章-RDD-编程" class="headerlink" title="第 2 章 RDD 编程"></a>第 2 章 RDD 编程</h1><h2 id="2-1-RDD-编程模型"><a href="#2-1-RDD-编程模型" class="headerlink" title="2.1 RDD 编程模型"></a>2.1 RDD 编程模型</h2><p>在 Spark 中，RDD 可视为一个对象，通过调用对象的方法来对 RDD 进行转换。</p>
<p>RDD 经过一系列的 <code>transformations</code> 转换操作之后，就可以调用 <code>actions</code> 触发 RDD 的计算。</p>
<p><code>action</code> 可以向应用程序返回结果（<code>count</code>，<code>collect</code>等），也可以向存储系统保存数据（<code>saveAsTextFile</code>等）。</p>
<p>在 Spark 中，只有遇到 <code>action</code> 后，才会执行 RDD 的计算（即延迟计算），这样在运行时可以通过管道的方式传输多个转换。</p>
<p>要使用 Spark，开发者需要编写一个 <code>Driver</code> 程序，它最终会被提交到集群上来调度运行 <code>Worker</code></p>
<p><code>Driver</code> 中定义了一个或多个 RDD，并调用 RDD 上的 <code>action</code>，<code>Worker</code> 则执行 RDD 分区计算任务。</p>
<hr>
<h2 id="2-2-RDD-的创建"><a href="#2-2-RDD-的创建" class="headerlink" title="2.2    RDD 的创建"></a>2.2    RDD 的创建</h2><p>在 Spark 中创建 RDD 的方式可以分为 2 种：</p>
<ol>
<li>从数据源得到 RDD<ul>
<li>  外部数据源：文件、数据库等</li>
<li>  集合数据源</li>
</ul>
</li>
<li> 从其它 RDD 转换得到新的 RDD</li>
</ol>
<hr>
<h3 id="2-2-1-从集合中创建-RDD"><a href="#2-2-1-从集合中创建-RDD" class="headerlink" title="2.2.1  从集合中创建 RDD"></a>2.2.1  从集合中创建 RDD</h3><ol>
<li> <code>parallelize()</code>方法</li>
<li> <code>makeRDD()</code> 方法</li>
</ol>
<p><strong>源码</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment">  参数1：seq，传递一个Scala集合，作为RDD的数据源。</span></span><br><span class="line"><span class="comment">  Distribute a local Scala collection to form an RDD.</span></span><br><span class="line"><span class="comment">  </span></span><br><span class="line"><span class="comment">  参数2：numSlices，传递一个整型变量，表示以几个CPU核心执行该方法。默认使用SparkContext指定的核心数</span></span><br><span class="line"><span class="comment">  This method is identical to parallelize.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],	</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    <span class="comment">// makeRDD 底层调用的还是 parallelize 方法，所以这两个方法是等价的</span></span><br><span class="line">    parallelize(seq, numSlices)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>使用 <code>parallelize()</code> 函数创建 RDD</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1、准备数据源</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">50</span>,<span class="number">60</span>)</span><br><span class="line">arr： <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">10</span>，<span class="number">20</span>，<span class="number">30</span>，<span class="number">40</span>，<span class="number">50</span>，<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2、创建 RDD 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(arr)</span><br><span class="line">rdd1： org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;：<span class="number">26</span></span><br></pre></td></tr></table></figure>



<p><strong>使用 <code>makeRDD()</code> 函数创建 RDD</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>(<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">50</span>,<span class="number">60</span>))</span><br><span class="line">rdd1： org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at &lt;console&gt;：<span class="number">24</span></span><br></pre></td></tr></table></figure>



<p><strong>说明：</strong></p>
<ul>
<li>  一旦 RDD 创建成功，就可以通过并行的方式去操作这个分布式的数据集了。</li>
<li>  <code>parallelize()</code> 和 <code>makeRDD()</code> 还有一个重要的参数就是把数据集切分成的<strong>分区数</strong>。</li>
<li>  Spark 会为每个分区运行一个 Task 线程。默认情况下，Spark 会自动的根据你的集群来设置分区数。</li>
</ul>
<hr>
<h3 id="2-2-2-从外部存储创建-RDD"><a href="#2-2-2-从外部存储创建-RDD" class="headerlink" title="2.2.2  从外部存储创建 RDD"></a>2.2.2  从外部存储创建 RDD</h3><p>Spark 也可以从任意 Hadoop 支持的存储数据源来创建分布式数据集。可以是<strong>本地文件系统、HDFS、Cassandra、HBase、Amazon S3</strong> 等等。</p>
<p>Spark 支持<strong>文本文件、SequenceFiles</strong>、和其它<strong>所有的 Hadoop InputFormat</strong> 。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd2 = sc.textFile（<span class="string">&quot;words.txt&quot;</span>）</span><br><span class="line">rdd2： org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = words.txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;：<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; distFile.collect</span><br><span class="line">res0： <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>（atguigu hello，hello world，how are you，abc efg）</span><br></pre></td></tr></table></figure>



<p><strong>说明：</strong></p>
<ul>
<li><p>  URL 可以是：<code>本地文件系统文件</code>，<code>hdfs：//...</code>，<code>s3n：//...</code> 等等</p>
</li>
<li><p>  如果使用的是本地文件系统的路径，则集群中的每个节点上都必须存在这个路径</p>
</li>
<li><p>所有基于文件的方法，都支持目录，压缩文件，和通配符（*）。例如： </p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">textFile(<span class="string">&quot;/my/directory&quot;</span>)</span><br><span class="line">textFile(<span class="string">&quot;/my/directory/.txt&quot;</span>) </span><br><span class="line">textFile(<span class="string">&quot;/my/directory/.gz&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>  <code>textFile()</code> 方法还可以有第二个参数，表示分区数。默认情况下，每个块对应一个分区（对 HDFS 来说，块大小默认是 128M），可以传递一个大于块数的分区数，但是不能传递一个比块数小的分区数。</p>
</li>
<li><p>  关于读取文件和保存文件的其它知识，后面专门的章节介绍。</p>
</li>
</ul>
<hr>
<h3 id="2-2-3-从其它-RDD-转换得到新的-RDD"><a href="#2-2-3-从其它-RDD-转换得到新的-RDD" class="headerlink" title="2.2.3  从其它 RDD 转换得到新的 RDD"></a>2.2.3  从其它 RDD 转换得到新的 RDD</h3><p>就是通过 RDD 的各种转换算子来得到新的 RDD.</p>
<hr>
<h2 id="2-3-RDD-的转换（transformation）"><a href="#2-3-RDD-的转换（transformation）" class="headerlink" title="2.3 RDD 的转换（transformation）"></a>2.3 RDD 的转换（transformation）</h2><p>在 RDD 上支持 2 种类型的操作：</p>
<ol>
<li> <strong>transformation</strong>：由一个已知的 RDD 转换得到一个新的 RDD。</li>
<li> <strong>action</strong>：在数据集上计算结束之后，给驱动程序 Driver 返回一个值。</li>
</ol>
<p>在 Spark 中几乎所有的 <code>transformation</code> 操作都是懒执行的，也就是说 <code>transformation</code> 操作并不会立即计算它们的结果，而是记住了这个操作，只有当通过一个 <code>action</code> 来获取结果返回给驱动程序的时候这些转换操作才开始执行。这种设计可以使 Spark 运行起来更加的高效。</p>
<p>默认情况下，你每次在一个 RDD 上运行一个 <code>action</code> 的时候，前面的每个 <code>transformed RDD</code> 都会被重新计算。</p>
<p>但是我们可以通过 <code>persist (or cache)</code> 方法来持久化一个 RDD 道内存中或磁盘上，来加快访问速度。 </p>
<p>根据 RDD 中数据类型的不同，整体分为 2 种 RDD：</p>
<ul>
<li>  <strong>Value 类型</strong></li>
<li>  <strong>Key-Value 类型</strong></li>
</ul>
<hr>
<h3 id="2-3-1-Value-类型"><a href="#2-3-1-Value-类型" class="headerlink" title="2.3.1  Value 类型"></a>2.3.1  Value 类型</h3><h4 id="1-map-func"><a href="#1-map-func" class="headerlink" title="1    map(func)"></a>1    <code>map(func)</code></h4><p><strong>作用：</strong>返回一个新的 RDD，该 RDD 是由原 RDD 中的每个元素经过函数转换后的值组成，就是对 RDD 中的每个数据做转换。</p>
<p><strong>案例：</strong>创建一个包含 1~10 的 RDD，然后将 <code>每个元素*2</code> 形成新的 RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala &gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd1： org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;：<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 得到一个新的 RDD，但是这个 RDD 中的元素并不是立即计算出来的</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = rdd1.map(_ * <span class="number">2</span>)</span><br><span class="line">rdd2： org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at map at &lt;console&gt;：<span class="number">26</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 开始计算 rdd2 中的元素，并把计算后的结果传递给 Driver 展示</span></span><br><span class="line">scala&gt; rdd2.collect</span><br><span class="line">res0： <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>，<span class="number">4</span>，<span class="number">6</span>，<span class="number">8</span>，<span class="number">10</span>，<span class="number">12</span>，<span class="number">14</span>，<span class="number">16</span>，<span class="number">18</span>，<span class="number">20</span>)</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="2-mapPartitions-func"><a href="#2-mapPartitions-func" class="headerlink" title="2   mapPartitions(func)"></a>2   <code>mapPartitions(func)</code></h4><p><strong>源码：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment">  参数1：f是一个函数，f函数的作用就是把一个集合转换成另一个集合。</span></span><br><span class="line"><span class="comment">  Iterator是一个迭代器，可以看做是一个集合，表示一个分区内所有的元素，</span></span><br><span class="line"><span class="comment">  Iterator[T] =&gt; Iterator[U] 表示处理 Iterator[T] 中的所有元素并将结</span></span><br><span class="line"><span class="comment">  果封装到另一个集合Iterator[U]中返回，</span></span><br><span class="line"><span class="comment">  </span></span><br><span class="line"><span class="comment">  参数2：preservesPartitioning，分区数量相关</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">      preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>作用：</strong>类似于 <code>map(func)</code>，但是 <code>func</code> 的类型是：<code>Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;</code>，表示每次处理一个集合中的数据到另一个集合中，在 Spark 中每个分区的数据保存在一个集合中， 所以 <code>func</code> 在每个分区上只运行一次。</p>
<p>假设有 N 个元素，有 M 个分区，那么 <code>map</code> 函数将会被调用 N 次，而 <code>mapPartitions</code> 只会被调用 M 次，<code>func</code>函数一次处理一个分区内的所有数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> source = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">source： org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">5</span>] at parallelize at &lt;console&gt;：<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; source.mapPartitions(it =&gt; it.map(x =&gt; x * <span class="number">2</span>))	<span class="comment">// 这里的 it 就表示某一个分区内的数据，保存在一个集合中</span></span><br><span class="line">res7： org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at mapPartitions at &lt;console&gt;：<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; res7.collect</span><br><span class="line">res8： <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>（<span class="number">2</span>，<span class="number">4</span>，<span class="number">6</span>，<span class="number">8</span>，<span class="number">10</span>，<span class="number">12</span>，<span class="number">14</span>，<span class="number">16</span>，<span class="number">18</span>，<span class="number">20</span>）</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="3-map-和-mapPartitions-的区别"><a href="#3-map-和-mapPartitions-的区别" class="headerlink" title="3   map() 和 mapPartitions() 的区别"></a>3   <code>map()</code> 和 <code>mapPartitions()</code> 的区别</h4><ol>
<li> <code>map(func)</code>：处理一个集合中的数据，集合中的每个元素都需要执行一次 <code>func</code> 函数。</li>
<li> <code>mapPartitions(func)</code>：也是处理一个集合中的数据，但是该集合按照数据存储的不同分区又被划分为了 M 个子集合，M 等于分区数，每个子集合保存一个分区内的所有数据，每执行一次 <code>func</code> 函数就处理一个分区中的数据。也就是说如果集合数据被分到三个分区中存储，那么 <code>func</code> 只需要执行三次，每次处理一个分区的数据。</li>
<li> 当内存空间较大的时候建议使用 <code>mapPartitions()</code>，以提高处理效率。</li>
</ol>
<hr>
<h4 id="4-mapPartitionsWithIndex-func"><a href="#4-mapPartitionsWithIndex-func" class="headerlink" title="4   mapPartitionsWithIndex(func)"></a>4   <code>mapPartitionsWithIndex(func)</code></h4><p><strong>作用：</strong> <code>mapPartitionsWithIndex(func)</code> 和 <code>mapPartitions(func)</code> 类似，但是会给 <code>func</code> 多提供一个 <code>Int</code> 值来表示分区编号，<code>index</code> 的含义就是数据所在分区的编号。所以 <code>func</code> 的类型是：<code>(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">50</span>,<span class="number">60</span>))</span><br><span class="line">rdd1： org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;：<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.mapPartitionsWithIndex( (index，it) =&gt; it.map((index, _)) )</span><br><span class="line">res8： org.apache.spark.rdd.<span class="type">RDD</span>[（<span class="type">Int</span>，<span class="type">Int</span>）] = <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at mapPartitionsWithIndex at &lt;console&gt;：<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; res8.collect</span><br><span class="line">res9： <span class="type">Array</span>[(<span class="type">Int</span>，<span class="type">Int</span>)] = <span class="type">Array</span>( (<span class="number">0</span>,<span class="number">10</span>)，(<span class="number">0</span>,<span class="number">20</span>)，(<span class="number">0</span>,<span class="number">30</span>)，(<span class="number">1</span>,<span class="number">40</span>)，(<span class="number">1</span>,<span class="number">50</span>)，(<span class="number">1</span>,<span class="number">60</span>) )</span><br></pre></td></tr></table></figure>



<p><strong>分区数的确定，和对数组中的元素如何进行分区</strong></p>
<ol>
<li><p><strong>确定分区数：</strong></p>
<p> 读取配置文件中的 <code>spark.default.parallelism</code> 属性，如果未读取到该属性，就使用 <code>totalCores</code>，这个值是在初始化 <code>SparkContext</code> 时确定的核心数。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1、先读取 spark.default.parallelism 属性</span></span><br><span class="line"><span class="comment">// 2、若未读取到，则使用 totalCores</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">defaultParallelism</span></span>(): <span class="type">Int</span> = scheduler.conf.getInt(<span class="string">&quot;spark.default.parallelism&quot;</span>, totalCores)</span><br></pre></td></tr></table></figure>

</li>
<li><p>  <strong>对元素进行分区</strong></p>
</li>
</ol>
<pre><code><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment">	length： RDD 中数据的长度 </span></span><br><span class="line"><span class="comment">	numSlices： 分区数</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positions</span></span>(length: <span class="type">Long</span>, numSlices: <span class="type">Int</span>)： <span class="type">Iterator</span>[(<span class="type">Int</span>，<span class="type">Int</span>)] = &#123;</span><br><span class="line">    (<span class="number">0</span> until numSlices).iterator.map &#123; i =&gt;</span><br><span class="line">        <span class="keyword">val</span> start = ((i * length) / numSlices).toInt</span><br><span class="line">        <span class="keyword">val</span> end = (((i + <span class="number">1</span>) * length) / numSlices).toInt</span><br><span class="line">        (start，end)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">seq <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> r： <span class="type">Range</span> =&gt;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> nr： <span class="type">NumericRange</span>[_] =&gt;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    <span class="keyword">val</span> array = seq.toArray <span class="comment">// To prevent O（n^2） operations for List etc</span></span><br><span class="line">    positions(array.length，numSlices).map &#123; <span class="keyword">case</span> (start，end) =&gt;</span><br><span class="line">        array.slice(start，end).toSeq</span><br><span class="line">    &#125;.toSeq</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</code></pre>
<hr>
<h4 id="5-flatMap-func"><a href="#5-flatMap-func" class="headerlink" title="5   flatMap(func)"></a>5   <code>flatMap(func)</code></h4><p><strong>源码：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">	flatMap 方法的参数需要是一个外层集合嵌套内层集合。</span></span><br><span class="line"><span class="comment">	参数1：f: T =&gt; TraversableOnce[U]，T 就表示某一个内层集合，返回值一定也要是一个集合</span></span><br><span class="line"><span class="comment">	最终会把外层集合里的内层集合扁平化</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>作用</strong>：类似于 <code>map</code>，但是每一个输入元素可以被映射为 0 或多个输出元素（所以 <code>func</code> 应该返回一个序列，而不是单一元素 <code>T =&gt; TraversableOnce[U]</code>）</p>
<p><strong>案例：</strong></p>
<p>创建一个元素为 <code>1~5</code> 的 RDD，运用 <code>flatMap</code> 创建一个新的 RDD，新的 RDD 为原 RDD 每个元素的平方和三次方来组成。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">5</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.flatMap(x =&gt; <span class="type">Array</span>(x * x, x * x * x))</span><br><span class="line">res13: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at flatMap at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; res13.collect</span><br><span class="line">res14: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">27</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">25</span>, <span class="number">125</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<hr>
<h4 id="6-glom"><a href="#6-glom" class="headerlink" title="6    glom()"></a>6    <code>glom()</code></h4><p><strong>作用：</strong>将每一个分区内的所有元素合并成一个数组，形成新的 RDD 类型是 <code>RDD[Array[T]]</code></p>
<p><strong>案例：</strong>创建一个 4 个分区的 RDD，并将每个分区的数据放到一个数组</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">50</span>,<span class="number">60</span>), <span class="number">4</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.glom.collect</span><br><span class="line">res2: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">10</span>), <span class="type">Array</span>(<span class="number">20</span>, <span class="number">30</span>), <span class="type">Array</span>(<span class="number">40</span>), <span class="type">Array</span>(<span class="number">50</span>, <span class="number">60</span>))</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="7-groupBy-func"><a href="#7-groupBy-func" class="headerlink" title="7   groupBy(func)"></a>7   <code>groupBy(func)</code></h4><p><strong>作用：</strong><code>func</code> 会对集合中的每个元素进行运算，按照 <code>func</code> 的返回值对元素进行分组。</p>
<p>分组时按照 <code>func</code> 的返回值作为 key，对应的值放入一个迭代器（集合）中。</p>
<p>返回的 RDD 类型：<code>RDD[(K，Iterable[T])]</code></p>
<p>每组内元素的顺序不能保证，并且甚至每次调用得到的顺序也有可能不同。</p>
<p><strong>案例：</strong>创建一个 RDD，按照元素的奇偶性进行分组</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">20</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">8</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">2</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.groupBy(x =&gt; <span class="keyword">if</span>(x % <span class="number">2</span> == <span class="number">1</span>) <span class="string">&quot;odd&quot;</span> <span class="keyword">else</span> <span class="string">&quot;even&quot;</span>)</span><br><span class="line">res4: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">ShuffledRDD</span>[<span class="number">4</span>] at groupBy at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; res4.collect</span><br><span class="line">res5: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">Array</span>((even,<span class="type">CompactBuffer</span>(<span class="number">4</span>, <span class="number">20</span>, <span class="number">4</span>, <span class="number">8</span>))，(odd,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<hr>
<h4 id="8-filter-func"><a href="#8-filter-func" class="headerlink" title="8    filter(func)"></a>8    <code>filter(func)</code></h4><p><strong>作用：</strong>过滤。 返回一个新的 RDD 是由 <code>func</code> 的返回值为 true 的那些元素组成。</p>
<p><strong>案例</strong>：创建一个 RDD（由字符串组成），过滤出一个新 RDD（包含“xiao”子串）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> names = sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;xiaoli&quot;</span>，<span class="string">&quot;laoli&quot;</span>，<span class="string">&quot;laowang&quot;</span>，<span class="string">&quot;xiaocang&quot;</span>，<span class="string">&quot;xiaojing&quot;</span>，<span class="string">&quot;xiaokong&quot;</span>))</span><br><span class="line">names: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; names.filter(_.contains(<span class="string">&quot;xiao&quot;</span>))</span><br><span class="line">res3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at filter at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; res3.collect</span><br><span class="line">res4: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(xiaoli, xiaocang, xiaojing, xiaokong) </span><br></pre></td></tr></table></figure>



<hr>
<h4 id="9-sample-withReplacement-fraction-seed"><a href="#9-sample-withReplacement-fraction-seed" class="headerlink" title="9    sample(withReplacement, fraction, seed)"></a>9    <code>sample(withReplacement, fraction, seed)</code></h4><p><code>sample(withReplacement, fraction, seed)</code></p>
<p><strong>作用：</strong></p>
<ol>
<li>   以指定的随机种子随机抽样出比例为 <code>fraction</code> 的数据，<strong>抽取到的数量在 <code>size * fraction</code> 附近</strong>，需要注意的是得到的结果并不能保证准确的比例。</li>
<li>   <code>withReplacement</code> 表示是抽出的数据是否放回。true 表示有放回的抽样，false 表示无放回的抽样。放回表示数据有可能会被重复抽取到，false 则不可能重复抽取到。如果是 false，则 <code>fraction</code> 的范围必须是 <code>[0,1]</code>，是 true 则 <code>fraction &gt;= 0</code> 就可以了。</li>
<li>   <code>seed</code> 用于指定随机数生成器种子。一般用默认的，或者传入当前的时间戳。</li>
</ol>
<ul>
<li>  <strong>不放回抽样</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">15</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.sample(<span class="literal">false</span>, <span class="number">0.5</span>).collect</span><br><span class="line">res15: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>  <strong>放回抽样</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.sample(<span class="literal">true</span>, <span class="number">2</span>).collect</span><br><span class="line">res25: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="10-distinct-numTasks"><a href="#10-distinct-numTasks" class="headerlink" title="10    distinct([numTasks]))"></a>10    <code>distinct([numTasks]))</code></h4><p><strong>作用：</strong>对 RDD 中元素执行去重操作。参数 <code>numTasks</code> 表示任务的数量，默认值和分区数保持一致。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">10</span>,<span class="number">10</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">9</span>,<span class="number">1</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">28</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.distinct().collect</span><br><span class="line">res29: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">6</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<p>对于自定义类型的对象，如果想要去重，则需要重写对应的 <code>hashCode()</code> 和 <code>equals()</code> 方法。</p>
<hr>
<h4 id="11-coalesce-numPartitions"><a href="#11-coalesce-numPartitions" class="headerlink" title="11    coalesce(numPartitions)"></a>11    <code>coalesce(numPartitions)</code></h4><p><strong>作用：</strong>缩减分区数量。多用于大数据集过滤后，提高小数据集的执行效率。【大数据集过滤后，留下的数据集可以用更少的分区就能够存储了，所以可以缩减分区的数量】</p>
<p><strong>分析：</strong>假如我们从 HDFS 中取出的数据保存在 3 个分区中，这些数据经过数据清洗后，分区1剩余了 100W 条数据，分区2和分区3各剩5W条数据，如果不缩减分区继续进行运算的话，根据木桶效应，整个程序的运算速度一定会被运算速度最慢的分区所拖累，所以我们经常需要适当的缩减分区，来保证各个分区的运算效率尽量相近。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">0</span> to <span class="number">100</span>, <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">45</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查看当前RDD的分区数</span></span><br><span class="line">scala&gt; rdd1.partitions.length	</span><br><span class="line">res39: <span class="type">Int</span> = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 减少分区的数量至 2 </span></span><br><span class="line">scala&gt; rdd1.coalesce(<span class="number">2</span>)</span><br><span class="line">res40: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">CoalescedRDD</span>[<span class="number">46</span>] at coalesce at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; res40.partitions.length</span><br><span class="line">res41: <span class="type">Int</span> = <span class="number">2</span></span><br></pre></td></tr></table></figure>



<p><strong>注意：</strong></p>
<ul>
<li>  第二个参数表示是否 <code>shuffle</code>，如果不传或者传入的为 false，则表示不进行 <code>shuffer</code>，此时分区数减少有效，增加分区数无效。</li>
<li>  如果想要增加分区，那么就必须设置 <code>shuffle</code> 为 <code>true</code></li>
</ul>
<hr>
<h4 id="12-repartition-numPartitions"><a href="#12-repartition-numPartitions" class="headerlink" title="12  repartition(numPartitions)"></a>12  <code>repartition(numPartitions)</code></h4><p><strong>作用：</strong>根据新的分区数，重新 <code>shuffle</code> 所有的数据，这个操作总会通过网络。</p>
<p>新的分区数相比以前可以多，也可以少</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">0</span> to <span class="number">100</span>, <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">45</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.repartition(<span class="number">3</span>)</span><br><span class="line">res44: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">51</span>] at repartition at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; res44.partitions.length</span><br><span class="line">res45: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.repartition(<span class="number">10</span>)</span><br><span class="line">res46: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">55</span>] at repartition at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; res46.partitions.length</span><br><span class="line">res47: <span class="type">Int</span> = <span class="number">10</span></span><br></pre></td></tr></table></figure>





<hr>
<h4 id="13-coalasce-和-repartition-的区别"><a href="#13-coalasce-和-repartition-的区别" class="headerlink" title="13  coalasce 和 repartition 的区别"></a>13  coalasce 和 repartition 的区别</h4><ol>
<li><p>   <code>coalesce</code> 重新分区，可以选择是否进行 <code>shuffle</code> 过程，由参数 <code>shuffle: Boolean = false/true</code> 决定。</p>
</li>
<li><p><code>repartition</code> 底层调用的还是 <code>coalesce</code> 进行 <code>shuffle</code>。源码如下：</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    coalesce(numPartitions, shuffle = <span class="literal">true</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>   如果是减少分区，尽量避免 <code>shuffle</code></p>
</li>
</ol>
<hr>
<h4 id="14-sortBy-func-ascending-，-numTasks"><a href="#14-sortBy-func-ascending-，-numTasks" class="headerlink" title="14  sortBy(func,[ascending]，[numTasks])"></a>14  <code>sortBy(func,[ascending]，[numTasks])</code></h4><p><strong>作用：</strong>使用 <code>func</code> 先对数据进行处理，按照处理后结果排序，默认为正序。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">10</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">9</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">16</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">46</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.sortBy(x =&gt; x).collect</span><br><span class="line">res17: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">16</span>, <span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.sortBy(x =&gt; x, <span class="literal">true</span>).collect</span><br><span class="line">res18: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">16</span>, <span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 不用正序</span></span><br><span class="line">scala&gt; rdd1.sortBy(x =&gt; x, <span class="literal">false</span>).collect</span><br><span class="line">res19: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">30</span>, <span class="number">20</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>





<hr>
<h4 id="15-pipe-command-envVars"><a href="#15-pipe-command-envVars" class="headerlink" title="15  pipe(command, [envVars])"></a>15  <code>pipe(command, [envVars])</code></h4><p><strong>作用：</strong>管道，针对每个分区，把 RDD 中的每个数据通过管道传递给 shell 命令或脚本，返回输出的 RDD。一个分区执行一次这个命令。 如果只有一个分区，则执行一次命令。</p>
<p><strong>注意：</strong>脚本要放在 worker 节点可以访问到的位置</p>
<ol>
<li><p>步骤1：创建一个脚本文件 <code>pipe.sh</code>，文件内容如下：</p>
 <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;hello&quot;</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">read</span> line;<span class="keyword">do</span></span><br><span class="line">	<span class="built_in">echo</span> <span class="string">&quot;&gt;&gt;&gt;&quot;</span><span class="variable">$line</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>

</li>
<li><p>步骤2： 创建只有 1 个分区的RDD</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>)，<span class="number">1</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.pipe(<span class="string">&quot;。/pipe.sh&quot;</span>).collect</span><br><span class="line">res1: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(hello, &gt;&gt;&gt;<span class="number">10</span>, &gt;&gt;&gt;<span class="number">20</span>, &gt;&gt;&gt;<span class="number">30</span>, &gt;&gt;&gt;<span class="number">40</span>)</span><br></pre></td></tr></table></figure>

</li>
<li><p>步骤3： 创建有 2 个分区的 RDD</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>)，<span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">3</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.pipe(<span class="string">&quot;。/pipe.sh&quot;</span>).collect</span><br><span class="line">res2: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(hello, &gt;&gt;&gt;<span class="number">10</span>, &gt;&gt;&gt;<span class="number">20</span>, hello, &gt;&gt;&gt;<span class="number">30</span>, &gt;&gt;&gt;<span class="number">40</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>总结： 每个分区执行一次脚本，但是每个元素算是标准输入中的一行</strong></p>
<hr>
<h3 id="2-3-2-双-Value-类型交互"><a href="#2-3-2-双-Value-类型交互" class="headerlink" title="2.3.2  双 Value 类型交互"></a>2.3.2  双 Value 类型交互</h3><p>这里的<strong>“双 Value 类型交互”</strong>指的是两个 <code>RDD[V]</code> 进行交互。</p>
<h4 id="1-union-otherDataset"><a href="#1-union-otherDataset" class="headerlink" title="1   union(otherDataset)"></a>1   <code>union(otherDataset)</code></h4><p><strong>作用：</strong>求并集。对源 RDD 和参数 RDD 求并集后返回一个新的 RDD</p>
<p><strong>案例</strong>：创建两个 RDD，求并集</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">6</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">2</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">4</span> to <span class="number">10</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">3</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.union(rdd2)</span><br><span class="line">res0: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">UnionRDD</span>[<span class="number">4</span>] at union at &lt;console&gt;:<span class="number">29</span></span><br><span class="line"></span><br><span class="line">scala&gt; res0.collect</span><br><span class="line">res1: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>,  <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong><code>union</code> 和 <code>++</code> 是等价的</p>
<hr>
<h5 id="2-subtract-otherDataset"><a href="#2-subtract-otherDataset" class="headerlink" title="2    subtract (otherDataset)"></a>2    <code>subtract (otherDataset)</code></h5><p><strong>作用：</strong>计算差集。从原 RDD 中减去原 RDD 和 <code>otherDataset</code> 中的共同的部分。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.subtract(rdd2).collect</span><br><span class="line">res4: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.subtract(rdd1).collect</span><br><span class="line">res5: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">8</span>, <span class="number">10</span>, <span class="number">7</span>, <span class="number">9</span>)</span><br></pre></td></tr></table></figure>



<h4 id="3-intersection-otherDataset"><a href="#3-intersection-otherDataset" class="headerlink" title="3    intersection(otherDataset)"></a>3    <code>intersection(otherDataset)</code></h4><p><strong>作用：</strong>计算交集。对源 RDD 和参数 RDD 求交集后返回一个新的 RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.intersection(rdd2).collect</span><br><span class="line">res8: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>



<hr>
<h5 id="4-cartesian-otherDataset"><a href="#4-cartesian-otherDataset" class="headerlink" title="4    cartesian(otherDataset)"></a>4    <code>cartesian(otherDataset)</code></h5><p><strong>作用：</strong>计算 2 个 RDD 的笛卡尔积。尽量避免使用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.cartesian(rdd2).collect</span><br><span class="line">res11: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>)，(<span class="number">1</span>,<span class="number">5</span>)，(<span class="number">1</span>,<span class="number">6</span>)，(<span class="number">2</span>,<span class="number">4</span>)，(<span class="number">2</span>,<span class="number">5</span>)，(<span class="number">2</span>,<span class="number">6</span>)，(<span class="number">3</span>,<span class="number">4</span>)，(<span class="number">3</span>,<span class="number">5</span>)，(<span class="number">3</span>,<span class="number">6</span>)，(<span class="number">1</span>,<span class="number">7</span>)，(<span class="number">1</span>,<span class="number">8</span>)，(<span class="number">1</span>,<span class="number">9</span>)，(<span class="number">1</span>,<span class="number">10</span>)，(<span class="number">2</span>,<span class="number">7</span>)，(<span class="number">2</span>,<span class="number">8</span>)，(<span class="number">2</span>,<span class="number">9</span>)，(<span class="number">2</span>,<span class="number">10</span>)，(<span class="number">3</span>,<span class="number">7</span>)，(<span class="number">3</span>,<span class="number">8</span>)，(<span class="number">3</span>,<span class="number">9</span>)，(<span class="number">3</span>,<span class="number">10</span>)，(<span class="number">4</span>,<span class="number">4</span>)，(<span class="number">4</span>,<span class="number">5</span>)，(<span class="number">4</span>,<span class="number">6</span>)，(<span class="number">5</span>,<span class="number">4</span>)，(<span class="number">5</span>,<span class="number">5</span>)，(<span class="number">5</span>,<span class="number">6</span>)，(<span class="number">6</span>,<span class="number">4</span>)，(<span class="number">6</span>,<span class="number">5</span>)，(<span class="number">6</span>,<span class="number">6</span>)，(<span class="number">4</span>,<span class="number">7</span>)，(<span class="number">4</span>,<span class="number">8</span>)，(<span class="number">4</span>,<span class="number">9</span>)，(<span class="number">4</span>,<span class="number">10</span>)，(<span class="number">5</span>,<span class="number">7</span>)，(<span class="number">5</span>,<span class="number">8</span>)，(<span class="number">5</span>,<span class="number">9</span>)，(<span class="number">5</span>,<span class="number">10</span>)，(<span class="number">6</span>,<span class="number">7</span>)，(<span class="number">6</span>,<span class="number">8</span>)，(<span class="number">6</span>,<span class="number">9</span>)，(<span class="number">6</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>





<hr>
<h5 id="5-zip-otherDataset"><a href="#5-zip-otherDataset" class="headerlink" title="5    zip(otherDataset)"></a>5    <code>zip(otherDataset)</code></h5><p><strong>作用：</strong>拉链操作。</p>
<p><strong>使用的前提条件：</strong></p>
<ol>
<li> 两个 RDD 需要拥有相同的分区数；</li>
<li> 要求每个分区的元素数量相同。</li>
</ol>
<p>在 Spark 中，执行拉链操作时要求两个 RDD 中的分区数和分区内元素的数量和都必须相同，否则会抛出异常（在 scala 中，两个集合的长度可以不同）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">34</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">11</span> to <span class="number">15</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">35</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.zip(rdd2).collect</span><br><span class="line">res17: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">11</span>)，(<span class="number">2</span>,<span class="number">12</span>)，(<span class="number">3</span>,<span class="number">13</span>)，(<span class="number">4</span>,<span class="number">14</span>)，(<span class="number">5</span>,<span class="number">15</span>))</span><br></pre></td></tr></table></figure>

<p><strong>类似算子：</strong><code>zipWithIndex，zipPartitions</code></p>
<hr>
<h3 id="2-3-3-Key-Value-类型"><a href="#2-3-3-Key-Value-类型" class="headerlink" title="2.3.3    Key-Value 类型"></a>2.3.3    Key-Value 类型</h3><p>大多数的 Spark 操作可以用在任意类型的 RDD 上，但是有一些特殊的操作只能用在 <code>key-value</code> 类型的 RDD 上。</p>
<p>这些特殊操作大多都涉及到 <code>shuffle</code> 操作，比如：按照 key 分组<code>(group)</code>，聚集<code>(aggregate)</code>等。</p>
<p>在 Spark 中，这些操作在 <code>K-V</code> 类型的 RDD 上自动可用(通过隐式转换)。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDD</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 通过隐式转换，对于 K-V 类型的 RDD 进行增强</span></span><br><span class="line">    <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">rddToPairRDDFunctions</span></span>[<span class="type">K</span>, <span class="type">V</span>](rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)])</span><br><span class="line">    (<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>], vt: <span class="type">ClassTag</span>[<span class="type">V</span>], ord: <span class="type">Ordering</span>[<span class="type">K</span>] = <span class="literal">null</span>): <span class="type">PairRDDFunctions</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairRDDFunctions</span>(rdd)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>键值对的操作是定义在 <code>PairRDDFunctions</code> 类上，这个类是对 <code>RDD[(K, V)]</code> 的装饰。</p>
<hr>
<h4 id="1-partitionBy"><a href="#1-partitionBy" class="headerlink" title="1    partitionBy"></a>1    <code>partitionBy</code></h4><p><strong>作用：</strong>根据传入的分区器 <code>Partitioner</code>，对 <code>&lt;K,V&gt;</code> 类型的 RDD 数据重新进行分区。</p>
<p><strong>源码</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Partitioner 是一个分区器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionBy</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    <span class="keyword">if</span> (self.partitioner == <span class="type">Some</span>(partitioner)) &#123;</span><br><span class="line">        self</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](self, partitioner)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>  <code>Partitioner</code> 类是分区器的基类，我们自定义的分区器必须继承这个类</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Partitioner</span> <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span>	<span class="comment">// 分区数量</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span>	<span class="comment">// 通过 key 计算出对应的 &lt;K,V&gt; 数据保存在哪一个分区中</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<p><strong>测试</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;c&quot;</span>), (<span class="number">4</span>, <span class="string">&quot;d&quot;</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.partitions.length</span><br><span class="line">res1: <span class="type">Int</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 显式指定分区器</span></span><br><span class="line">scala&gt; rdd1.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">3</span>)).partitions.length</span><br><span class="line">res3: <span class="type">Int</span> = <span class="number">3</span></span><br></pre></td></tr></table></figure>







<hr>
<h4 id="2-groupByKey"><a href="#2-groupByKey" class="headerlink" title="2    groupByKey()"></a>2    <code>groupByKey()</code></h4><p><strong>作用：</strong>按照 key 进行分组。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;hello&quot;</span>, <span class="string">&quot;world&quot;</span>, <span class="string">&quot;atguigu&quot;</span>, <span class="string">&quot;hello&quot;</span>, <span class="string">&quot;are&quot;</span>, <span class="string">&quot;go&quot;</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">2</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = rdd1.map((_, <span class="number">1</span>))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.groupByKey()</span><br><span class="line">res3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">ShuffledRDD</span>[<span class="number">4</span>] at groupByKey at &lt;console&gt;:<span class="number">29</span></span><br><span class="line"></span><br><span class="line">scala&gt; res3.collect</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">Array</span>((are,<span class="type">CompactBuffer</span>(<span class="number">1</span>)), (hello,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">1</span>)), (go,<span class="type">CompactBuffer</span>(<span class="number">1</span>)), (atguigu,<span class="type">CompactBuffer</span>(<span class="number">1</span>)), (world,<span class="type">CompactBuffer</span>(<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; res3.map(t =&gt; (t._1, t._2.sum))</span><br><span class="line">res5: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">5</span>] at map at &lt;console&gt;:<span class="number">31</span></span><br><span class="line">                </span><br><span class="line">scala&gt; res5.collect</span><br><span class="line">res7: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((are,<span class="number">1</span>), (hello,<span class="number">2</span>), (go,<span class="number">1</span>), (atguigu,<span class="number">1</span>), (world,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<ol>
<li>   基于当前的实现，<code>groupByKey</code> 必须在内存中持有所有的键值对，如果一个 key 有太多的 value，则会导致内存溢出(OutOfMemoryError)。</li>
<li>   所以这个操作非常耗资源，如果分组的目的是为了在每个 key 上执行聚合操作（比如 <code>sum</code> 和 <code>average</code>），则应该使用<code>PairRDDFunctions.aggregateByKey</code> 或者 <code>PairRDDFunctions.reduceByKey</code>，因为它们有更好的性能(会先在分区进行预聚合)</li>
</ol>
<hr>
<h4 id="3-reduceByKey-func-numTasks"><a href="#3-reduceByKey-func-numTasks" class="headerlink" title="3    reduceByKey(func, [numTasks])"></a>3    <code>reduceByKey(func, [numTasks])</code></h4><p><strong>作用：</strong>在 <code>&lt;K, V&gt;</code> 格式的 RDD 上调用，返回一个 <code>&lt;K,V&gt;</code> 的 RDD，使用指定的 <code>reduce</code> 函数，将相同 key 的 value 聚合到一起。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">&quot;female&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;male&quot;</span>,<span class="number">5</span>),(<span class="string">&quot;female&quot;</span>,<span class="number">5</span>),(<span class="string">&quot;male&quot;</span>,<span class="number">2</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.reduceByKey(_ + _)</span><br><span class="line">res1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">1</span>] at reduceByKey at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; res1.collect</span><br><span class="line">res2: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((female,<span class="number">6</span>), (male,<span class="number">7</span>))</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="4-reduceByKey-和-groupByKey-的区别"><a href="#4-reduceByKey-和-groupByKey-的区别" class="headerlink" title="4    reduceByKey 和 groupByKey 的区别"></a>4    <code>reduceByKey</code> 和 <code>groupByKey</code> 的区别</h4><ol>
<li>   <code>reduceByKey</code>：按照 key 进行聚合，在 <code>shuffle</code> 之前有 <code>combine</code>（<strong>预聚合</strong>）操作，返回结果是 <code>RDD[k,v]</code>。【预聚合==分区内聚合】</li>
<li>   <code>groupByKey</code>：按照 key 进行分组，直接进行 <code>shuffle</code>。</li>
<li>   开发指导：<code>reduceByKey</code> 比 <code>groupByKey</code> 性能更好，因为 <code>reduceByKey</code> 有预聚合的过程，建议使用。但是需要注意是否会影响业务逻辑。</li>
</ol>
<hr>
<h4 id="5-foldByKey"><a href="#5-foldByKey" class="headerlink" title="5    foldByKey"></a>5    <code>foldByKey</code></h4><p><strong>参数：</strong> <code>(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]</code></p>
<p><strong>作用：</strong>类似于 <code>reduceByKey</code> ，有预聚合，用于聚合操作，但是多了一个零值的功能。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>,<span class="number">3</span>)，(<span class="string">&quot;a&quot;</span>,<span class="number">2</span>)，(<span class="string">&quot;c&quot;</span>,<span class="number">4</span>)，(<span class="string">&quot;b&quot;</span>,<span class="number">3</span>)，(<span class="string">&quot;c&quot;</span>,<span class="number">6</span>)，(<span class="string">&quot;c&quot;</span>,<span class="number">8</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">2</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.foldByKey(<span class="number">0</span>)(_ + _).collect</span><br><span class="line">res5: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((b,<span class="number">3</span>)，(a,<span class="number">5</span>)，(c,<span class="number">18</span>))</span><br></pre></td></tr></table></figure>







<hr>
<h4 id="6-aggregateByKey-zeroValue-seqOp-combOp-numTasks"><a href="#6-aggregateByKey-zeroValue-seqOp-combOp-numTasks" class="headerlink" title="6   aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])"></a>6   <code>aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</code></h4><p><strong>思考：</strong><code>reduceByKey，aggregateByKey，foldByKey</code> 的区别和联系？</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、reduceByKey、foldByKey 都有预聚合，且分区内聚合和分区间聚合的逻辑是一样的。</span><br><span class="line">2、aggregateByKey 也有预聚合，但是区内聚合和分区间聚合的逻辑可以不一样。</span><br><span class="line">3、零值只在分区内聚合的时候使用</span><br></pre></td></tr></table></figure>

<p><img src="Spark-Core/image-20211215213010832.png" alt="image-20211215213010832"></p>
<p><strong>函数声明：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">	1. zeroValue：零值，分区内使用。给每一个分区中的每一个key一个初始值；</span></span><br><span class="line"><span class="comment">	2. seqOp：分区内的聚合逻辑。函数用于在每一个分区中用初始值逐步迭代value；</span></span><br><span class="line"><span class="comment">	3. combOp：分区间的聚合逻辑。函数用于合并每个分区中的结果。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregateByKey</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>,</span><br><span class="line">                                              combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)] = self.withScope &#123;</span><br><span class="line">    aggregateByKey(zeroValue, defaultPartitioner(self))(seqOp, combOp)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用给定的 <code>combine</code> 函数和一个初始化的 <code>zeroValue</code>，对每个 key 的 value 进行聚合。</p>
<p>这个函数返回的类型 U 不同于源 RDD 中的 V 类型。U 的类型是由初始化的 <code>zeroValue</code> 来决定的。所以我们需要两个操作：</p>
<ul>
<li>  一个操作 <code>seqOp</code> 用于把 1 个 V 变成 1 个U </li>
<li>  另外一个操作 <code>combOp</code> 用于合并 2 个 U</li>
</ul>
<p>第一个操作用于在一个分区内进行合并，第二个操作用在两个分区间进行合并。</p>
<p>为了避免内存分配，这两个操作函数都允许返回第一个参数，而不用创建一个新的 U</p>
<p><strong>案例：</strong>计算每个分区相同 key 对应 value 的最大值，然后让多个分区间的最大值相加</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">3</span>),(<span class="string">&quot;a&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">4</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">3</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">6</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">8</span>)),<span class="number">2</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1、分区内计算最大值	2、分区间求和</span></span><br><span class="line">scala&gt; rdd.aggregateByKey(<span class="type">Int</span>.<span class="type">MinValue</span>)(math.max(_, _), _ +_)</span><br><span class="line">res0: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">1</span>] at aggregateByKey at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; res0.collect</span><br><span class="line">res1: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((b,<span class="number">3</span>), (a,<span class="number">3</span>), (c,<span class="number">12</span>)) </span><br></pre></td></tr></table></figure>





<hr>
<h4 id="7-combineByKey-C"><a href="#7-combineByKey-C" class="headerlink" title="7   combineByKey[C]"></a>7   <code>combineByKey[C]</code></h4><p><strong>描述：</strong></p>
<ul>
<li>  用于聚合</li>
<li>  分区内聚合和分区间聚合逻辑可以不同</li>
<li>  零值不是写死的，零值是根据每个 key 的第一个 value 来动态生成</li>
</ul>
<p><strong>函数声明：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  1.createCombiner: 创建一个零值。combineByKey会遍历分区中的每个key-value对，</span></span><br><span class="line"><span class="comment">  如果第一次碰到这个key, 则调用createCombiner函数，传入value，得到一个C类型的值。</span></span><br><span class="line"><span class="comment">  (如果不是第一次碰到这个 key, 则不会调用这个方法)</span></span><br><span class="line"><span class="comment">  </span></span><br><span class="line"><span class="comment">  2.mergeValue：分区内的聚合函数。如果不是第一个遇到这个key, 则调用这个函数进行合并操作。</span></span><br><span class="line"><span class="comment">  </span></span><br><span class="line"><span class="comment">  3.mergeCombiners：分区间的聚合函数。跨分区合并相同的key的值(C)。 </span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span><br><span class="line">    combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners,</span><br><span class="line">                             partitioner, mapSideCombine, serializer)(<span class="literal">null</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>作用：</strong>针对每个 K，将 V 进行合并成 C，得到 <code>RDD[(K,C)]</code></p>
<p><strong>案例：</strong>workcount</p>
<p>需求1: 创建一个 pairRDD，根据 key 计算每种 key 的value的平均值。（先计算每个key出现的次数以及可以对应值的总和，再相除得到结果）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> input = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>，<span class="number">88</span>)，(<span class="string">&quot;b&quot;</span>，<span class="number">95</span>)，(<span class="string">&quot;a&quot;</span>，<span class="number">91</span>)，(<span class="string">&quot;b&quot;</span>，<span class="number">93</span>)，(<span class="string">&quot;a&quot;</span>，<span class="number">95</span>)，(<span class="string">&quot;b&quot;</span>，<span class="number">98</span>)),<span class="number">2</span>)</span><br><span class="line">input: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">5</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// acc 累加器, 用来记录分区内的值的和这个 key 出现的次数</span></span><br><span class="line"><span class="comment">// acc1, acc2 跨分区的累加器</span></span><br><span class="line">scala&gt; input.combineByKey((, <span class="number">1</span>)，(acc:(<span class="type">Int</span>, <span class="type">Int</span>), v) =&gt; (acc<span class="number">.1</span> + v, acc<span class="number">.2</span> + <span class="number">1</span>)，(acc1:(<span class="type">Int</span>, <span class="type">Int</span>), acc2: (<span class="type">Int</span>, <span class="type">Int</span>))=&gt; (acc1<span class="number">.1</span> + acc2<span class="number">.1</span>, acc1<span class="number">.2</span> + acc2<span class="number">.2</span>))</span><br><span class="line">res10: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = <span class="type">ShuffledRDD</span>[<span class="number">7</span>] at combineByKey at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; res10.collect</span><br><span class="line">res11: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = <span class="type">Array</span>((b,(<span class="number">286</span>,<span class="number">3</span>))，(a,(<span class="number">274</span>,<span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; res10.map(t =&gt; (t<span class="number">.1</span>, t<span class="number">.2</span><span class="number">.1</span>.toDouble / t<span class="number">.2</span><span class="number">.2</span>)).collect</span><br><span class="line">res12: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = <span class="type">Array</span>((b,<span class="number">95.33333333333333</span>)，(a,<span class="number">91.33333333333333</span>))</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="8-sortByKey"><a href="#8-sortByKey" class="headerlink" title="8   sortByKey"></a>8   <code>sortByKey</code></h4><p><strong>作用：</strong>在一个 <code>(K,V)</code> 的 RDD 上调用，K 必须实现 <code>Ordered[K]</code> 接口或者有一个隐式值：<code>Ordering[K]</code>，返回一个按照 key 进行排序的<code>(K,V)</code> 的 RDD。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>)，(<span class="number">10</span>, <span class="string">&quot;b&quot;</span>)，(<span class="number">11</span>, <span class="string">&quot;c&quot;</span>)，(<span class="number">4</span>, <span class="string">&quot;d&quot;</span>)，(<span class="number">20</span>, <span class="string">&quot;d&quot;</span>)，(<span class="number">10</span>, <span class="string">&quot;e&quot;</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">11</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortByKey()</span><br><span class="line">res25: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">14</span>] at sortByKey at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; res25.collect</span><br><span class="line">res26: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">1</span>,a)，(<span class="number">4</span>,d)，(<span class="number">10</span>,b)，(<span class="number">10</span>,e)，(<span class="number">11</span>,c)，(<span class="number">20</span>,d))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortByKey(<span class="literal">true</span>).collect</span><br><span class="line">res27: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">1</span>,a)，(<span class="number">4</span>,d)，(<span class="number">10</span>,b)，(<span class="number">10</span>,e)，(<span class="number">11</span>,c)，(<span class="number">20</span>,d))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 倒序</span></span><br><span class="line">scala&gt; rdd.sortByKey(<span class="literal">false</span>).collect</span><br><span class="line">res28: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">20</span>,d)，(<span class="number">11</span>,c)，(<span class="number">10</span>,b)，(<span class="number">10</span>,e)，(<span class="number">4</span>,d)，(<span class="number">1</span>,a))</span><br></pre></td></tr></table></figure>





<hr>
<h4 id="9-mapValues"><a href="#9-mapValues" class="headerlink" title="9   mapValues"></a>9   <code>mapValues</code></h4><p><strong>作用：</strong>针对 <code>(K,V)</code> 形式的类型只对 V 进行操作</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>)，(<span class="number">10</span>, <span class="string">&quot;b&quot;</span>)，(<span class="number">11</span>, <span class="string">&quot;c&quot;</span>)，(<span class="number">4</span>, <span class="string">&quot;d&quot;</span>)，(<span class="number">20</span>, <span class="string">&quot;d&quot;</span>)，(<span class="number">10</span>, <span class="string">&quot;e&quot;</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">21</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.mapValues(<span class="string">&quot;&lt;&quot;</span> + _ + <span class="string">&quot;&gt;&quot;</span>).collect</span><br><span class="line">res29: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">1</span>,&lt;a&gt;)，(<span class="number">10</span>,&lt;b&gt;)，(<span class="number">11</span>,&lt;c&gt;)，(<span class="number">4</span>,&lt;d&gt;)，(<span class="number">20</span>,&lt;d&gt;)，(<span class="number">10</span>,&lt;e&gt;))</span><br></pre></td></tr></table></figure>





<hr>
<h4 id="10-join-otherDataset-numTasks"><a href="#10-join-otherDataset-numTasks" class="headerlink" title="10  join(otherDataset, [numTasks])"></a>10  <code>join(otherDataset, [numTasks])</code></h4><ol>
<li> 含义和 SQL 差不多，用来连接两个 RDD</li>
<li> 连接肯定需要连接条件，在 SQL 中是 <code>tb_a.id = tb_b.id</code>，对应在 RDD 中就是 <code>rdd1.id = rdd2.id</code>，把 key 相等的连在一起</li>
<li> 也支持外连接：<code>leftOuterJoin, rightOuterJoin, and fullOuterJoin</code>。</li>
<li> 如果某一个 RDD 有重复的 Key, 则会分别与另外一个 RDD 的相同的 Key 进行组合。</li>
</ol>
<p><strong>内连接</strong>：在类型为 <code>(K,V)</code> 和 <code>(K,W)</code> 的 RDD 上调用，返回一个相同 key 对应的所有元素对在一起的 <code>(K,(V,W))</code> 的 RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>)，(<span class="number">1</span>, <span class="string">&quot;b&quot;</span>)，(<span class="number">2</span>, <span class="string">&quot;c&quot;</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">6</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> rdd2 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">&quot;aa&quot;</span>)，(<span class="number">3</span>, <span class="string">&quot;bb&quot;</span>)，(<span class="number">2</span>, <span class="string">&quot;cc&quot;</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">7</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.join(rdd2).collect</span><br><span class="line">res2: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">String</span>, <span class="type">String</span>))] = <span class="type">Array</span>((<span class="number">2</span>,(c,cc))，(<span class="number">1</span>,(a,aa))，(<span class="number">1</span>,(b,aa)))</span><br></pre></td></tr></table></figure>





<hr>
<h4 id="11-cogroup-otherDataset-numTasks"><a href="#11-cogroup-otherDataset-numTasks" class="headerlink" title="11  cogroup(otherDataset, [numTasks])"></a>11  <code>cogroup(otherDataset, [numTasks])</code></h4><p><strong>作用：</strong>在类型为 <code>(K,V)</code> 和 <code>(K,W)</code> 的 RDD 上调用，返回一个 <code>(K,(Iterable&lt;V&gt;, Iterable&lt;W&gt;))</code> 类型的 RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>, <span class="number">10</span>),(<span class="number">2</span>, <span class="number">20</span>),(<span class="number">1</span>, <span class="number">100</span>),(<span class="number">3</span>, <span class="number">30</span>)),<span class="number">1</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">23</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>),(<span class="number">2</span>, <span class="string">&quot;b&quot;</span>),(<span class="number">1</span>, <span class="string">&quot;aa&quot;</span>),(<span class="number">3</span>, <span class="string">&quot;c&quot;</span>)),<span class="number">1</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">24</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.cogroup(rdd2).collect</span><br><span class="line">res9: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">Int</span>], <span class="type">Iterable</span>[<span class="type">String</span>]))] = <span class="type">Array</span>((<span class="number">1</span>,(<span class="type">CompactBuffer</span>(<span class="number">10</span>, <span class="number">100</span>),<span class="type">CompactBuffer</span>(a, aa)))，(<span class="number">3</span>,(<span class="type">CompactBuffer</span>(<span class="number">30</span>),<span class="type">CompactBuffer</span>(c)))，(<span class="number">2</span>,(<span class="type">CompactBuffer</span>(<span class="number">20</span>),<span class="type">CompactBuffer</span>(b))))</span><br></pre></td></tr></table></figure>







<hr>
<h3 id="2-3-4-案例实操"><a href="#2-3-4-案例实操" class="headerlink" title="2.3.4  案例实操"></a>2.3.4  案例实操</h3><h4 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h4><ol>
<li><p><strong>数据结构：</strong>时间戳，省份，城市，用户，广告，字段使用空格分割。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">时间戳	|	省份	|	城市	|	用户	|	广告</span><br><span class="line">1516609143867	6	7	64	16</span><br><span class="line">1516609143869	9	4	75	18</span><br><span class="line">1516609143869	1	7	87	12</span><br></pre></td></tr></table></figure>

<p> 每点击一次就会产生一条记录</p>
</li>
<li><p> <strong>需求：</strong>统计出每一个省份广告被点击次数的 TOP3</p>
</li>
<li><p><strong>分析：</strong>使用倒推法</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">==&gt; <span class="type">RDD</span>[(省份, (广告, <span class="number">10</span>))]	<span class="comment">// groupByKey，把相同省份的集合划分到一起</span></span><br><span class="line">==&gt; <span class="type">RDD</span>[(省份, list(广告<span class="number">1</span>信息， 广告<span class="number">2</span>信息， 广告<span class="number">3</span>信息, ...))]	<span class="comment">// 对list进行排序，取前三</span></span><br><span class="line">==&gt; 最终结果：<span class="type">RDD</span>[(省份, list(广告<span class="number">1</span>信息， 广告<span class="number">2</span>信息， 广告<span class="number">3</span>信息))]</span><br><span class="line"></span><br><span class="line"> =&gt; [((pid, cid)，<span class="number">1</span>)，((pid, cid)，<span class="number">1</span>)]  reuceByKey</span><br><span class="line"> =&gt; [(pid, cid), count)，(pid, cid), count] map</span><br><span class="line"> =&gt; [pid, (cid, count)，(cid, count)] groupByKey</span><br><span class="line"> =&gt; [pid, <span class="type">Iterable</span>((cid, count)，(cid, count)，。。。)]</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h4><ol>
<li><p> 步骤1：把文件放入到 resources 目录下</p>
</li>
<li><p>步骤2：具体代码</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author lnd</span></span><br><span class="line"><span class="comment"> * @Date 2021/12/16 18:05</span></span><br><span class="line"><span class="comment"> * @Version 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 时间戳，省份，城市，用户，广告</span></span><br><span class="line"><span class="comment">// 1516609143867 6 7 64 16</span></span><br><span class="line"><span class="comment">// 统计出每一个省份广告被点击次数的 TOP3</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Practice</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 1. 初始化spark配置信息，并建立到spark的连接</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Practice&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、从文件中读取数据得到 RDD</span></span><br><span class="line">    <span class="keyword">val</span> lines: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;E:\\workspace_bigData\\spark\\spark-core\\src\\main\\resources\\agent.log&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、((province, ad), 1)</span></span><br><span class="line">    <span class="keyword">val</span> provinceADAndOne: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = lines.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> splits: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">      ((splits(<span class="number">1</span>), splits(<span class="number">4</span>)), <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、计算每个省份每个广告被点击的总次数</span></span><br><span class="line">    <span class="keyword">val</span> provinceADSum: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = provinceADAndOne.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5、将省份作为key，广告加点击数为value：(province, (ad, sum))</span></span><br><span class="line">    <span class="keyword">val</span> provinceToAdSum: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">Int</span>))] = provinceADSum.map(x =&gt; (x._1._1, (x._1._2, x._2)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6、按照省份进行分组</span></span><br><span class="line">    <span class="keyword">val</span> provinceGroup: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = provinceToAdSum.groupByKey()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7、对同一个省份的广告进行排序，按照点击数降序</span></span><br><span class="line">    <span class="keyword">val</span> result: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = provinceGroup.map(&#123;</span><br><span class="line">      <span class="keyword">case</span> (province, adSumIt) =&gt; &#123;</span><br><span class="line">        <span class="comment">// Iterable没有实现排序，需要先转换成容器式集合才能排序</span></span><br><span class="line">        <span class="keyword">val</span> tuples: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = adSumIt.toList.sortBy(-_._2) <span class="comment">//降序排</span></span><br><span class="line">          .take(<span class="number">3</span>) <span class="comment">// 取前三</span></span><br><span class="line"></span><br><span class="line">        (province, tuples)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 8、展示最终结果</span></span><br><span class="line">    result.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 9、关闭连接</span></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  结果展示：</span></span><br><span class="line"><span class="comment">  </span></span><br><span class="line"><span class="comment">  (4,List((12,25), (2,22), (16,22)))</span></span><br><span class="line"><span class="comment">  (8,List((2,27), (20,23), (11,22)))</span></span><br><span class="line"><span class="comment">  (6,List((16,23), (24,21), (22,20)))</span></span><br><span class="line"><span class="comment">  (0,List((2,29), (24,25), (26,24)))</span></span><br><span class="line"><span class="comment">  (2,List((6,24), (21,23), (29,20)))</span></span><br><span class="line"><span class="comment">  (7,List((16,26), (26,25), (1,23)))</span></span><br><span class="line"><span class="comment">  (5,List((14,26), (21,21), (12,21)))</span></span><br><span class="line"><span class="comment">  (9,List((1,31), (28,21), (0,20)))</span></span><br><span class="line"><span class="comment">  (3,List((14,28), (28,27), (22,25)))</span></span><br><span class="line"><span class="comment">  (1,List((3,25), (6,23), (5,22)))</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="2-4-RDD-的-Action-操作"><a href="#2-4-RDD-的-Action-操作" class="headerlink" title="2.4    RDD 的 Action 操作"></a>2.4    RDD 的 Action 操作</h2><h3 id="2-4-1-reduce-func"><a href="#2-4-1-reduce-func" class="headerlink" title="2.4.1  reduce(func)"></a>2.4.1  <code>reduce(func)</code></h3><p>通过 <code>func</code> 函数聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">100</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.reduce(_ + _)</span><br><span class="line">res0: <span class="type">Int</span> = <span class="number">5050</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">3</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">1</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.reduce((x, y) =&gt; (x._1 + y._1, x._2 + y._2))</span><br><span class="line">res2: (<span class="type">String</span>, <span class="type">Int</span>) = (abc,<span class="number">6</span>)</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="2-4-2-collect"><a href="#2-4-2-collect" class="headerlink" title="2.4.2  collect"></a>2.4.2  <code>collect</code></h3><ul>
<li>  以数组的形式返回 RDD 中的所有元素。 </li>
<li>  所有的数据都会被拉到 Driver 端，所以要慎用。</li>
</ul>
<hr>
<h3 id="2-4-3-count"><a href="#2-4-3-count" class="headerlink" title="2.4.3  count()"></a>2.4.3  <code>count()</code></h3><ul>
<li>  返回 RDD 中元素的个数。</li>
</ul>
<hr>
<h3 id="2-4-4-take-n"><a href="#2-4-4-take-n" class="headerlink" title="2.4.4  take(n)"></a>2.4.4  <code>take(n)</code></h3><ul>
<li>  返回 RDD 中前 n 个元素组成的数组。 </li>
<li>  <code>take()</code> 的数据也会拉到 Driver 端，应该只对小数据集使用</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>(<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>, <span class="number">60</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">2</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.take(<span class="number">2</span>)</span><br><span class="line">res3: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">10</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>





<hr>
<h3 id="2-4-5-first"><a href="#2-4-5-first" class="headerlink" title="2.4.5  first"></a>2.4.5  <code>first</code></h3><p>返回 RDD 中的第一个元素。类似于 <code>take(1)</code>。</p>
<hr>
<h3 id="2-4-6-takeOrdered-n-ordering"><a href="#2-4-6-takeOrdered-n-ordering" class="headerlink" title="2.4.6  takeOrdered(n, [ordering])"></a>2.4.6  <code>takeOrdered(n, [ordering])</code></h3><ul>
<li>  返回排序后的前 n 个元素，默认是升序排列。</li>
<li>  数据也会拉到 Driver 端</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>(<span class="number">100</span>, <span class="number">20</span>, <span class="number">130</span>, <span class="number">500</span>, <span class="number">60</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">4</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.takeOrdered(<span class="number">2</span>)</span><br><span class="line">res6: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">20</span>, <span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.takeOrdered(<span class="number">2</span>)(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse)</span><br><span class="line">res7: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">500</span>, <span class="number">130</span>)</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="2-4-7-aggregate"><a href="#2-4-7-aggregate" class="headerlink" title="2.4.7    aggregate"></a>2.4.7    <code>aggregate</code></h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">T</span>) =&gt; <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">U</span></span><br></pre></td></tr></table></figure>

<p><strong>作用：</strong><code>aggregate</code> 函数将每个分区里面的元素通过 <code>seqOp</code> 和初始值进行聚合，然后用 <code>combine</code> 函数将每个分区的结果和初始值 <code>zeroValue</code> 进行 <code>combine</code> 操作。这个函数最终返回的类型不需要和 RDD 中元素类型一致</p>
<p><strong>注意：<code>zeroValue</code> 分区内聚合和分区间聚合的时候各会使用一次</strong>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>(<span class="number">100</span>, <span class="number">30</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">1</span>, <span class="number">50</span>, <span class="number">1</span>, <span class="number">60</span>, <span class="number">1</span>), <span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">8</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="number">0</span>)(_ + _, _ + _)</span><br><span class="line">res12: <span class="type">Int</span> = <span class="number">283</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>), <span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">9</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="string">&quot;x&quot;</span>)(_ + _, _ + _)</span><br><span class="line">res13: <span class="type">String</span> = xxabxcd</span><br></pre></td></tr></table></figure>





<hr>
<h3 id="2-4-8-fold"><a href="#2-4-8-fold" class="headerlink" title="2.4.8  fold"></a>2.4.8  <code>fold</code></h3><p>折叠操作，<code>aggregate</code> 的简化操作，<code>seqop</code> 和 <code>combop</code> 一样的时候，可以使用 <code>fold</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>(<span class="number">100</span>, <span class="number">30</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">1</span>, <span class="number">50</span>, <span class="number">1</span>, <span class="number">60</span>, <span class="number">1</span>), <span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.fold(<span class="number">0</span>)(_ + _)</span><br><span class="line">res16: <span class="type">Int</span> = <span class="number">283</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>), <span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">11</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.fold(<span class="string">&quot;x&quot;</span>)(_ + _)</span><br><span class="line">res17: <span class="type">String</span> = xxabxcd</span><br></pre></td></tr></table></figure>





<hr>
<h3 id="2-4-9-saveAsTextFile-path"><a href="#2-4-9-saveAsTextFile-path" class="headerlink" title="2.4.9    saveAsTextFile(path)"></a>2.4.9    <code>saveAsTextFile(path)</code></h3><p><strong>作用：</strong>将数据集的元素以 <code>textfile</code> 的形式保存到 HDFS 文件系统或者其它支持的文件系统，对于每个元素，Spark 将会调用 <code>toString</code> 方法，将它装换为文件中的文本。</p>
<hr>
<h3 id="2-4-10-saveAsSequenceFile-path"><a href="#2-4-10-saveAsSequenceFile-path" class="headerlink" title="2.4.10    saveAsSequenceFile(path)"></a>2.4.10    <code>saveAsSequenceFile(path)</code></h3><p><strong>作用：</strong>将数据集中的元素以 <code>Hadoop sequencefile</code> 的格式保存到指定的目录下，可以使 HDFS 或者其它 Hadoop 支持的文件系统。</p>
<hr>
<h3 id="2-4-11-saveAsObjectFile-path"><a href="#2-4-11-saveAsObjectFile-path" class="headerlink" title="2.4.11    saveAsObjectFile(path)"></a>2.4.11    <code>saveAsObjectFile(path)</code></h3><p><strong>作用：</strong>用于将 RDD 中的元素序列化成对象，存储到文件中。</p>
<hr>
<h3 id="2-4-12-countByKey"><a href="#2-4-12-countByKey" class="headerlink" title="2.4.12    countByKey()"></a>2.4.12    <code>countByKey()</code></h3><p><strong>作用：</strong>针对 <code>(K,V)</code> 类型的 RDD，返回一个 <code>(K,Int)</code> 的 <code>map</code>，表示每一个 key 对应的元素个数。</p>
<p><strong>应用：</strong>可以用来查看数据是否倾斜。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">20</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">100</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">200</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">15</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.countByKey()</span><br><span class="line">res19: scala.collection.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Long</span>] = <span class="type">Map</span>(b -&gt; <span class="number">1</span>, a -&gt; <span class="number">2</span>, c -&gt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure>





<hr>
<h3 id="2-4-13-foreach-func"><a href="#2-4-13-foreach-func" class="headerlink" title="2.4.13    foreach(func)"></a>2.4.13    <code>foreach(func)</code></h3><p><strong>作用：</strong>针对 RDD 中的每个元素都执行一次 <code>func</code>。每个函数是在 <code>Executor</code> 上执行的，不是在 <code>Driver</code> 端执行的。</p>
<hr>
<h2 id="2-5-RDD-中函数的传递"><a href="#2-5-RDD-中函数的传递" class="headerlink" title="2.5    RDD 中函数的传递"></a>2.5    RDD 中函数的传递</h2><p>我们在使用 Spark 进行编程的时候，初始化工作是在 <code>Driver</code> 端完成的，而实际的运行程序是在 <code>Executor</code> 端进行的，所以就涉及到了进程间的通讯，数据是需要序列化的。</p>
<h3 id="2-5-1-传递函数"><a href="#2-5-1-传递函数" class="headerlink" title="2.5.1  传递函数"></a>2.5.1  传递函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SerDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;SerDemo&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello atguigu&quot;</span>, <span class="string">&quot;atguigu&quot;</span>, <span class="string">&quot;hahah&quot;</span>), <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 需求: 在 RDD 中查找出来包含 &quot;hello&quot; 子字符串的元素</span></span><br><span class="line">        <span class="keyword">val</span> searcher = <span class="keyword">new</span> <span class="type">Searcher</span>(<span class="string">&quot;hello&quot;</span>)	<span class="comment">// 用来查找RDD中包含hello的字符串组成的子RDD</span></span><br><span class="line">        <span class="keyword">val</span> result: <span class="type">RDD</span>[<span class="type">String</span>] = searcher.getMatchedRDD1(rdd)</span><br><span class="line">        result.collect.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//需求: 在 RDD 中查找出来包含 query 子字符串的元素</span></span><br><span class="line"><span class="comment">// query 为需要查找的子字符串</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Searcher</span>(<span class="params">val query: <span class="type">String</span></span>)</span>&#123;</span><br><span class="line">    <span class="comment">// 判断 s 中是否包括子字符串 query</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s : <span class="type">String</span>) =&#123;</span><br><span class="line">        s.contains(query)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 过滤出包含 query 字符串的字符串组成的新的 RDD</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatchedRDD1</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) =&#123;</span><br><span class="line">        rdd.filter(isMatch) </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 过滤出包含 query字符串的字符串组成的新的 RDD</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatchedRDD2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) =&#123;</span><br><span class="line">        rdd.filter(_.contains(query))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>说明：</strong>直接运行程序会发现报错，没有序列化化。因为 <code>rdd.filter(isMatch)</code> 是在 <code>Executor</code> 上执行的，而且用到了对象 <code>this</code> 的方法 <code>isMatch</code>，但是 <code>Executor</code> 上并没有 <code>Searcher</code> 对象，所以对象 <code>this</code> 需要序列化，才能把对象从 <code>driver</code> 发送到 <code>executor</code>。</p>
<p><img src="Spark-Core/image-20211217102243803.png" alt="image-20211217102243803"></p>
<p><strong>解决方案：</strong>让 <code>Searcher</code> 类实现序列化接口 <code>Serializable</code></p>
<p> <img src="Spark-Core/image-20211217102251217.png" alt="image-20211217102251217"></p>
<hr>
<h3 id="2-5-2-传递变量"><a href="#2-5-2-传递变量" class="headerlink" title="2.5.2  传递变量"></a>2.5.2  传递变量</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SerDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;SerDemo&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello atguigu&quot;</span>, <span class="string">&quot;atguigu&quot;</span>, <span class="string">&quot;hahah&quot;</span>), <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">val</span> searcher = <span class="keyword">new</span> <span class="type">Searcher</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> result: <span class="type">RDD</span>[<span class="type">String</span>] = searcher.getMatchedRDD2(rdd)</span><br><span class="line">        result.collect.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>说明：</strong>这次没有传递函数，而是传递了一个属性过去，仍然会报错没有序列化，因为 <code>query</code> 变量是 <code>Searcher</code> 对象的一个属性，所以 <code>this</code> 仍然需要序列化。</p>
<p><strong>解决方案有 2 种：</strong></p>
<ul>
<li><p>  让类实现序列化接口：Serializable</p>
</li>
<li><p>传递局部变量而不是属性。</p>
<p>   <img src="Spark-Core/image-20211217102320709.png" alt="image-20211217102320709"></p>
<p>  如果使用 <code>val q = query</code> 把 <code>query</code> 变量赋值给常量 <code>q</code>，因为 <code>q</code> 是 String 类型的常量，这一系统类型已经默认实现了序列化接口，现在 <code>rdd.filter(_.contains(q))</code> 与 <code>Seacher</code> 对象没有了任何关系，所以 <code>Searcher</code> 对象可以无需序列化。</p>
</li>
</ul>
<hr>
<h3 id="2-5-3-kryo-序列化框架"><a href="#2-5-3-kryo-序列化框架" class="headerlink" title="2.5.3  kryo 序列化框架"></a>2.5.3  kryo 序列化框架</h3><p>参考地址：<a target="_blank" rel="noopener" href="https://github.com/EsotericSoftware/kryo">https://github.com/EsotericSoftware/kryo</a></p>
<p>Java 的序列化比较重，能够序列化任何的类，比较灵活，但是相当的慢，并且序列化后对象的体积也比较大。</p>
<p>Spark 出于性能的考虑，支持另外一种序列化机制 <code>kryo (2.0开始支持)</code>。kryo 比较快和简洁(速度是Serializable的10倍)，想获取更好的性能应该使用 kryo 来序列化。</p>
<p>从 2.0 开始，Spark 内部已经在使用 kryo 序列化机制：当 RDD 在 Shuffle数据的时候，简单数据类型，简单数据类型的数组和字符串类型已经在使用 kryo 来序列化。</p>
<p>有一点需要注意的是：<strong>即使使用 kryo 序列化，自定义类也要继承 Serializable 接口。</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> day03</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SerDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        .setAppName(<span class="string">&quot;SerDemo&quot;</span>)</span><br><span class="line">        .setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        <span class="comment">// 替换默认的序列化机制 可以省(如果调用registerKryoClasses)</span></span><br><span class="line">        .set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line">        <span class="comment">// 注册需要使用 kryo 序列化的自定义类</span></span><br><span class="line">        .registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">Searcher</span>]))</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello atguigu&quot;</span>, <span class="string">&quot;atguigu&quot;</span>, <span class="string">&quot;hahah&quot;</span>), <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">val</span> searcher = <span class="keyword">new</span> <span class="type">Searcher</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> result: <span class="type">RDD</span>[<span class="type">String</span>] = searcher.getMatchedRDD1(rdd)</span><br><span class="line">        result.collect.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Searcher</span>(<span class="params">val query: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 判断 s 中是否包括子字符串 query</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>) = &#123;</span><br><span class="line">        s.contains(query)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 过滤出包含 query字符串的字符串组成的新的 RDD</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatchedRDD1</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) = &#123;</span><br><span class="line">        rdd.filter(isMatch) <span class="comment">//</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 过滤出包含 query字符串的字符串组成的新的 RDD</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatchedRDD2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) = &#123;</span><br><span class="line">        <span class="keyword">val</span> q = query</span><br><span class="line">        rdd.filter(_.contains(q))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="2-6-RDD-的依赖关系"><a href="#2-6-RDD-的依赖关系" class="headerlink" title="2.6    RDD 的依赖关系"></a>2.6    RDD 的依赖关系</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.textFile(<span class="string">&quot;./words.txt&quot;</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = ./words.txt <span class="type">MapPartitionsRDD</span>[<span class="number">16</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = rdd1.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">17</span>] at flatMap at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd2.map((_, <span class="number">1</span>))</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">18</span>] at map at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd4 = rdd3.reduceByKey(_ + _)</span><br><span class="line">rdd4: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">19</span>] at reduceByKey at &lt;console&gt;:<span class="number">30</span></span><br></pre></td></tr></table></figure>

<h3 id="2-6-1-查看-RDD-的血缘关系"><a href="#2-6-1-查看-RDD-的血缘关系" class="headerlink" title="2.6.1  查看 RDD 的血缘关系"></a>2.6.1  查看 RDD 的血缘关系</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.toDebugString</span><br><span class="line">res1: <span class="type">String</span> =</span><br><span class="line">(<span class="number">2</span>) ./words.txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line">| ./words.txt <span class="type">HadoopRDD</span>[<span class="number">0</span>] at textFile at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.toDebugString</span><br><span class="line">res2: <span class="type">String</span> =</span><br><span class="line">(<span class="number">2</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at &lt;console&gt;:<span class="number">26</span> []</span><br><span class="line">| ./words.txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line">| ./words.txt <span class="type">HadoopRDD</span>[<span class="number">0</span>] at textFile at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.toDebugString</span><br><span class="line">res3: <span class="type">String</span> =</span><br><span class="line">(<span class="number">2</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">28</span> []</span><br><span class="line">| <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at &lt;console&gt;:<span class="number">26</span> []</span><br><span class="line">| ./words.txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line">| ./words.txt <span class="type">HadoopRDD</span>[<span class="number">0</span>] at textFile at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line"></span><br><span class="line">scala&gt; rdd4.toDebugString</span><br><span class="line">res4: <span class="type">String</span> =</span><br><span class="line">(<span class="number">2</span>) <span class="type">ShuffledRDD</span>[<span class="number">4</span>] at reduceByKey at &lt;console&gt;:<span class="number">30</span> []</span><br><span class="line">+-(<span class="number">2</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">28</span> []</span><br><span class="line">    | <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at &lt;console&gt;:<span class="number">26</span> []</span><br><span class="line">    | ./words.txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line">    | ./words.txt <span class="type">HadoopRDD</span>[<span class="number">0</span>] at textFile at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>说明：</strong>圆括号中的数字表示 RDD 的并行度，也就是有几个分区。</p>
<hr>
<h3 id="2-6-2-查看-RDD-的依赖关系"><a href="#2-6-2-查看-RDD-的依赖关系" class="headerlink" title="2.6.2  查看 RDD 的依赖关系"></a>2.6.2  查看 RDD 的依赖关系</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.dependencies</span><br><span class="line">res28: <span class="type">Seq</span>[org.apache.spark.<span class="type">Dependency</span>[_]] = <span class="type">List</span>(org.apache.spark.<span class="type">OneToOneDependency</span>@<span class="number">70</span>dbde75)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.dependencies</span><br><span class="line">res29: <span class="type">Seq</span>[org.apache.spark.<span class="type">Dependency</span>[_]] = <span class="type">List</span>(org.apache.spark.<span class="type">OneToOneDependency</span>@<span class="number">21</span>a87972)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.dependencies</span><br><span class="line">res30: <span class="type">Seq</span>[org.apache.spark.<span class="type">Dependency</span>[_]] = <span class="type">List</span>(org.apache.spark.<span class="type">OneToOneDependency</span>@<span class="number">4776</span>f6af)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd4.dependencies</span><br><span class="line">res31: <span class="type">Seq</span>[org.apache.spark.<span class="type">Dependency</span>[_]] = <span class="type">List</span>(org.apache.spark.<span class="type">ShuffleDependency</span>@<span class="number">4809035</span>f)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>想理解 RDDs 是如何工作的，最重要的事情就是了解 <code>transformations</code>。</p>
<p>RDD 之间的关系可以从两个维度来理解：一个是 RDD 是从哪些 RDD 转换而来，也就是 RDD 的 parent RDD(s) 是什么；另一个就是 RDD 依赖于 parent RDD(s) 的哪些 Partition(s)。这种关系就是 RDD 之间的依赖。</p>
<p>依赖有 2 种策略:</p>
<ol>
<li> 窄依赖(transformations with narrow dependencies)</li>
<li> 宽依赖(transformations with wide dependencies)</li>
</ol>
<p>宽依赖对 Spark 去评估一个 transformations 有更加重要的影响，比如对性能的影响。</p>
<hr>
<h3 id="2-6-3-窄依赖"><a href="#2-6-3-窄依赖" class="headerlink" title="2.6.3  窄依赖"></a>2.6.3  窄依赖</h3><p>如果 <code>B RDD</code> 是由 <code>A RDD</code> 计算得到的，则 <code>B RDD</code> 就是 <code>Child RDD</code>， <code>A RDD</code> 就是 <code>parent RDD</code>。</p>
<p>如果依赖关系在设计的时候就可以确定，而不需要考虑父 RDD 分区中的记录，并且如果父 RDD 中的每个分区最多只有一个子分区，这样的依赖就叫窄依赖。</p>
<p>一句话总结：<strong>父RDD 的每个分区最多被一个 RDD 的分区使用</strong>。</p>
<p><img src="Spark-Core/image-20211217104250450.png" alt="image-20211217104250450"></p>
<p>​                               </p>
<p>具体来说，窄依赖的时候，子 RDD 中的分区要么只依赖一个父 RDD 中的一个分区(比如map, filter操作)，要么在设计时候就能确定子 RDD 是父 RDD 的一个子集(比如: coalesce)。</p>
<p>所以，窄依赖的转换可以在任何的的一个分区上单独执行，而不需要其它分区的任何信息。</p>
<hr>
<h3 id="2-6-4-宽依赖"><a href="#2-6-4-宽依赖" class="headerlink" title="2.6.4  宽依赖"></a>2.6.4  宽依赖</h3><p>如果父 RDD 的分区被不止一个子 RDD 的分区依赖，就是宽依赖。</p>
<p><img src="Spark-Core/image-20211217104447823.png" alt="image-20211217104447823"></p>
<p>宽依赖工作的时候，不能随意在某些记录上运行，而是需要使用特殊的方式(比如按照 key)来获取分区中的所有数据。</p>
<p>例如：在排序(sort)的时候，数据必须被分区，同样范围的 key 必须在同一个分区内。具有宽依赖的 <code>transformations</code> 包括：sort， reduceByKey，groupByKey，join 和调用 <code>rePartition</code> 函数的任何操作。</p>
<hr>
<h2 id="2-7-Spark-Job-的划分"><a href="#2-7-Spark-Job-的划分" class="headerlink" title="2.7    Spark Job 的划分"></a>2.7    Spark Job 的划分</h2><p>由于 Spark 的懒执行，在驱动程序调用一个 <code>action</code> 算子之前，Spark 应用不会做任何事情。</p>
<p>针对每个 <code>action</code>，Spark 调度器就创建一个 <code>执行图(execution graph)</code> 和启动一个 <code>Spark job</code>。</p>
<p>每个 job 由多个 <code>stages</code> 组成，这些 <code>stages</code> 就是实现最终的 RDD 所需的数据转换的步骤，一个宽依赖划分一个 <code>stage</code>。</p>
<p>每个 <code>stage</code> 由多个 <code>tasks</code> 来组成，这些 <code>tasks</code> 就表示每个并行计算，并且会在多个执行器上执行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Application	应用</span><br><span class="line"></span><br><span class="line">Job：</span><br><span class="line">	一个应用中，每碰到一个 action ，就会启动一个 Job</span><br><span class="line">	一个应用可以有多个 Job</span><br><span class="line"></span><br><span class="line">stage：	阶段</span><br><span class="line">	每碰到一个宽依赖，就会在这个Job内创建一个新的 stage</span><br><span class="line">	一个 job 至少有一个 stage</span><br><span class="line"></span><br><span class="line">task：	并行度</span><br><span class="line">	每个stage执行任务时的并行度</span><br><span class="line">	task是一个线程，是执行代码的最小单位</span><br></pre></td></tr></table></figure>



<p><img src="Spark-Core/image-20211217121753477.png" alt="image-20211217121753477"></p>
<hr>
<h3 id="2-7-1-DAG-Directed-Acyclic-Graph-有向无环图"><a href="#2-7-1-DAG-Directed-Acyclic-Graph-有向无环图" class="headerlink" title="2.7.1    DAG(Directed Acyclic Graph) 有向无环图"></a>2.7.1    DAG(Directed Acyclic Graph) 有向无环图</h3><p>Spark 的顶层调度层使用 RDD 的依赖为每个 job 创建一个由 stages 组成的 DAG(有向无环图)。 在 Spark API 中，这被称作 DAG 调度器(DAG Scheduler)。</p>
<p>我们已经注意到，有些错误，比如：连接集群的错误，配置参数错误，启动一个 Spark job 的错误，这些错误必须处理，并且都表现为 DAG Scheduler 错误。这是因为一个 Spark job 的执行是被 DAG 来处理。</p>
<p>DAG 为每个 job 构建一个 stages 组成的图表，从而确定运行每个 task 的位置，然后传递这些信息给 TaskSheduler。TaskSheduler 负责在集群中运行任务。</p>
<hr>
<h3 id="2-7-2-Jobs"><a href="#2-7-2-Jobs" class="headerlink" title="2.7.2    Jobs"></a>2.7.2    Jobs</h3><p>Spark job 处于 Spark 执行层级结构中的最高层。每个 Spark job 对应一个 action，每个 action 被 Spark 应用中的驱动所程序调用。</p>
<p>可以把 Action 理解成把数据从 RDD 的数据带到其它存储系统的组件(通常是带到驱动程序所在的位置或者写到稳定的存储系统中)。</p>
<p>只要一个 action 被调用，Spark 就不会再向这个 job 增加新的东西。</p>
<hr>
<h3 id="2-7-3-stages"><a href="#2-7-3-stages" class="headerlink" title="2.7.3    stages"></a>2.7.3    stages</h3><p>前面说过，RDD 的转换是懒执行的，直到调用一个 action 才开始执行 RDD 的转换。</p>
<p>正如前面所提到的，一个 job 是由调用一个 action 来定义的。一个 action 可能会包含一个或多个转换( transformation )，Spark 根据宽依赖把 job 分解成 stage。</p>
<p>从整体来看，一个 stage 可以任务是“计算(task)”的集合，这些每个“计算”在各自的 Executor 中进行运算，而不需要同其它的执行器或者驱动进行网络通讯。换句话说，当任何两个 workers 之间开始需要网络通讯的时候，这时候一个新的 stage 就产生了。例如：shuffle 的时候。</p>
<p>这些创建 stage 边界的依赖称为 <code>ShuffleDependencies</code>。shuffle 是由宽依赖所引起的，比如：sort，groupBy，因为它们需要在分区中重新分发数据，那些窄依赖的转换会被分到同一个 stage 中。</p>
<p>想想我们以前学习的 “worldcount 案例”</p>
<p> <img src="Spark-Core/image-20211217124958363.png" alt="image-20211217124958363"></p>
<p>Spark 会把 flatMap，map 合并到一个 stage 中，因为这些转换不需要 shuffle。所以，数据只需要传递一次，每个执行器就可以顺序的执行这些操作。</p>
<p>因为边界 stage 需要同驱动进行通讯，所以与 job 有关的 stage 通常必须顺序执行而不能并行执行。</p>
<p>如果这个 stage 是用来计算不同的 RDDs，被用来合并成一个下游的转换(比如: join)，也是有可能并行执行的。但是仅需要计算一个 RDD 的宽依赖转换必须顺序计算。</p>
<p>所以, 设计程序的时候, 尽量少用 shuffle。</p>
<hr>
<h3 id="2-7-4-Tasks"><a href="#2-7-4-Tasks" class="headerlink" title="2.7.4  Tasks"></a>2.7.4  Tasks</h3><p>stage 由 tasks 组成。在执行层级中，task 是最小的执行单位，每一个 task 表现为一个本地计算。</p>
<p>一个 stage 中的所有 tasks 会对不同的数据执行相同的代码。(程序代码一样, 只是作用在了不同的数据上)</p>
<p>一个 task 不能被多个执行器来执行，但是，每个执行器会动态的分配多个 slots 来执行 tasks，并且在整个生命周期内会并行的运行多个 task，每个 stage 的 task 的数量对应着分区的数量，即每个 Partition 都被分配一个 Task。 </p>
<p>在大多数情况下，每个 stage 的所有 task 在下一个 stage 开启之前必须全部完成。</p>
<hr>
<h2 id="2-8-RDD-的持久化"><a href="#2-8-RDD-的持久化" class="headerlink" title="2.8    RDD 的持久化"></a>2.8    RDD 的持久化</h2><p>每碰到一个 Action 就会产生一个 job，每个 job 开始计算的时候总是从这个 job 最开始的 RDD 开始计算。</p>
<h3 id="2-8-1-先看一段代码"><a href="#2-8-1-先看一段代码" class="headerlink" title="2.8.1  先看一段代码"></a>2.8.1  先看一段代码</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CacheDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Practice&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;ab&quot;</span>, <span class="string">&quot;bc&quot;</span>))</span><br><span class="line">        <span class="keyword">val</span> rdd2 = rdd1.flatMap(x =&gt; &#123;</span><br><span class="line">            println(<span class="string">&quot;flatMap...&quot;</span>)</span><br><span class="line">            x.split(<span class="string">&quot;&quot;</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="keyword">val</span> rdd3: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd2.map(x =&gt; &#123;</span><br><span class="line">            (x, <span class="number">1</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">        </span><br><span class="line">        rdd3.collect.foreach(println)</span><br><span class="line">        println(<span class="string">&quot;-----------&quot;</span>)</span><br><span class="line">        rdd3.collect.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>执行结果：</strong></p>
<p> <img src="Spark-Core/image-20211217125526721.png" alt="image-20211217125526721"></p>
<p><strong>说明：</strong></p>
<ol>
<li> 每调用一次 <code>collect</code>，都会创建一个新的 <code>job</code>，每个 <code>job</code> 总是从它血缘的起始开始计算。所以，会发现中间的这些计算过程都会重复的执行。</li>
<li> 原因是因为 <code>rdd</code> 记录了整个计算过程，如果计算的过程中出现哪个分区的数据损坏或丢失，则可以从头开始计算来达到容错的目的。</li>
</ol>
<hr>
<h3 id="2-8-2-RDD-数据的持久化"><a href="#2-8-2-RDD-数据的持久化" class="headerlink" title="2.8.2    RDD 数据的持久化"></a>2.8.2    RDD 数据的持久化</h3><p>每个 <code>job</code> 都会重新进行计算，在有些情况下是没有必要的，如何解决这个问题呢?</p>
<p>Spark 一个重要的能力就是可以持久化数据集在内存中。当我们持久化一个 RDD 时，每个节点都会存储它在内存中计算的那些分区，然后在其它的 <code>action</code> 中可以重用这些数据。这个特性会让将来的 <code>action</code> 计算起来更快(通常块 10 倍)。对于迭代算法和快速交互式查询来说，缓存(Caching)是一个关键工具。</p>
<p>可以使用方法 <code>persist()</code> 或者 <code>cache()</code> 来持久化一个 RDD，持久化的操作会在第一个 <code>action</code> 执行之后才真正执行。<code>action</code> 算子会计算这个 RDD，然后把结果的存储到它的节点的内存中。Spark 的 <code>Cache</code> 也是容错：如果 RDD 的任何一个分区的数据丢失了，Spark 会自动的重新计算。</p>
<p>RDD 的各个 <code>Partition</code> 是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部 <code>Partition</code>。</p>
<p>另外，允许我们对持久化的 RDD 使用不同的存储级别。</p>
<p>例如：可以存在磁盘上，存储在内存中(堆内存中)，跨节点做副本。</p>
<p>可以给 <code>persist()</code> 来传递存储级别。<code>cache()</code> 方法是使用默认存储级别( <code>StorageLevel.MEMORY_ONLY</code> )的简写方法。</p>
<table>
<thead>
<tr>
<th>Storage Level</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td>MEMORY_ONLY</td>
<td>Store RDD as  deserialized Java objects in the JVM. If the RDD does not fit in memory, some  partitions will not be cached and will be recomputed on the fly each time  they’re needed. This is the default level.</td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>Store RDD as  deserialized Java objects in the JVM. If the RDD does not fit in memory,  store the partitions that don’t fit on disk, and read them from there when  they’re needed.</td>
</tr>
<tr>
<td>MEMORY_ONLY_SER  (Java and Scala)</td>
<td>Store RDD as <em>serialized</em> Java objects (one byte  array per partition). This is generally more space-efficient than  deserialized objects, especially when using a <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.1.1/tuning.html">fast serializer</a>,  but more CPU-intensive to read.</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER  (Java and Scala)</td>
<td>Similar to  MEMORY_ONLY_SER, but spill partitions that don’t fit in memory to disk  instead of recomputing them on the fly each time they’re needed.</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>Store the RDD  partitions only on disk.</td>
</tr>
<tr>
<td>MEMORY_ONLY_2,  MEMORY_AND_DISK_2, etc.</td>
<td>Same as the  levels above, but replicate each partition on two cluster nodes.</td>
</tr>
<tr>
<td>OFF_HEAP  (experimental)</td>
<td>Similar to  MEMORY_ONLY_SER, but store the data in <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.1.1/configuration.html#memory-management">off-heap   memory</a>. This requires off-heap memory to be enabled.</td>
</tr>
</tbody></table>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// rdd2.cache() // 等价于 rdd2.persist(StorageLevel.MEMORY_ONLY)	对rdd2进行持久化</span></span><br><span class="line">rdd2.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br></pre></td></tr></table></figure>

<p><img src="Spark-Core/image-20211217170539985.png" alt="image-20211217170539985"></p>
<p><strong>说明：</strong></p>
<ol>
<li> 第一个 job 会计算 RDD2，以后的 job 就不用再计算了。</li>
<li> 有一点需要说明的是，即使我们不手动设置持久化，Spark 也会自动的对一些 <code>shuffle</code> 操作的中间数据做持久化操作（比如: <code>reduceByKey</code>）。这样做的目的是为了当一个节点 <code>shuffle</code> 失败后避免重新计算整个输入。当然，在实际使用的时候，如果想重用数据，仍然建议调用 <code>persist</code> 或 <code>cache</code>。</li>
</ol>
<hr>
<h2 id="2-9-设置检查点"><a href="#2-9-设置检查点" class="headerlink" title="2.9    设置检查点"></a>2.9    设置检查点</h2><p>Spark 中对于数据的保存除了持久化操作之外，还提供了一种检查点的机制，检查点（本质是通过将 RDD 写入 Disk 做检查点）是为了通过 Lineage 做容错的辅助。</p>
<p>Lineage 过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的 RDD 开始重做 Lineage，就会减少开销。</p>
<p>检查点通过将数据写入到 HDFS 文件系统实现了 RDD 的检查点功能。</p>
<p>为当前 RDD 设置检查点。该函数将会创建一个二进制的文件，并存储到 checkpoint 目录中，该目录是用 <code>SparkContext.setCheckpointDir()</code> 设置的。在 checkpoint 的过程中，该 RDD 的所有依赖于父 RDD 中的信息将全部被移除。</p>
<p>对 RDD 进行 <code>checkpoint</code> 操作并不会马上被执行，必须执行 <code>action</code> 操作才能触发，在触发的时候需要对这个 RDD 重新计算。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CheckPointDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 要在SparkContext初始化之前设置, 都在无效</span></span><br><span class="line">        <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;atguigu&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Practice&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">// 设置 checkpoint的目录. 如果spark运行在集群上, 则必须是 hdfs 目录</span></span><br><span class="line">        sc.setCheckpointDir(<span class="string">&quot;hdfs://hadoop201:9000/checkpoint&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;abc&quot;</span>))</span><br><span class="line">        <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">String</span>] = rdd1.map(_ + <span class="string">&quot; : &quot;</span> + <span class="type">System</span>.currentTimeMillis())</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">     标记 RDD2的 checkpoint.</span></span><br><span class="line"><span class="comment">     RDD2会被保存到文件中(文件位于前面设置的目录中), 并且会切断到父RDD的引用, 也就是切断了它向上的血缘关系</span></span><br><span class="line"><span class="comment">     该函数必须在job被执行之前调用.</span></span><br><span class="line"><span class="comment">     强烈建议把这个RDD序列化到内存中, 否则, 把它保存到文件的时候需要重新计算.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">        rdd2.checkpoint()</span><br><span class="line">        rdd2.collect().foreach(println)</span><br><span class="line">        rdd2.collect().foreach(println)</span><br><span class="line">        rdd2.collect().foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<hr>
<h3 id="持久化和checkpoint的区别"><a href="#持久化和checkpoint的区别" class="headerlink" title="持久化和checkpoint的区别"></a>持久化和checkpoint的区别</h3><ol>
<li>   持久化只是将数据保存在 BlockManager 中，而 RDD 的 Lineage 是不变的。但是 <code>checkpoint</code> 执行完后，RDD 已经没有之前所谓的依赖 RDD 了，而只有一个强行为其设置的 <code>checkpointRDD</code>，RDD 的 Lineage 改变了。</li>
<li>   持久化的数据丢失可能性更大，磁盘、内存都可能会存在数据丢失的情况。但是 <code>checkpoint</code> 的数据通常是存储在如 HDFS 等容错、高可用的文件系统，数据丢失可能性较小。</li>
<li>   <strong>注意：</strong>默认情况下，如果某个 RDD 没有持久化，但是设置了checkpoint，会存在问题。本来这个 job 都执行结束了，但是由于中间 RDD 没有持久化，checkpoint job 想要将 RDD 的数据写入外部文件系统的话，需要全部重新计算一次，再将计算出来的 RDD 数据 checkpoint 到外部文件系统。所以，建议对 <code>checkpoint()</code> 的 RDD 使用持久化，这样 RDD 只需要计算一次就可以了。</li>
</ol>
<hr>
<h1 id="第-3-章-Key-Value-类型-RDD-的数据分区器"><a href="#第-3-章-Key-Value-类型-RDD-的数据分区器" class="headerlink" title="第 3 章 Key-Value 类型 RDD 的数据分区器"></a>第 3 章 Key-Value 类型 RDD 的数据分区器</h1><ul>
<li>  对于只存储 value 的 RDD，是不需要分区器的。只有存储 <code>Key-Value</code> 类型的数据才会需要分区器。</li>
<li>  Spark 目前支持 Hash 分区器和 Range 分区器，用户也可以实现自定义的分区器。</li>
<li>  Hash 分区为器为 Spark 默认的分区器，Spark 中分区器直接决定了 RDD 分区的个数、RDD 中每条数据经过 <code>shuffle</code> 过程后属于哪个分区和 <code>Reduce</code> 的个数。</li>
</ul>
<hr>
<h2 id="3-1-查看-RDD-的分区"><a href="#3-1-查看-RDD-的分区" class="headerlink" title="3.1 查看 RDD 的分区"></a>3.1 查看 RDD 的分区</h2><h3 id="1-value-RDD-的分区器"><a href="#1-value-RDD-的分区器" class="headerlink" title="1.   value RDD 的分区器"></a>1.   value RDD 的分区器</h3><ul>
<li>  对于只存储 value 的 RDD，是没有分区器的。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">10</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">2</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.partitioner</span><br><span class="line">res8: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">None</span></span><br></pre></td></tr></table></figure>



<hr>
<h3 id="2-key-value-RDD-的分区器"><a href="#2-key-value-RDD-的分区器" class="headerlink" title="2.   key-value RDD 的分区器"></a>2.   key-value RDD 的分区器</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;hello&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;world&quot;</span>, <span class="number">1</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">3</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.partitioner</span><br><span class="line">res11: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 导入HashPartitioner</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">HashPartitioner</span></span><br><span class="line"><span class="comment">// 使用 HashPartitioner 分区器对 rdd1 重新分区, 得到分区后的 RDD </span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = rdd1.partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">3</span>))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">5</span>] at partitionBy at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.partitioner</span><br><span class="line">res14: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">Some</span>(org.apache.spark.<span class="type">HashPartitioner</span>@<span class="number">3</span>)</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="3-2-HashPartitioner"><a href="#3-2-HashPartitioner" class="headerlink" title="3.2 HashPartitioner"></a>3.2 HashPartitioner</h2><p><code>HashPartitioner</code> 分区器的原理：对于给定的 <code>key</code>，计算其 <code>hashCode</code>，并除以分区的个数取余，如果余数小于 0，则用 <code>余数+分区的个数（否则加0）</code>，最后返回的值就是这个 key 所属的分区 ID。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">HashPartitioner</span>, <span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Practice&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">10</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">20</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">30</span>, <span class="string">&quot;c&quot;</span>), (<span class="number">40</span>, <span class="string">&quot;d&quot;</span>), (<span class="number">50</span>, <span class="string">&quot;e&quot;</span>), (<span class="number">60</span>, <span class="string">&quot;f&quot;</span>)))</span><br><span class="line">        <span class="comment">// 把分区号取出来, 检查元素的分区情况</span></span><br><span class="line">        <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = rdd1.mapPartitionsWithIndex((index, it) =&gt; it.map(x =&gt; (index, x<span class="number">.1</span> + <span class="string">&quot; : &quot;</span> + x<span class="number">.2</span>)))</span><br><span class="line"></span><br><span class="line">        println(rdd2.collect.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 把 RDD1使用 HashPartitioner重新分区</span></span><br><span class="line">        <span class="keyword">val</span> rdd3 = rdd1.partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">5</span>))</span><br><span class="line">        <span class="comment">// 检测RDD3的分区情况</span></span><br><span class="line">        <span class="keyword">val</span> rdd4: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = rdd3.mapPartitionsWithIndex((index, it) =&gt; it.map(x =&gt; (index, x<span class="number">.1</span> + <span class="string">&quot; : &quot;</span> + x<span class="number">.2</span>)))</span><br><span class="line">        println(rdd4.collect.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="3-3-RangePartitioner"><a href="#3-3-RangePartitioner" class="headerlink" title="3.3 RangePartitioner"></a>3.3 RangePartitioner</h2><p><code>HashPartitioner</code> 分区的弊端：可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有 RDD 的全部数据。比如我们前面的例子就是一个极端，它们都进入了 0 分区。</p>
<p><code>RangePartitioner</code> 作用：将一定范围内的数映射到某一个分区内，尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大，但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。实现过程为：</p>
<ol>
<li> 第一步：先从整个 RDD 中抽取出样本数据，将样本数据排序，计算出每个分区的最大 key 值，形成一个 <code>Array[KEY]</code> 类型的数组变量 <code>rangeBounds</code>（边界数组）。</li>
<li> 第二步：判断 <code>key</code> 在 <code>rangeBounds</code> 中所处的范围，给出该 <code>key</code> 值在下一个 RDD 中的分区 id 下标；该分区器要求 RDD 中的 KEY 类型必须是可以排序的。比如 <code>[1,100,200,300,400]</code>，然后对比传进来的 <code>key</code>，返回对应的 <code>分区id</code>。</li>
</ol>
<hr>
<h2 id="3-4-自定义分区器"><a href="#3-4-自定义分区器" class="headerlink" title="3.4 自定义分区器"></a>3.4 自定义分区器</h2><p>要实现自定义的分区器，你需要继承 <code>org.apache.spark.Partitioner</code>，并且需要实现下面的方法：</p>
<ol>
<li>   <code>numPartitions</code>：该方法需要返回分区数，必须要大于 0。</li>
<li>   <code>getPartition(key)</code>：返回指定键的分区编号(0 到 <code>numPartitions-1</code>)。</li>
<li>   <code>equals</code>：Java 判断相等性的标准方法。这个方法的实现非常重要，Spark 需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样 Spark 才可以判断两个 RDD 的分区方式是否相同</li>
<li>   <code>hashCode</code>：如果你重写了 <code>equals</code>，则也应该重写这个方法。</li>
</ol>
<h3 id="MyPartitioner"><a href="#MyPartitioner" class="headerlink" title="MyPartitioner"></a>MyPartitioner</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">Partitioner</span>, <span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">使用自定义的 Partitioner 是很容易的 :只要把它传给 partitionBy() 方法即可。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">Spark 中有许多依赖于数据混洗的方法，比如 join() 和 groupByKey()，</span></span><br><span class="line"><span class="comment">它们也可以接收一个可选的 Partitioner 对象来控制输出数据的分区方式。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyPartitionerDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Practice&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd1 = sc.parallelize(</span><br><span class="line">            <span class="type">Array</span>((<span class="number">10</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">20</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">30</span>, <span class="string">&quot;c&quot;</span>), (<span class="number">40</span>, <span class="string">&quot;d&quot;</span>), (<span class="number">50</span>, <span class="string">&quot;e&quot;</span>), (<span class="number">60</span>, <span class="string">&quot;f&quot;</span>)),</span><br><span class="line">            <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = rdd1.partitionBy(<span class="keyword">new</span> <span class="type">MyPartitioner</span>(<span class="number">4</span>))</span><br><span class="line">        <span class="keyword">val</span> rdd3: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = rdd2.mapPartitionsWithIndex((index, items) =&gt; items.map(x =&gt; (index, x._1 + <span class="string">&quot; : &quot;</span> + x._2)))</span><br><span class="line">        println(rdd3.collect.mkString(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span>(<span class="params">numPars: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = numPars</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<hr>
<h1 id="第-4-章-文件中数据的读取和保存"><a href="#第-4-章-文件中数据的读取和保存" class="headerlink" title="第 4 章    文件中数据的读取和保存"></a>第 4 章    文件中数据的读取和保存</h1><p>本章专门学习如何<strong>从文件中读取数据</strong>和<strong>保存数据到文件中</strong>。</p>
<p>从文件中读取数据是创建 RDD 的一种方式。</p>
<p>把数据保存到文件中的操作是一种 Action。</p>
<p>Spark 的数据读取及数据保存可以从两个维度来作区分：<strong>文件格式</strong>以及<strong>文件系统</strong>。</p>
<ul>
<li>  文件格式分为：Text 文件、Json 文件、csv 文件、Sequence 文件以及 Object 文件；</li>
<li>  文件系统分为：本地文件系统、HDFS、Hbase 以及 数据库。</li>
</ul>
<p>平时用的比较多的就是：从 HDFS 读取和保存 Text 文件。</p>
<hr>
<h2 id="4-1-读写-Text-文件"><a href="#4-1-读写-Text-文件" class="headerlink" title="4.1    读写 Text 文件"></a>4.1    读写 Text 文件</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取本地文件</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;./words.txt&quot;</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = ./words.txt <span class="type">MapPartitionsRDD</span>[<span class="number">5</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = rdd1.flatMap(.split(<span class="string">&quot; &quot;</span>)).map((, <span class="number">1</span>)).reduceByKey(_ +_)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">8</span>] at reduceByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 保存数据到 hdfs 上.    </span></span><br><span class="line">scala&gt; rdd2.saveAsTextFile(<span class="string">&quot;hdfs://hadoop201:9000/words_output&quot;</span>)</span><br></pre></td></tr></table></figure>





<hr>
<h2 id="4-2-读取-Json-文件"><a href="#4-2-读取-Json-文件" class="headerlink" title="4.2    读取 Json 文件"></a>4.2    读取 Json 文件</h2><p>如果 JSON 文件中每一行就是一个 JSON 记录，那么可以通过将 JSON 文件当做文本文件来读取，然后利用相关的 JSON 库对每一条数据进行 JSON 解析。</p>
<p><strong>注意：</strong>使用 RDD 读取 JSON 文件处理很复杂，同时 SparkSQL 集成了很好的处理 JSON 文件的方式，所以实际应用中多是采用 SparkSQL 处理 JSON 文件。</p>
<p>关于 SparkSQL 后面的章节专门去讲</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取 json 数据的文件, 每行是一个 json 对象</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;/opt/module/spark-local/examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = /opt/module/spark-local/examples/src/main/resources/people.json <span class="type">MapPartitionsRDD</span>[<span class="number">11</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 导入 scala 提供的可以解析 json 的工具类</span></span><br><span class="line">scala&gt; <span class="keyword">import</span> scala.util.parsing.json.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> scala.util.parsing.json.<span class="type">JSON</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 map 来解析 Json, 需要传入 JSON.parseFull</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = rdd1.map(<span class="type">JSON</span>.parseFull)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Option</span>[<span class="type">Any</span>]] = <span class="type">MapPartitionsRDD</span>[<span class="number">12</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 解析到的结果其实就是 Option 组成的数组, Option 存储的就是 Map 对象</span></span><br><span class="line">scala&gt; rdd2.collect</span><br><span class="line">res2: <span class="type">Array</span>[<span class="type">Option</span>[<span class="type">Any</span>]] = <span class="type">Array</span>(<span class="type">Some</span>(<span class="type">Map</span>(name -&gt; <span class="type">Michael</span>)), <span class="type">Some</span>(<span class="type">Map</span>(name -&gt; <span class="type">Andy</span>, age -&gt; <span class="number">30.0</span>)), <span class="type">Some</span>(<span class="type">Map</span>(name -&gt; <span class="type">Justin</span>, age -&gt; <span class="number">19.0</span>)))</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="4-3-读写-SequenceFile-文件"><a href="#4-3-读写-SequenceFile-文件" class="headerlink" title="4.3    读写 SequenceFile 文件"></a>4.3    读写 SequenceFile 文件</h2><p> SequenceFile 文件是 Hadoop 用来存储二进制形式的 <code>key-value</code> 对而设计的一种平面文件(Flat File)。</p>
<p>Spark 有专门用来读取 SequenceFile 的接口。在 SparkContext 中，可以调用 <code>sequenceFile[ keyClass, valueClass](path)</code>。</p>
<p>注意：SequenceFile 文件只针对 PairRDD</p>
<ol>
<li><p>先保存一个 SequenceFile 文件</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>),(<span class="string">&quot;c&quot;</span>, <span class="number">3</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">13</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.saveAsSequenceFile(<span class="string">&quot;hdfs://hadoop201:9000/seqFiles&quot;</span>)</span><br></pre></td></tr></table></figure>

</li>
<li><p>读取 SequenceFile 文件</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.sequenceFile[<span class="type">String</span>, <span class="type">Int</span>](<span class="string">&quot;hdfs://hadoop201:9000/seqFiles&quot;</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">18</span>] at sequenceFile at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.collect</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((a,<span class="number">1</span>), (b,<span class="number">2</span>), (c,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<p> 注意：需要指定泛型的类型 <code>sc.sequenceFile[String, Int]</code></p>
</li>
</ol>
<hr>
<h2 id="4-4-读写-objectFile-文件"><a href="#4-4-读写-objectFile-文件" class="headerlink" title="4.4    读写 objectFile 文件"></a>4.4    读写 objectFile 文件</h2><p>对象文件是将对象序列化后保存的文件，采用 Java 的序列化机制。</p>
<p>可以通过 <code>objectFile[k,v](path)</code> 函数接收一个路径，读取对象文件，返回对应的 RDD，也可以通过调用 <code>saveAsObjectFile()</code> 实现对对象文件的输出</p>
<ol>
<li><p>把 RDD 保存为 <code>objectFile</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>),(<span class="string">&quot;c&quot;</span>, <span class="number">3</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">19</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.saveAsObjectFile(<span class="string">&quot;hdfs://hadoop201:9000/obj_file&quot;</span>)</span><br></pre></td></tr></table></figure>

</li>
<li><p>读取 <code>objectFile</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.objectFile(<span class="type">String</span>, <span class="type">Int</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">25</span>] at objectFile at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.collect</span><br><span class="line">res8: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((a,<span class="number">1</span>), (b,<span class="number">2</span>), (c,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<h2 id="4-5-从-HDFS-读写文件"><a href="#4-5-从-HDFS-读写文件" class="headerlink" title="4.5    从 HDFS 读写文件"></a>4.5    从 HDFS 读写文件</h2><p>Spark 的整个生态系统与 Hadoop 完全兼容的，所以对于 Hadoop 所支持的文件类型或者数据库类型，Spark 也同样支持。</p>
<p>另外，由于 Hadoop 的 API 有新旧两个版本，所以 Spark 为了能够兼容 Hadoop 所有的版本，也提供了两套创建操作接口。</p>
<p>对于外部存储创建操作而言，<code>HadoopRDD</code> 和 <code>newHadoopRDD</code> 是最为抽象的两个函数接口，主要包含以下四个参数：</p>
<ol>
<li> <strong>输入格式（InputFormat）</strong>：制定数据输入的类型，如 TextInputFormat 等，新旧两个版本所引用的版本分别是 <code>org.apache.hadoop.mapred.InputFormat</code> 和 <code>org.apache.hadoop.mapreduce.InputFormat(NewInputFormat)</code></li>
<li> <strong>键类型</strong>：指定 <code>[K,V]</code> 键值对中 K 的类型</li>
<li> <strong>值类型</strong>：指定 <code>[K,V]</code> 键值对中 V 的类型</li>
<li> <strong>分区值</strong>：指定由外部存储生成的 RDD 的 partition 数量的最小值，如果没有指定，系统会使用默认值 <code>defaultMinSplits</code></li>
</ol>
<p>注意：其它创建操作的 API 接口都是为了方便最终的 Spark 程序开发者而设置的，是这两个接口的高效实现版本。例如，对于 <code>textFile</code> 而言，只有 <code>path</code> 这个指定文件路径的参数，其它参数在系统内部指定了默认值。</p>
<ol>
<li> 在 Hadoop 中以压缩形式存储的数据，不需要指定解压方式就能够进行读取，因为 Hadoop 本身有一个解压器会根据压缩文件的后缀推断解压算法进行解压。</li>
<li> 如果用 Spark 从 Hadoop 中读取某种类型的数据不知道怎么读取的时候，上网查找一个使用 <code>map-reduce</code> 的时候是怎么读取这种这种数据的，然后再将对应的读取方式改写成上面的 hadoopRDD 和 newAPIHadoopRDD 两个类就行了。</li>
</ol>
<hr>
<h2 id="4-6-从-Mysql-数据读写文件"><a href="#4-6-从-Mysql-数据读写文件" class="headerlink" title="4.6    从 Mysql 数据读写文件"></a>4.6    从 Mysql 数据读写文件</h2><h3 id="1、引入-MySQL-依赖"><a href="#1、引入-MySQL-依赖" class="headerlink" title="1、引入 MySQL 依赖"></a>1、引入 MySQL 依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>





<h3 id="2、从-Mysql-读取数据"><a href="#2、从-Mysql-读取数据" class="headerlink" title="2、从 Mysql 读取数据"></a>2、从 Mysql 读取数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> day04</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">DriverManager</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">JdbcRDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Practice&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">//定义连接mysql的参数</span></span><br><span class="line">        <span class="keyword">val</span> driver = <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span></span><br><span class="line">        <span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://hadoop201:3306/rdd&quot;</span></span><br><span class="line">        <span class="keyword">val</span> userName = <span class="string">&quot;root&quot;</span></span><br><span class="line">        <span class="keyword">val</span> passWd = <span class="string">&quot;aaa&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = <span class="keyword">new</span> <span class="type">JdbcRDD</span>(</span><br><span class="line">            sc,</span><br><span class="line">            () =&gt; &#123;</span><br><span class="line">                <span class="type">Class</span>.forName(driver)</span><br><span class="line">                <span class="type">DriverManager</span>.getConnection(url, userName, passWd)</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;select id, name from user where id &gt;= ? and id &lt;= ?&quot;</span>,</span><br><span class="line">            <span class="number">1</span>,</span><br><span class="line">            <span class="number">20</span>,</span><br><span class="line">            <span class="number">2</span>,</span><br><span class="line">            result =&gt; (result.getInt(<span class="number">1</span>), result.getString(<span class="number">2</span>))</span><br><span class="line">        )</span><br><span class="line">        rdd.collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="3、向-Mysql-写入数据"><a href="#3、向-Mysql-写入数据" class="headerlink" title="3、向 Mysql 写入数据"></a>3、向 Mysql 写入数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> day04</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.&#123;<span class="type">Connection</span>, <span class="type">DriverManager</span>, <span class="type">PreparedStatement</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCDemo2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Practice&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">//定义连接mysql的参数</span></span><br><span class="line">        <span class="keyword">val</span> driver = <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span></span><br><span class="line">        <span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://hadoop201:3306/rdd&quot;</span></span><br><span class="line">        <span class="keyword">val</span> userName = <span class="string">&quot;root&quot;</span></span><br><span class="line">        <span class="keyword">val</span> passWd = <span class="string">&quot;aaa&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = sc.parallelize(<span class="type">Array</span>((<span class="number">110</span>, <span class="string">&quot;police&quot;</span>), (<span class="number">119</span>, <span class="string">&quot;fire&quot;</span>)))</span><br><span class="line">        <span class="comment">// 对每个分区执行 参数函数</span></span><br><span class="line">        rdd.foreachPartition(it =&gt; &#123;</span><br><span class="line">            <span class="type">Class</span>.forName(driver)</span><br><span class="line">            <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">DriverManager</span>.getConnection(url, userName, passWd)</span><br><span class="line">            it.foreach(x =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> statement: <span class="type">PreparedStatement</span> = conn.prepareStatement(<span class="string">&quot;insert into user values(?, ?)&quot;</span>)</span><br><span class="line">                statement.setInt(<span class="number">1</span>, x<span class="number">.1</span>)</span><br><span class="line">                statement.setString(<span class="number">2</span>, x<span class="number">.2</span>)</span><br><span class="line">                statement.executeUpdate()</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="4-7-从-Hbase-读写文件"><a href="#4-7-从-Hbase-读写文件" class="headerlink" title="4.7    从 Hbase 读写文件"></a>4.7    从 Hbase 读写文件</h2><p>由于 <code>org.apache.hadoop.hbase.mapreduce.TableInputFormat</code> 类的实现，Spark 可以通过 Hadoop 输入格式访问 HBase。这个输入格式会返回键值对数据，其中键的类型为 <code>org. apache.hadoop.hbase.io.ImmutableBytesWritable</code>，而值的类型为 <code>org.apache.hadoop.hbase.client.Result</code>。</p>
<h3 id="1、导入依赖"><a href="#1、导入依赖" class="headerlink" title="1、导入依赖"></a>1、导入依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mortbay.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>servlet-api-2.5<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>javax.servlet<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>servlet-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mortbay.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>servlet-api-2.5<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>javax.servlet<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>servlet-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="2、从-HBase-读取数据"><a href="#2、从-HBase-读取数据" class="headerlink" title="2、从 HBase 读取数据"></a>2、从 HBase 读取数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> day04</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">HBaseConfiguration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.<span class="type">Result</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.<span class="type">TableInputFormat</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HBaseDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Practice&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> hbaseConf: <span class="type">Configuration</span> = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">        hbaseConf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop201,hadoop202,hadoop203&quot;</span>)</span><br><span class="line">        hbaseConf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>, <span class="string">&quot;student&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">ImmutableBytesWritable</span>, <span class="type">Result</span>)] = sc.newAPIHadoopRDD(</span><br><span class="line">            hbaseConf,</span><br><span class="line">            classOf[<span class="type">TableInputFormat</span>],</span><br><span class="line">            classOf[<span class="type">ImmutableBytesWritable</span>],</span><br><span class="line">            classOf[<span class="type">Result</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">String</span>] = rdd.map &#123;</span><br><span class="line">            <span class="keyword">case</span> (_, result) =&gt; <span class="type">Bytes</span>.toString(result.getRow)</span><br><span class="line">        &#125;</span><br><span class="line">        rdd2.collect.foreach(println)</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="3、向-HBase-写入数据"><a href="#3、向-HBase-写入数据" class="headerlink" title="3、向 HBase 写入数据"></a>3、向 HBase 写入数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> day04</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">HBaseConfiguration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.<span class="type">Put</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.<span class="type">TableOutputFormat</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.<span class="type">Job</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HBaseDemo2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Practice&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> hbaseConf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">        hbaseConf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop201,hadoop202,hadoop203&quot;</span>)</span><br><span class="line">        hbaseConf.set(<span class="type">TableOutputFormat</span>.<span class="type">OUTPUT_TABLE</span>, <span class="string">&quot;student&quot;</span>)</span><br><span class="line">        <span class="comment">// 通过job来设置输出的格式的类</span></span><br><span class="line">        <span class="keyword">val</span> job = <span class="type">Job</span>.getInstance(hbaseConf)</span><br><span class="line">        job.setOutputFormatClass(classOf[<span class="type">TableOutputFormat</span>[<span class="type">ImmutableBytesWritable</span>]])</span><br><span class="line">        job.setOutputKeyClass(classOf[<span class="type">ImmutableBytesWritable</span>])</span><br><span class="line">        job.setOutputValueClass(classOf[<span class="type">Put</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> initialRDD = sc.parallelize(<span class="type">List</span>((<span class="string">&quot;100&quot;</span>, <span class="string">&quot;apple&quot;</span>, <span class="string">&quot;11&quot;</span>), (<span class="string">&quot;200&quot;</span>, <span class="string">&quot;banana&quot;</span>, <span class="string">&quot;12&quot;</span>), (<span class="string">&quot;300&quot;</span>, <span class="string">&quot;pear&quot;</span>, <span class="string">&quot;13&quot;</span>)))</span><br><span class="line">        <span class="keyword">val</span> hbaseRDD = initialRDD.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(<span class="type">Bytes</span>.toBytes(x<span class="number">.1</span>))</span><br><span class="line">            put.addColumn(<span class="type">Bytes</span>.toBytes(<span class="string">&quot;info&quot;</span>), <span class="type">Bytes</span>.toBytes(<span class="string">&quot;name&quot;</span>), <span class="type">Bytes</span>.toBytes(x<span class="number">.2</span>))</span><br><span class="line">            put.addColumn(<span class="type">Bytes</span>.toBytes(<span class="string">&quot;info&quot;</span>), <span class="type">Bytes</span>.toBytes(<span class="string">&quot;weight&quot;</span>), <span class="type">Bytes</span>.toBytes(x._3))</span><br><span class="line">            (<span class="keyword">new</span> <span class="type">ImmutableBytesWritable</span>(), put)</span><br><span class="line">        &#125;)</span><br><span class="line">        hbaseRDD.saveAsNewAPIHadoopDataset(job.getConfiguration)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<hr>
<h1 id="第-5-章-RDD-编程进阶"><a href="#第-5-章-RDD-编程进阶" class="headerlink" title="第 5 章 RDD 编程进阶"></a>第 5 章 RDD 编程进阶</h1><h2 id="5-1共享变量问题"><a href="#5-1共享变量问题" class="headerlink" title="5.1共享变量问题"></a>5.1共享变量问题</h2><p>看下面的代码:</p>
<p>package day04</p>
<p> import org.apache.spark.rdd.RDD<br> import org.apache.spark.{SparkConf, SparkContext}</p>
<p> object AccDemo1 {<br>   def main(args: Array[String]): Unit = {<br>     val conf = new SparkConf().setAppName(“Practice”).setMaster(“local[2]”)<br>     val sc = new SparkContext(conf)<br>     val p1 = Person(10)<br>     // 将来会把对象序列化之后传递到每个节点上<br>     val rdd1 = sc.parallelize(Array(p1))<br>     val rdd2: RDD[Person] = rdd1.map(p =&gt; {p.age = 100; p})</p>
<pre><code> rdd2.count()
 // 仍然是 10
 println(p1.age)
</code></pre>
<p>   }<br> }</p>
<p> case class Person(var age:Int)</p>
<p>正常情况下, 传递给 Spark 算子(比如: map, reduce 等)的函数都是在远程的集群节点上执行, 函数中用到的所有变量都是独立的拷贝.</p>
<p>这些变量被拷贝到集群上的每个节点上, 都这些变量的更改不会传递回驱动程序.</p>
<p>支持跨 task 之间共享变量通常是低效的, 但是 Spark 对共享变量也提供了两种支持:</p>
<p>\1.    累加器</p>
<p>\2.    广播变量</p>
<h3 id="5-2累加器-Accumulator"><a href="#5-2累加器-Accumulator" class="headerlink" title="5.2累加器(Accumulator)"></a>5.2累加器(Accumulator)</h3><p>累加器用来对信息进行聚合，通常在向 Spark 传递函数时，比如使用 map() 函数或者用 filter() 传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，所以更新这些副本的值不会影响驱动器中的对应变量。</p>
<p>如果我们想实现所有分片处理时更新共享变量的功能，那么累加器可以实现我们想要的效果。</p>
<p>累加器是一种变量, 仅仅支持“add”, 支持并发. 累加器用于去实现计数器或者求和. Spark 内部已经支持数字类型的累加器, 开发者可以添加其他类型的支持.</p>
<h4 id="内置累加器"><a href="#内置累加器" class="headerlink" title="内置累加器"></a>内置累加器</h4><p>需求:计算文件中空行的数量</p>
<p>package day04</p>
<p> import org.apache.spark.rdd.RDD<br> import org.apache.spark.util.LongAccumulator<br> import org.apache.spark.{SparkConf, SparkContext}</p>
<p> object AccDemo1 {<br>   def main(args: Array[String]): Unit = {<br>     val conf = new SparkConf().setAppName(“Practice”).setMaster(“local[2]”)<br>     val sc = new SparkContext(conf)<br>     val rdd: RDD[String] = sc.textFile(“file://“ + ClassLoader.getSystemResource(“words.txt”).getPath)<br>     // 得到一个 Long 类型的累加器. 将从 0 开始累加<br>     val emptyLineCount: LongAccumulator = sc.longAccumulator<br>     rdd.foreach(s =&gt; if (s.trim.length == 0) emptyLineCount.add(1))<br>     println(emptyLineCount.value)<br>   }<br> }</p>
<p>说明:</p>
<p>\1.    在驱动程序中通过sc.longAccumulator得到Long类型的累加器, 还有Double类型的</p>
<p>\2.    可以通过value来访问累加器的值.(与sum等价). avg得到平均值</p>
<p>\3.    只能通过add来添加值.</p>
<p>\4.    累加器的更新操作最好放在action中, Spark 可以保证每个 task 只执行一次. 如果放在 transformations 操作中则不能保证只更新一次.有可能会被重复执行.</p>
<h4 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h4><p>通过继承类AccumulatorV2来自定义累加器.</p>
<p>下面这个累加器可以用于在程序运行过程中收集一些文本类信息，最终以List[String]的形式返回。</p>
<p>package day04</p>
<p> import java.util<br> import java.util.{ArrayList, Collections}</p>
<p> import org.apache.spark.util.AccumulatorV2</p>
<p> object MyAccDemo {<br>   def main(args: Array[String]): Unit = {</p>
<p>   }<br> }</p>
<p> class MyAcc extends AccumulatorV2[String, java.util.List[String]] {<br>   private val _list: java.util.List[String] = Collections.synchronizedList(new ArrayList<a href="">String</a>)<br>   override def isZero: Boolean = _list.isEmpty</p>
<p>   override def copy(): AccumulatorV2[String, util.List[String]] = {<br>     val newAcc = new MyAcc<br>     _list.synchronized {<br>       newAcc._list.addAll(_list)<br>     }<br>     newAcc<br>   }</p>
<p>   override def reset(): Unit = _list.clear()</p>
<p>   override def add(v: String): Unit = _list.add(v)</p>
<p>   override def merge(other: AccumulatorV2[String, util.List[String]]): Unit =other match {<br>     case o: MyAcc =&gt; _list.addAll(o.value)<br>     case _ =&gt; throw new UnsupportedOperationException(<br>       s”Cannot merge ${this.getClass.getName} with ${other.getClass.getName}”)<br>   }</p>
<p>   override def value: util.List[String] = java.util.Collections.unmodifiableList(new util.ArrayList<a href="_list">String</a>)<br> }</p>
<p>测试:</p>
<p>object MyAccDemo {<br>   def main(args: Array[String]): Unit = {<br>     val pattern = “””^\d+$”””<br>     val conf = new SparkConf().setAppName(“Practice”).setMaster(“local[2]”)<br>     val sc = new SparkContext(conf)<br>     // 统计出来非纯数字, 并计算纯数字元素的和<br>     val rdd1 = sc.parallelize(Array(“abc”, “a30b”, “aaabb2”, “60”, “20”))</p>
<pre><code> val acc = new MyAcc
 sc.register(acc)
 val rdd2: RDD[Int] = rdd1.filter(x =&gt; &#123;
   val flag: Boolean = x.matches(pattern)
   if (!flag) acc.add(x)
   flag
 &#125;).map(_.toInt)
 println(rdd2.reduce(_ + _))
 println(acc.value)
</code></pre>
<p>   }<br> }</p>
<p>注意:</p>
<p>在使用自定义累加器的不要忘记注册sc.register(acc)</p>
<p>​                               </p>
<h3 id="5-3广播变量"><a href="#5-3广播变量" class="headerlink" title="5.3广播变量"></a>5.3广播变量</h3><p>广播变量在每个节点上保存一个只读的变量的缓存, 而不用给每个 task 来传送一个 copy.</p>
<p>例如, 给每个节点一个比较大的输入数据集是一个比较高效的方法. Spark 也会用该对象的广播逻辑去分发广播变量来降低通讯的成本.</p>
<p>广播变量通过调用SparkContext.broadcast(v)来创建. 广播变量是对v的包装, 通过调用广播变量的 value方法可以访问.</p>
<p>scala&gt; val broadcastVar = sc.broadcast(Array(1, 2, 3))<br> broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)</p>
<p> scala&gt; broadcastVar.value<br> res0: Array[Int] = Array(1, 2, 3)</p>
<p>说明:</p>
<p>\1.     通过对一个类型T的对象调用SparkContext.broadcast创建出一个Broadcast[T]对象。任何可序列化的类型都可以这么实现。</p>
<p>\2.     通过value属性访问该对象的值(在Java中为value()方法)。</p>
<p>\3.     变量只会被发到各个节点一次，应作为只读值处理(修改这个值不会影响到别的节点)。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/05/31/hello-world/" rel="prev" title="Hello World">
                  <i class="fa fa-chevron-left"></i> Hello World
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/05/31/temp/" rel="next" title="">
                   <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  





</body>
</html>
